title,section,text,ntokens,context,questions,answers
Artificial intelligence,Summary,"Artificial intelligence (AI) is intelligence demonstrated by computers, as opposed to human or animal intelligence. ""Intelligence"" encompasses the ability to learn and to reason, to generalize, and to infer meaning.AI applications include advanced web search engines (e.g., Google Search), recommendation systems (used by YouTube, Amazon, and Netflix), understanding human speech (such as Siri and Alexa), self-driving cars (e.g., Waymo), generative or creative tools (ChatGPT and AI art), automated decision-making, and competing at the highest level in strategic game systems (such as chess and Go).Artificial intelligence was founded as an academic discipline in 1956, and in the years since it has experienced several waves of optimism, followed by disappointment and the loss of funding (known as an ""AI winter""), followed by new approaches, success, and renewed funding. AI research has tried and discarded many different approaches, including simulating the brain, modeling human problem solving, formal logic, large databases of knowledge, and imitating animal behavior. In the first decades of the 21st century, highly mathematical and statistical machine learning has dominated the field, and this technique has proved highly successful, helping to solve many challenging problems throughout industry and academia.The various sub-fields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and the ability to move and manipulate objects. General intelligence (the ability to solve an arbitrary problem) is among the field's long-term goals. To solve these problems, AI researchers have adapted and integrated a wide range of problem-solving techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, probability, and economics. AI also draws upon computer science, psychology, linguistics, philosophy, and many other fields.
The field was founded on the assumption that human intelligence ""can be so precisely described that a machine can be made to simulate it"". This raised philosophical arguments about the mind and the ethical consequences of creating artificial beings endowed with human-like intelligence; these issues have previously been explored by myth, fiction (science fiction), and philosophy since antiquity. Computer scientists and philosophers have since suggested that AI may become an existential risk to humanity if its rational capacities are not steered towards goals beneficial to humankind. Economists have frequently highlighted the risks of redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment. The term artificial intelligence has also been criticized for overhyping AI's true technological capabilities.",536,"Artificial intelligence
Summary

Artificial intelligence (AI) is intelligence demonstrated by computers, as opposed to human or animal intelligence. ""Intelligence"" encompasses the ability to learn and to reason, to generalize, and to infer meaning.AI applications include advanced web search engines (e.g., Google Search), recommendation systems (used by YouTube, Amazon, and Netflix), understanding human speech (such as Siri and Alexa), self-driving cars (e.g., Waymo), generative or creative tools (ChatGPT and AI art), automated decision-making, and competing at the highest level in strategic game systems (such as chess and Go).Artificial intelligence was founded as an academic discipline in 1956, and in the years since it has experienced several waves of optimism, followed by disappointment and the loss of funding (known as an ""AI winter""), followed by new approaches, success, and renewed funding. AI research has tried and discarded many different approaches, including simulating the brain, modeling human problem solving, formal logic, large databases of knowledge, and imitating animal behavior. In the first decades of the 21st century, highly mathematical and statistical machine learning has dominated the field, and this technique has proved highly successful, helping to solve many challenging problems throughout industry and academia.The various sub-fields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and the ability to move and manipulate objects. General intelligence (the ability to solve an arbitrary problem) is among the field's long-term goals. To solve these problems, AI researchers have adapted and integrated a wide range of problem-solving techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, probability, and economics. AI also draws upon computer science, psychology, linguistics, philosophy, and many other fields.
The field was founded on the assumption that human intelligence ""can be so precisely described that a machine can be made to simulate it"". This raised philosophical arguments about the mind and the ethical consequences of creating artificial beings endowed with human-like intelligence; these issues have previously been explored by myth, fiction (science fiction), and philosophy since antiquity. Computer scientists and philosophers have since suggested that AI may become an existential risk to humanity if its rational capacities are not steered towards goals beneficial to humankind. Economists have frequently highlighted the risks of redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment. The term artificial intelligence has also been criticized for overhyping AI's true technological capabilities.","1. What are the goals of AI research?
2. What are some of the tools used in AI research?
3. What are the risks associated with artificial intelligence?
4. What is the definition of artificial intelligence?","1. The goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and the ability to move and manipulate objects.

2. AI research uses a variety of tools, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, probability, and economics.

3. The risks associated with artificial intelligence include the possibility of existential risk to humanity, unemployment, and redundancies.

4. Artificial intelligence is intelligence demonstrated by computers, as opposed to human or animal intelligence."
Artificial intelligence,History,"Artificial beings with intelligence appeared as storytelling devices in antiquity, and have been common in fiction, as in Mary Shelley's Frankenstein or Karel Čapek's R.U.R. These characters and their fates raised many of the same issues now discussed in the ethics of artificial intelligence.The study of mechanical or ""formal"" reasoning began with philosophers and mathematicians in antiquity. The study of mathematical logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as ""0"" and ""1"", could simulate any conceivable act of mathematical deduction. This insight that digital computers can simulate any process of formal reasoning is known as the Church–Turing thesis. This, along with concurrent discoveries in neurobiology, information theory and cybernetics, led researchers to consider the possibility of building an electronic brain. The first work that is now generally recognized as AI was McCullouch and Pitts' 1943 formal design for Turing-complete ""artificial neurons"".The field of AI research was born at a workshop at Dartmouth College in 1956. The attendees became the founders and leaders of AI research. They and their students produced programs that the press described as ""astonishing"": computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English. 
By the middle of the 1960s, research in the U.S. was heavily funded by the Department of Defense and laboratories had been established around the world.Researchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with artificial general intelligence and considered this the goal of their field. Herbert Simon predicted, ""machines will be capable, within twenty years, of doing any work a man can do"". Marvin Minsky agreed, writing, ""within a generation ... the problem of creating 'artificial intelligence' will substantially be solved"".They had failed to recognize the difficulty of some of the remaining tasks. Progress slowed and in 1974, in response to the criticism of Sir James Lighthill and ongoing pressure from the US Congress to fund more productive projects, both the U.S. and British governments cut off exploratory research in AI. The next few years would later be called an ""AI winter"", a period when obtaining funding for AI projects was difficult.In the early 1980s, AI research was revived by the commercial success of expert systems, a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research. However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.Many researchers began to doubt that the current practices would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition. A number of researchers began to look into ""sub-symbolic"" approaches to specific AI problems. Robotics researchers, such as Rodney Brooks, rejected symbolic AI and focused on the basic engineering problems that would allow robots to move, survive, and learn their environment.Interest in neural networks and ""connectionism"" was revived by Geoffrey Hinton, David Rumelhart and others in the middle of the 1980s. Soft computing tools were developed in the 1980s, such as neural networks, fuzzy systems, Grey system theory, evolutionary computation and many tools drawn from statistics or mathematical optimization.
AI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This ""narrow"" and ""formal"" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics, economics and mathematics). 
By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as ""artificial intelligence"".Faster computers
and access to large amounts of data 
enabled advances in machine learning and perception; data-hungry deep learning methods started to dominate accuracy benchmarks around 2012.According to Bloomberg's Jack Clark, 2015 was a landmark year for artificial intelligence, with the number of software projects that use AI within Google increased from a ""sporadic usage"" in 2012 to more than 2,700 projects. He attributed this to an increase in affordable neural networks, due to a rise in cloud computing infrastructure and to an increase in research tools and datasets. In a 2017 survey, one in five companies reported they had ""incorporated AI in some offerings or processes"". The amount of research into AI (measured by total publications) increased by 50% in the years 2015–2019. According to AI Impacts at Stanford, around 2022 about $50 billion annually is invested in artificial intelligence in the US, and about 20% of new US Computer Science PhD graduates have specialized in artificial intelligence; about 800,000 AI-related US job openings existed in 2022.Numerous academic researchers became concerned that AI was no longer pursuing the original goal of creating versatile, fully intelligent machines. Much of current research involves statistical AI, which is overwhelmingly used to solve specific problems, even highly successful techniques such as deep learning. This concern has led to the subfield of artificial general intelligence (or ""AGI""), which had several well-funded institutions by the 2010s.",1113,"Artificial intelligence
History

Artificial beings with intelligence appeared as storytelling devices in antiquity, and have been common in fiction, as in Mary Shelley's Frankenstein or Karel Čapek's R.U.R. These characters and their fates raised many of the same issues now discussed in the ethics of artificial intelligence.The study of mechanical or ""formal"" reasoning began with philosophers and mathematicians in antiquity. The study of mathematical logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as ""0"" and ""1"", could simulate any conceivable act of mathematical deduction. This insight that digital computers can simulate any process of formal reasoning is known as the Church–Turing thesis. This, along with concurrent discoveries in neurobiology, information theory and cybernetics, led researchers to consider the possibility of building an electronic brain. The first work that is now generally recognized as AI was McCullouch and Pitts' 1943 formal design for Turing-complete ""artificial neurons"".The field of AI research was born at a workshop at Dartmouth College in 1956. The attendees became the founders and leaders of AI research. They and their students produced programs that the press described as ""astonishing"": computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English. 
By the middle of the 1960s, research in the U.S. was heavily funded by the Department of Defense and laboratories had been established around the world.Researchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with artificial general intelligence and considered this the goal of their field. Herbert Simon predicted, ""machines will be capable, within twenty years, of doing any work a man can do"". Marvin Minsky agreed, writing, ""within a generation ... the problem of creating 'artificial intelligence' will substantially be solved"".They had failed to recognize the difficulty of some of the remaining tasks. Progress slowed and in 1974, in response to the criticism of Sir James Lighthill and ongoing pressure from the US Congress to fund more productive projects, both the U.S. and British governments cut off exploratory research in AI. The next few years would later be called an ""AI winter"", a period when obtaining funding for AI projects was difficult.In the early 1980s, AI research was revived by the commercial success of expert systems, a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research. However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.Many researchers began to doubt that the current practices would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition. A number of researchers began to look into ""sub-symbolic"" approaches to specific AI problems. Robotics researchers, such as Rodney Brooks, rejected symbolic AI and focused on the basic engineering problems that would allow robots to move, survive, and learn their environment.Interest in neural networks and ""connectionism"" was revived by Geoffrey Hinton, David Rumelhart and others in the middle of the 1980s. Soft computing tools were developed in the 1980s, such as neural networks, fuzzy systems, Grey system theory, evolutionary computation and many tools drawn from statistics or mathematical optimization.
AI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This ""narrow"" and ""formal"" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics, economics and mathematics). 
By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as ""artificial intelligence"".Faster computers
and access to large amounts of data 
enabled advances in machine learning and perception; data-hungry deep learning methods started to dominate accuracy benchmarks around 2012.According to Bloomberg's Jack Clark, 2015 was a landmark year for artificial intelligence, with the number of software projects that use AI within Google increased from a ""sporadic usage"" in 2012 to more than 2,700 projects. He attributed this to an increase in affordable neural networks, due to a rise in cloud computing infrastructure and to an increase in research tools and datasets. In a 2017 survey, one in five companies reported they had ""incorporated AI in some offerings or processes"". The amount of research into AI (measured by total publications) increased by 50% in the years 2015–2019. According to AI Impacts at Stanford, around 2022 about $50 billion annually is invested in artificial intelligence in the US, and about 20% of new US Computer Science PhD graduates have specialized in artificial intelligence; about 800,000 AI-related US job openings existed in 2022.Numerous academic researchers became concerned that AI was no longer pursuing the original goal of creating versatile, fully intelligent machines. Much of current research involves statistical AI, which is overwhelmingly used to solve specific problems, even highly successful techniques such as deep learning. This concern has led to the subfield of artificial general intelligence (or ""AGI""), which had several well-funded institutions by the 2010s.","1. What is the history of artificial intelligence?
2. What are the goals of artificial intelligence?
3. What are some of the challenges that remain in the field of artificial intelligence?
4. What caused the ""AI winter"" in the 1970s?
5. What is the current state of artificial intelligence research?","1. The history of artificial intelligence can be traced back to antiquity, when artificial beings with intelligence were used as storytelling devices. AI research began in earnest in the 1950s, and there was a lot of optimism about the field's potential to create machines with artificial general intelligence. However, research slowed in the 1970s due to criticism from experts and a lack of funding. The field has seen a resurgence in recent years, with increased funding and new methods that have led to significant advances in machine learning and perception.

2. The goals of artificial intelligence vary, but generally include the ability to reason, learn, and communicate like humans.

3. Some of the challenges that remain in the field of artificial intelligence include the ability to create machines that can reason, learn, and communicate like humans. Another challenge is the ability to create machines that can navigate and survive in the real world.

4. The ""AI winter"" in the 1970s was caused by a lack of funding and criticism from experts.

5. The current state of artificial intelligence research is very promising, with significant advances in machine learning and perception. However, there is still a lot of work to be done in order to create machines that can reason, learn, and communicate like humans."
Artificial intelligence,Goals,The general problem of simulating (or creating) intelligence has been broken down into sub-problems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention.,50,"Artificial intelligence
Goals

The general problem of simulating (or creating) intelligence has been broken down into sub-problems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention.","1. What are the sub-problems of simulating intelligence? 
2. What are the most important traits that researchers are trying to simulate?","1. The sub-problems of simulating intelligence are particular traits or capabilities that researchers expect an intelligent system to display. 
2. The most important traits that researchers are trying to simulate are reasoning, natural language processing, and machine learning."
Artificial intelligence,"Reasoning, problem-solving","Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, AI research had developed methods for dealing with uncertain or incomplete information, employing concepts from probability and economics.Many of these algorithms proved to be insufficient for solving large reasoning problems because they experienced a ""combinatorial explosion"": they became exponentially slower as the problems grew larger. Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments.",120,"Artificial intelligence
Reasoning, problem-solving

Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, AI research had developed methods for dealing with uncertain or incomplete information, employing concepts from probability and economics.Many of these algorithms proved to be insufficient for solving large reasoning problems because they experienced a ""combinatorial explosion"": they became exponentially slower as the problems grew larger. Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments.","1. What is the ""combinatorial explosion""?
2. How do humans usually solve problems?","1. The ""combinatorial explosion"" is when an algorithm becomes exponentially slower as the problem size grows.
2. Humans usually solve problems using fast, intuitive judgments."
Artificial intelligence,Knowledge representation,"Knowledge representation and knowledge engineering allow AI programs to answer questions intelligently and make deductions about real-world facts.
A representation of ""what exists"" is an ontology: the set of objects, relations, concepts, and properties formally described so that software agents can interpret them. The most general ontologies are called upper ontologies, which attempt to provide a foundation for all other knowledge and act as mediators between domain ontologies that cover specific knowledge about a particular knowledge domain (field of interest or area of concern). A truly intelligent program would also need access to commonsense knowledge, the set of facts that an average person knows. The semantics of an ontology is typically represented in description logic, such as the Web Ontology Language.AI research has developed tools to represent specific domains, such as objects, properties, categories and relations between objects; situations, events, states and time; causes and effects; knowledge about knowledge (what we know about what other people know); and  default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing);. Among the most difficult problems in AI are: the breadth of commonsense knowledge (the number of atomic facts that the average person knows is enormous); and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as ""facts"" or ""statements"" that they could express verbally).Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining ""interesting"" and actionable inferences from large databases), and other areas.",335,"Artificial intelligence
Knowledge representation

Knowledge representation and knowledge engineering allow AI programs to answer questions intelligently and make deductions about real-world facts.
A representation of ""what exists"" is an ontology: the set of objects, relations, concepts, and properties formally described so that software agents can interpret them. The most general ontologies are called upper ontologies, which attempt to provide a foundation for all other knowledge and act as mediators between domain ontologies that cover specific knowledge about a particular knowledge domain (field of interest or area of concern). A truly intelligent program would also need access to commonsense knowledge, the set of facts that an average person knows. The semantics of an ontology is typically represented in description logic, such as the Web Ontology Language.AI research has developed tools to represent specific domains, such as objects, properties, categories and relations between objects; situations, events, states and time; causes and effects; knowledge about knowledge (what we know about what other people know); and  default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing);. Among the most difficult problems in AI are: the breadth of commonsense knowledge (the number of atomic facts that the average person knows is enormous); and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as ""facts"" or ""statements"" that they could express verbally).Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining ""interesting"" and actionable inferences from large databases), and other areas.","1. What is an ontology?
2. What is the difference between a domain ontology and an upper ontology?
3. What is the most difficult problem in AI?
4. What is the purpose of a formal knowledge representation?","1. An ontology is a set of objects, relations, concepts, and properties formally described so that software agents can interpret them.
2. A domain ontology is a specific ontology that covers knowledge about a particular knowledge domain.
3. The most difficult problem in AI is the breadth of commonsense knowledge.
4. The purpose of a formal knowledge representation is to provide a foundation for all other knowledge and act as a mediator between domain ontologies."
Artificial intelligence,Learning,"Machine learning (ML), a fundamental concept of AI research since the field's inception, is the study of computer algorithms that improve automatically through experience.Unsupervised learning finds patterns in a stream of input.
Supervised learning requires a human to label the input data first, and comes in two main varieties: classification and numerical regression. Classification is used to determine what category something belongs in – the program sees a number of examples of things from several categories and will learn to classify new inputs. Regression is the attempt to produce a function that describes the relationship between inputs and outputs and predicts how the outputs should change as the inputs change. Both classifiers and regression learners can be viewed as ""function approximators"" trying to learn an unknown (possibly implicit) function; for example, a spam classifier can be viewed as learning a function that maps from the text of an email to one of two categories, ""spam"" or ""not spam"".In reinforcement learning the agent is rewarded for good responses and punished for bad ones. The agent classifies its responses to form a strategy for operating in its problem space.Transfer learning is when the knowledge gained from one problem is applied to a new problem.Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.",273,"Artificial intelligence
Learning

Machine learning (ML), a fundamental concept of AI research since the field's inception, is the study of computer algorithms that improve automatically through experience.Unsupervised learning finds patterns in a stream of input.
Supervised learning requires a human to label the input data first, and comes in two main varieties: classification and numerical regression. Classification is used to determine what category something belongs in – the program sees a number of examples of things from several categories and will learn to classify new inputs. Regression is the attempt to produce a function that describes the relationship between inputs and outputs and predicts how the outputs should change as the inputs change. Both classifiers and regression learners can be viewed as ""function approximators"" trying to learn an unknown (possibly implicit) function; for example, a spam classifier can be viewed as learning a function that maps from the text of an email to one of two categories, ""spam"" or ""not spam"".In reinforcement learning the agent is rewarded for good responses and punished for bad ones. The agent classifies its responses to form a strategy for operating in its problem space.Transfer learning is when the knowledge gained from one problem is applied to a new problem.Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.","1. What is machine learning? 
2. What are the different types of supervised learning? 
3. What is reinforcement learning? 
4. What is transfer learning? 
5. What is computational learning theory?","1. Machine learning is the study of computer algorithms that improve automatically through experience. 
2. There are two main types of supervised learning: classification and numerical regression. Classification is used to determine what category something belongs in, while regression is the attempt to produce a function that describes the relationship between inputs and outputs. 
3. Reinforcement learning is when the agent is rewarded for good responses and punished for bad ones. The agent classifies its responses to form a strategy for operating in its problem space. 
4. Transfer learning is when the knowledge gained from one problem is applied to a new problem. 
5. Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization."
Artificial intelligence,Natural language processing,"Natural language processing (NLP)
allows machines to read and understand human language. A sufficiently powerful natural language processing system would enable natural-language user interfaces and the acquisition of knowledge directly from human-written sources, such as newswire texts. Some straightforward applications of NLP include information retrieval, question answering and machine translation.Symbolic AI used formal syntax to translate the deep structure of sentences into logic. This failed to produce useful applications, due to the intractability of logic and the breadth of commonsense knowledge. Modern statistical techniques include co-occurrence frequencies (how often one word appears near another), ""Keyword spotting"" (searching for a particular word to retrieve information), transformer-based deep learning (which finds patterns in text), and others. They have achieved acceptable accuracy at the page or paragraph level, and, by 2019, could generate coherent text.",180,"Artificial intelligence
Natural language processing

Natural language processing (NLP)
allows machines to read and understand human language. A sufficiently powerful natural language processing system would enable natural-language user interfaces and the acquisition of knowledge directly from human-written sources, such as newswire texts. Some straightforward applications of NLP include information retrieval, question answering and machine translation.Symbolic AI used formal syntax to translate the deep structure of sentences into logic. This failed to produce useful applications, due to the intractability of logic and the breadth of commonsense knowledge. Modern statistical techniques include co-occurrence frequencies (how often one word appears near another), ""Keyword spotting"" (searching for a particular word to retrieve information), transformer-based deep learning (which finds patterns in text), and others. They have achieved acceptable accuracy at the page or paragraph level, and, by 2019, could generate coherent text.","1. What is the goal of natural language processing? 
2. What are some of the challenges of natural language processing? 
3. What are some of the modern techniques used in natural language processing? 
4. What has been the accuracy of natural language processing techniques at the page or paragraph level? 
5. What is the future of natural language processing?","1. The goal of natural language processing is to enable machines to read and understand human language. 
2. Some of the challenges of natural language processing include the intractability of logic and the breadth of commonsense knowledge. 
3. Modern techniques used in natural language processing include co-occurrence frequencies, ""Keyword spotting"", transformer-based deep learning, and others. 
4. The accuracy of natural language processing techniques at the page or paragraph level has been acceptable. 
5. The future of natural language processing is uncertain, but it is likely that the accuracy of natural language processing techniques will continue to improve."
Artificial intelligence,Perception,"Machine perception
is the ability to use input from sensors (such as cameras, microphones, wireless signals, and active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Applications include speech recognition,facial recognition, and object recognition.
Computer vision is the ability to analyze visual input.",70,"Artificial intelligence
Perception

Machine perception
is the ability to use input from sensors (such as cameras, microphones, wireless signals, and active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Applications include speech recognition,facial recognition, and object recognition.
Computer vision is the ability to analyze visual input.","1. What is machine perception? 
2. What are some applications of machine perception? 
3. What is computer vision? 
4. What are some applications of computer vision?","1. Machine perception is the ability to use input from sensors to deduce aspects of the world. 
2. Applications of machine perception include speech recognition, facial recognition, and object recognition. 
3. Computer vision is the ability to analyze visual input. 
4. Applications of computer vision include speech recognition, facial recognition, and object recognition."
Artificial intelligence,Social intelligence,"Affective computing is an interdisciplinary umbrella that comprises systems that recognize, interpret, process or simulate human feeling, emotion and mood.
For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human–computer interaction.
However, this tends to give naïve users an unrealistic conception of how intelligent existing computer agents actually are. Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis, wherein AI classifies the affects displayed by a videotaped subject.",129,"Artificial intelligence
Social intelligence

Affective computing is an interdisciplinary umbrella that comprises systems that recognize, interpret, process or simulate human feeling, emotion and mood.
For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human–computer interaction.
However, this tends to give naïve users an unrealistic conception of how intelligent existing computer agents actually are. Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis, wherein AI classifies the affects displayed by a videotaped subject.","1. What is affective computing?
2. What are some examples of moderate successes related to affective computing?
3. What is textual sentiment analysis?
4. What is multimodal sentiment analysis?","1. Affective computing is an interdisciplinary umbrella that comprises systems that recognize, interpret, process or simulate human feeling, emotion and mood.
2. Some examples of moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis.
3. Textual sentiment analysis is the process of automatically identifying and quantifying the emotions expressed in text.
4. Multimodal sentiment analysis is the process of automatically identifying and quantifying the emotions expressed in text and video."
Artificial intelligence,General intelligence,"A machine with general intelligence can solve a wide variety of problems with breadth and versatility similar to human intelligence. There are several competing ideas about how to develop artificial general intelligence. Hans Moravec and Marvin Minsky argue that work in different individual domains can be incorporated into an advanced multi-agent system or cognitive architecture with general intelligence.Pedro Domingos hopes that there is a conceptually straightforward, but mathematically difficult, ""master algorithm"" that could lead to AGI.
Others believe that anthropomorphic features like an artificial brain
or simulated child development
will someday reach a critical point where general intelligence emerges.",127,"Artificial intelligence
General intelligence

A machine with general intelligence can solve a wide variety of problems with breadth and versatility similar to human intelligence. There are several competing ideas about how to develop artificial general intelligence. Hans Moravec and Marvin Minsky argue that work in different individual domains can be incorporated into an advanced multi-agent system or cognitive architecture with general intelligence.Pedro Domingos hopes that there is a conceptually straightforward, but mathematically difficult, ""master algorithm"" that could lead to AGI.
Others believe that anthropomorphic features like an artificial brain
or simulated child development
will someday reach a critical point where general intelligence emerges.","1. What are the different ways in which artificial general intelligence can be developed?
2. What are the arguments for and against there being a ""master algorithm"" for AGI?
3. What are the anthropomorphic features that some believe will be necessary for AGI?","1. There are several different ways in which artificial general intelligence can be developed. Some believe that it can be done through the development of a ""master algorithm"", while others believe that it will require anthropomorphic features like an artificial brain or simulated child development.
2. The argument for a ""master algorithm"" is that it could lead to AGI in a conceptually straightforward, but mathematically difficult, way. The argument against it is that it may be impossible to find such an algorithm.
3. Anthropomorphic features like an artificial brain or simulated child development are seen as necessary for AGI by some because they believe that general intelligence cannot emerge without them."
Artificial intelligence,Search and optimization,"AI can solve many problems by intelligently searching through many possible solutions. Reasoning can be reduced to performing a search. For example, logical proof can be viewed as searching for a path that leads from premises to conclusions, where each step is the application of an inference rule. Planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis. Robotics algorithms for moving limbs and grasping objects use local searches in configuration space.Simple exhaustive searches
are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes. The solution, for many problems, is to use ""heuristics"" or ""rules of thumb"" that prioritize choices in favor of those more likely to reach a goal and to do so in a shorter number of steps. In some search methodologies, heuristics can also serve to eliminate some choices unlikely to lead to a goal (called ""pruning the search tree""). Heuristics supply the program with a ""best guess"" for the path on which the solution lies.
Heuristics limit the search for solutions into a smaller sample size.

A very different kind of search came to prominence in the 1990s, based on the mathematical theory of optimization. For many problems, it is possible to begin the search with some form of a guess and then refine the guess incrementally until no more refinements can be made. These algorithms can be visualized as blind hill climbing: we begin the search at a random point on the landscape, and then, by jumps or steps, we keep moving our guess uphill, until we reach the top. Other related optimization algorithms include random optimization, beam search and metaheuristics like simulated annealing. Evolutionary computation uses a form of optimization search. For example, they may begin with a population of organisms (the guesses) and then allow them to mutate and recombine, selecting only the fittest to survive each generation (refining the guesses). Classic evolutionary algorithms include genetic algorithms, gene expression programming, and genetic programming. Alternatively, distributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).",488,"Artificial intelligence
Search and optimization

AI can solve many problems by intelligently searching through many possible solutions. Reasoning can be reduced to performing a search. For example, logical proof can be viewed as searching for a path that leads from premises to conclusions, where each step is the application of an inference rule. Planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis. Robotics algorithms for moving limbs and grasping objects use local searches in configuration space.Simple exhaustive searches
are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes. The solution, for many problems, is to use ""heuristics"" or ""rules of thumb"" that prioritize choices in favor of those more likely to reach a goal and to do so in a shorter number of steps. In some search methodologies, heuristics can also serve to eliminate some choices unlikely to lead to a goal (called ""pruning the search tree""). Heuristics supply the program with a ""best guess"" for the path on which the solution lies.
Heuristics limit the search for solutions into a smaller sample size.

A very different kind of search came to prominence in the 1990s, based on the mathematical theory of optimization. For many problems, it is possible to begin the search with some form of a guess and then refine the guess incrementally until no more refinements can be made. These algorithms can be visualized as blind hill climbing: we begin the search at a random point on the landscape, and then, by jumps or steps, we keep moving our guess uphill, until we reach the top. Other related optimization algorithms include random optimization, beam search and metaheuristics like simulated annealing. Evolutionary computation uses a form of optimization search. For example, they may begin with a population of organisms (the guesses) and then allow them to mutate and recombine, selecting only the fittest to survive each generation (refining the guesses). Classic evolutionary algorithms include genetic algorithms, gene expression programming, and genetic programming. Alternatively, distributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).","1. What is the difference between AI and search and optimization?
2. How do AI and search and optimization differ in terms of the size of the search space?
3. What is the difference between a blind hill climb and other optimization algorithms?
4. How do evolutionary computation and swarm intelligence algorithms differ?","1. AI is a field of study that deals with the design and development of intelligent computer systems, while search and optimization is a method for finding a solution to a problem.
2. AI and search and optimization differ in terms of the size of the search space. AI typically deals with problems that have a very large search space, while search and optimization deals with problems that have a smaller search space.
3. The difference between a blind hill climb and other optimization algorithms is that a blind hill climb begins with a guess and then refines the guess incrementally, while other optimization algorithms begin with a specific solution.
4. Evolutionary computation and swarm intelligence algorithms are similar in that they both use a form of optimization search. However, evolutionary computation uses a population of organisms to find a solution, while swarm intelligence algorithms use a swarm of agents to find a solution."
Artificial intelligence,Logic,"Logic
is used for knowledge representation and problem-solving, but it can be applied to other problems as well. For example, the satplan algorithm uses logic for planning
and inductive logic programming is a method for learning.Several different forms of logic are used in AI research. Propositional logic involves truth functions such as ""or"" and ""not"". First-order logic
adds quantifiers and predicates and can express facts about objects, their properties, and their relations with each other. Fuzzy logic assigns a ""degree of truth"" (between 0 and 1) to vague statements such as ""Alice is old"" (or rich, or tall, or hungry), that are too linguistically imprecise to be completely true or false.Default logics, non-monotonic logics and circumscription are forms of logic designed to help with default reasoning and the qualification problem.
Several extensions of logic have been designed to handle specific domains of knowledge, such as description logics;situation calculus, event calculus and fluent calculus (for representing events and time);causal calculus;belief calculus (belief revision); and modal logics.
Logics to model contradictory or inconsistent statements arising in multi-agent systems have also been designed, such as paraconsistent logics.",271,"Artificial intelligence
Logic

Logic
is used for knowledge representation and problem-solving, but it can be applied to other problems as well. For example, the satplan algorithm uses logic for planning
and inductive logic programming is a method for learning.Several different forms of logic are used in AI research. Propositional logic involves truth functions such as ""or"" and ""not"". First-order logic
adds quantifiers and predicates and can express facts about objects, their properties, and their relations with each other. Fuzzy logic assigns a ""degree of truth"" (between 0 and 1) to vague statements such as ""Alice is old"" (or rich, or tall, or hungry), that are too linguistically imprecise to be completely true or false.Default logics, non-monotonic logics and circumscription are forms of logic designed to help with default reasoning and the qualification problem.
Several extensions of logic have been designed to handle specific domains of knowledge, such as description logics;situation calculus, event calculus and fluent calculus (for representing events and time);causal calculus;belief calculus (belief revision); and modal logics.
Logics to model contradictory or inconsistent statements arising in multi-agent systems have also been designed, such as paraconsistent logics.","1. What is the purpose of logic? 
2. What is the difference between propositional and first-order logic? 
3. What is fuzzy logic? 
4. What is default logic? 
5. What is non-monotonic logic? 
6. What is circumscription? 
7. What is description logic? 
8. What is situation calculus? 
9. What is event calculus? 
10. What is fluent calculus?","1. Logic is used for knowledge representation and problem-solving. 
2. Propositional logic is simpler than first-order logic and can only express facts about objects and their relations. First-order logic adds quantifiers and predicates and can express facts about objects' properties. 
3. Fuzzy logic assigns a ""degree of truth"" (between 0 and 1) to vague statements, such as ""Alice is old"". 
4. Default logics, non-monotonic logics and circumscription are forms of logic designed to help with default reasoning and the qualification problem. 
5. Non-monotonic logics are logics that allow for contradictory or inconsistent statements arising in multi-agent systems. 
6. Circumscription is a logic designed to help with the qualification problem, which is the problem of identifying which facts are relevant to a given problem. 
7. Description logics are a type of logic designed to model knowledge in specific domains, such as the domain of agents and their interactions. 
8. Situation calculus is a type of logic designed to model knowledge in the domain of time and events. 
9. Event calculus is a type of logic designed to model knowledge in the domain of time and"
Artificial intelligence,Probabilistic methods for uncertain reasoning,"Many problems in AI (including in reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics.Bayesian networks
are a very general tool that can be used for various problems, including reasoning (using the Bayesian inference algorithm),learning (using the expectation-maximization algorithm),planning (using decision networks) and perception (using dynamic Bayesian networks).
Probabilistic algorithms can also be used for filtering, prediction, smoothing and finding explanations for streams of data, helping perception systems to analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).A key concept from the science of economics is ""utility"", a measure of how valuable something is to an intelligent agent. Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis,
and information value theory. These tools include models such as Markov decision processes, dynamic decision networks, game theory and mechanism design.",236,"Artificial intelligence
Probabilistic methods for uncertain reasoning

Many problems in AI (including in reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics.Bayesian networks
are a very general tool that can be used for various problems, including reasoning (using the Bayesian inference algorithm),learning (using the expectation-maximization algorithm),planning (using decision networks) and perception (using dynamic Bayesian networks).
Probabilistic algorithms can also be used for filtering, prediction, smoothing and finding explanations for streams of data, helping perception systems to analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).A key concept from the science of economics is ""utility"", a measure of how valuable something is to an intelligent agent. Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis,
and information value theory. These tools include models such as Markov decision processes, dynamic decision networks, game theory and mechanism design.","1. What is the Bayesian inference algorithm? 
2. What is the expectation-maximization algorithm? 
3. What is decision theory? 
4. What is information value theory?","1. The Bayesian inference algorithm is a tool that can be used for various problems, including reasoning, learning, planning, and perception. 
2. The expectation-maximization algorithm is a tool that can be used for learning. 
3. Decision theory is a tool that can be used for making choices and planning. 
4. Information value theory is a tool that can be used for measuring the value of information."
Artificial intelligence,Classifiers and statistical learning methods,"The simplest AI applications can be divided into two types: classifiers (""if shiny then diamond"") and controllers (""if diamond then pick up""). Controllers do, however, also classify conditions before inferring actions, and therefore classification forms a central part of many AI systems. Classifiers are functions that use pattern matching to determine the closest match. They can be tuned according to examples, making them very attractive for use in AI. These examples are known as observations or patterns. In supervised learning, each pattern belongs to a certain predefined class. A class is a decision that has to be made. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.A classifier can be trained in various ways; there are many statistical and machine learning approaches.
The decision tree is the simplest and most widely used symbolic machine learning algorithm.K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s.Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.
The naive Bayes classifier is reportedly the ""most widely used learner"" at Google, due in part to its scalability.Neural networks are also used for classification.Classifier performance depends greatly on the characteristics of the data to be classified, such as the dataset size, distribution of samples across classes, dimensionality, and the level of noise. Model-based classifiers perform well if the assumed model is an extremely good fit for the actual data. Otherwise, if no matching model is available, and if accuracy (rather than speed or scalability) is the sole concern, conventional wisdom is that discriminative classifiers (especially SVM) tend to be more accurate than model-based classifiers such as ""naive Bayes"" on most practical data sets.",394,"Artificial intelligence
Classifiers and statistical learning methods

The simplest AI applications can be divided into two types: classifiers (""if shiny then diamond"") and controllers (""if diamond then pick up""). Controllers do, however, also classify conditions before inferring actions, and therefore classification forms a central part of many AI systems. Classifiers are functions that use pattern matching to determine the closest match. They can be tuned according to examples, making them very attractive for use in AI. These examples are known as observations or patterns. In supervised learning, each pattern belongs to a certain predefined class. A class is a decision that has to be made. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.A classifier can be trained in various ways; there are many statistical and machine learning approaches.
The decision tree is the simplest and most widely used symbolic machine learning algorithm.K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s.Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.
The naive Bayes classifier is reportedly the ""most widely used learner"" at Google, due in part to its scalability.Neural networks are also used for classification.Classifier performance depends greatly on the characteristics of the data to be classified, such as the dataset size, distribution of samples across classes, dimensionality, and the level of noise. Model-based classifiers perform well if the assumed model is an extremely good fit for the actual data. Otherwise, if no matching model is available, and if accuracy (rather than speed or scalability) is the sole concern, conventional wisdom is that discriminative classifiers (especially SVM) tend to be more accurate than model-based classifiers such as ""naive Bayes"" on most practical data sets.","1. What are the two types of AI applications?
2. What is a classifier?
3. What is a data set?
4. What is a class label?
5. What is a model-based classifier?
6. What is a discriminative classifier?","1. The two types of AI applications are classifiers and controllers.
2. A classifier is a function that uses pattern matching to determine the closest match.
3. A data set is a collection of observations and their class labels.
4. A class label is a decision that has to be made.
5. A model-based classifier is a classifier that uses a model to make decisions.
6. A discriminative classifier is a classifier that uses a discriminative function to make decisions."
Artificial intelligence,Artificial neural networks,"Neural networks
were inspired by the architecture of neurons in the human brain. A simple ""neuron"" N accepts input from other neurons, each of which, when activated (or ""fired""), casts a weighted ""vote"" for or against whether neuron N should itself activate. Learning requires an algorithm to adjust these weights based on the training data; one simple algorithm (dubbed ""fire together, wire together"") is to increase the weight between two connected neurons when the activation of one triggers the successful activation of another. Neurons have a continuous spectrum of activation; in addition, neurons can process inputs in a nonlinear way rather than weighing straightforward votes.
Modern neural networks model complex relationships between inputs and outputs and find patterns in data. They can learn continuous functions and even digital logical operations. Neural networks can be viewed as a type of mathematical optimization – they perform gradient descent on a multi-dimensional topology that was created by training the network. The most common training technique is the backpropagation algorithm.
Other learning techniques for neural networks are Hebbian learning (""fire together, wire together""), GMDH or competitive learning.The main categories of networks are acyclic or feedforward neural networks (where the signal passes in only one direction) and recurrent neural networks (which allow feedback and short-term memories of previous input events). Among the most popular feedforward networks are perceptrons, multi-layer perceptrons and radial basis networks.",299,"Artificial intelligence
Artificial neural networks

Neural networks
were inspired by the architecture of neurons in the human brain. A simple ""neuron"" N accepts input from other neurons, each of which, when activated (or ""fired""), casts a weighted ""vote"" for or against whether neuron N should itself activate. Learning requires an algorithm to adjust these weights based on the training data; one simple algorithm (dubbed ""fire together, wire together"") is to increase the weight between two connected neurons when the activation of one triggers the successful activation of another. Neurons have a continuous spectrum of activation; in addition, neurons can process inputs in a nonlinear way rather than weighing straightforward votes.
Modern neural networks model complex relationships between inputs and outputs and find patterns in data. They can learn continuous functions and even digital logical operations. Neural networks can be viewed as a type of mathematical optimization – they perform gradient descent on a multi-dimensional topology that was created by training the network. The most common training technique is the backpropagation algorithm.
Other learning techniques for neural networks are Hebbian learning (""fire together, wire together""), GMDH or competitive learning.The main categories of networks are acyclic or feedforward neural networks (where the signal passes in only one direction) and recurrent neural networks (which allow feedback and short-term memories of previous input events). Among the most popular feedforward networks are perceptrons, multi-layer perceptrons and radial basis networks.","1. What is the main difference between a neuron and a neural network?
2. How do neural networks learn?
3. What is the backpropagation algorithm?
4. What is the difference between a feedforward neural network and a recurrent neural network?","1. A neuron is a single unit in a neural network. Neural networks are composed of many interconnected neurons.
2. Neural networks learn by adjusting the weights between neurons. This is done through a process called gradient descent, which is a type of mathematical optimization.
3. The backpropagation algorithm is a common training technique for neural networks. It adjusts the weights between neurons based on the activation of other neurons.
4. Feedforward neural networks are networks where the signal passes in only one direction. Recurrent neural networks are networks that allow feedback and short-term memories of previous input events."
Artificial intelligence,Deep learning,"Deep learning
uses several layers of neurons between the network's inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits or letters or faces. Deep learning has drastically improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, image classification and others.
Deep learning often uses convolutional neural networks for many or all of its layers. In a convolutional layer, each neuron receives input from only a restricted area of the previous layer called the neuron's receptive field. This can substantially reduce the number of weighted connections between neurons, and creates a hierarchy similar to the organization of the animal visual cortex.In a recurrent neural network (RNN) the signal will propagate through a layer more than once;
thus, an RNN is an example of deep learning.
RNNs can be trained by gradient descent,
however long-term gradients which are back-propagated can ""vanish"" (that is, they can tend to zero) or ""explode"" (that is, they can tend to infinity), known as the vanishing gradient problem.
The long short term memory (LSTM) technique can prevent this in most cases.",282,"Artificial intelligence
Deep learning

Deep learning
uses several layers of neurons between the network's inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits or letters or faces. Deep learning has drastically improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, image classification and others.
Deep learning often uses convolutional neural networks for many or all of its layers. In a convolutional layer, each neuron receives input from only a restricted area of the previous layer called the neuron's receptive field. This can substantially reduce the number of weighted connections between neurons, and creates a hierarchy similar to the organization of the animal visual cortex.In a recurrent neural network (RNN) the signal will propagate through a layer more than once;
thus, an RNN is an example of deep learning.
RNNs can be trained by gradient descent,
however long-term gradients which are back-propagated can ""vanish"" (that is, they can tend to zero) or ""explode"" (that is, they can tend to infinity), known as the vanishing gradient problem.
The long short term memory (LSTM) technique can prevent this in most cases.","1. What is deep learning?
2. What are the benefits of deep learning?
3. What are some of the applications of deep learning?
4. What is the vanishing gradient problem?
5. What is the long short term memory (LSTM) technique?","1. Deep learning is a type of machine learning that uses several layers of neurons between the network's inputs and outputs.
2. The benefits of deep learning include drastically improved performance in many important subfields of artificial intelligence, including computer vision, speech recognition, image classification, and others.
3. Some applications of deep learning include:

-Computer vision: Deep learning has been used to improve the accuracy of object recognition and identification, as well as to create more realistic 3D models from images.
-Speech recognition: Deep learning has been used to create more accurate speech recognition systems, which can understand a wider range of accents and dialects.
-Image classification: Deep learning has been used to create more accurate image classification systems, which can more accurately identify the contents of an image.
4. The vanishing gradient problem is a problem that can occur in deep learning networks, where the gradients that are back-propagated tend to zero or explode.
5. The long short term memory (LSTM) technique is a technique that can be used to prevent the vanishing gradient problem in most cases."
Artificial intelligence,Specialized languages and hardware,"Specialized languages for artificial intelligence have been developed, such as Lisp, Prolog, TensorFlow and many others. Hardware developed for AI includes AI accelerators and neuromorphic computing. By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced central processing unit (CPUs) as the dominant means to train large-scale commercial cloud AI.",82,"Artificial intelligence
Specialized languages and hardware

Specialized languages for artificial intelligence have been developed, such as Lisp, Prolog, TensorFlow and many others. Hardware developed for AI includes AI accelerators and neuromorphic computing. By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced central processing unit (CPUs) as the dominant means to train large-scale commercial cloud AI.","1. What is Lisp?
2. What is Prolog?
3. What is TensorFlow?
4. What is a GPU?
5. What is a CPU?","1. Lisp is a specialized language for artificial intelligence.
2. Prolog is a specialized language for artificial intelligence.
3. TensorFlow is a specialized language for artificial intelligence.
4. A GPU is a specialized hardware for artificial intelligence.
5. A CPU is a specialized hardware for artificial intelligence."
Artificial intelligence,Applications,"AI is relevant to any intellectual task. Modern artificial intelligence techniques are pervasive and are too numerous to list here. Frequently, when a technique reaches mainstream use, it is no longer considered artificial intelligence; this phenomenon is described as the AI effect.In the 2010s, AI applications were at the heart of the most commercially successful areas of computing, and have become a ubiquitous feature of daily life. AI is used in search engines (such as Google Search),
targeting online advertisements, recommendation systems (offered by Netflix, YouTube or Amazon), driving internet traffic, targeted advertising (AdSense, Facebook), virtual assistants (such as Siri or Alexa), autonomous vehicles (including drones, ADAS and self-driving cars), automatic language translation (Microsoft Translator, Google Translate), facial recognition (Apple's Face ID or Microsoft's DeepFace), image labeling (used by Facebook, Apple's iPhoto and TikTok), spam filtering and chatbots (such as ChatGPT).
There are also thousands of successful AI applications used to solve problems for specific industries or institutions. A few examples are energy storage, deepfakes, medical diagnosis, military logistics, foreign policy, or supply chain management.
Game playing has been a test of AI's strength since the 1950s. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997. In 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin.
In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. Other programs handle imperfect-information games; such as for poker at a superhuman level, Pluribus and Cepheus. DeepMind in the 2010s developed a ""generalized artificial intelligence"" that could learn many diverse Atari games on its own.DeepMind's AlphaFold 2 (2020) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein. Other applications predict the result of judicial decisions, create art (such as poetry or painting) and prove mathematical theorems.
Generative AI gained widespread prominence in the 2020s. ""Large language model"" systems such as GPT-3 (2020), with 175 billion parameters, matched human performance on pre-existing benchmarks, albeit without the system attaining a commonsense understanding of the contents of the benchmarks. A Pew Research poll conducted in March 2023 found that 14% of Americans adults had tried ChatGPT, a large language model fine-tuned using reinforcement learning from human feedback.  While estimates varied wildly, Goldman Sachs suggested in 2023 that generative language AI could increase global GDP by 7% in the next ten years.In 2023, the increasing realism and ease-of-use of AI-based text-to-image generators such as Midjourney, DALL-E, and Stable Diffusion sparked a trend of viral AI-generated photos. Widespread attention was gained by a fake photo of Pope Francis wearing a white puffer coat, the fictional arrest of Donald Trump, and a hoax of an attack on the Pentagon, as well as the usage in professional creative arts.",700,"Artificial intelligence
Applications

AI is relevant to any intellectual task. Modern artificial intelligence techniques are pervasive and are too numerous to list here. Frequently, when a technique reaches mainstream use, it is no longer considered artificial intelligence; this phenomenon is described as the AI effect.In the 2010s, AI applications were at the heart of the most commercially successful areas of computing, and have become a ubiquitous feature of daily life. AI is used in search engines (such as Google Search),
targeting online advertisements, recommendation systems (offered by Netflix, YouTube or Amazon), driving internet traffic, targeted advertising (AdSense, Facebook), virtual assistants (such as Siri or Alexa), autonomous vehicles (including drones, ADAS and self-driving cars), automatic language translation (Microsoft Translator, Google Translate), facial recognition (Apple's Face ID or Microsoft's DeepFace), image labeling (used by Facebook, Apple's iPhoto and TikTok), spam filtering and chatbots (such as ChatGPT).
There are also thousands of successful AI applications used to solve problems for specific industries or institutions. A few examples are energy storage, deepfakes, medical diagnosis, military logistics, foreign policy, or supply chain management.
Game playing has been a test of AI's strength since the 1950s. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997. In 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin.
In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. Other programs handle imperfect-information games; such as for poker at a superhuman level, Pluribus and Cepheus. DeepMind in the 2010s developed a ""generalized artificial intelligence"" that could learn many diverse Atari games on its own.DeepMind's AlphaFold 2 (2020) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein. Other applications predict the result of judicial decisions, create art (such as poetry or painting) and prove mathematical theorems.
Generative AI gained widespread prominence in the 2020s. ""Large language model"" systems such as GPT-3 (2020), with 175 billion parameters, matched human performance on pre-existing benchmarks, albeit without the system attaining a commonsense understanding of the contents of the benchmarks. A Pew Research poll conducted in March 2023 found that 14% of Americans adults had tried ChatGPT, a large language model fine-tuned using reinforcement learning from human feedback.  While estimates varied wildly, Goldman Sachs suggested in 2023 that generative language AI could increase global GDP by 7% in the next ten years.In 2023, the increasing realism and ease-of-use of AI-based text-to-image generators such as Midjourney, DALL-E, and Stable Diffusion sparked a trend of viral AI-generated photos. Widespread attention was gained by a fake photo of Pope Francis wearing a white puffer coat, the fictional arrest of Donald Trump, and a hoax of an attack on the Pentagon, as well as the usage in professional creative arts.","1. What are some of the most common applications of artificial intelligence?
2. How do artificial intelligence systems beat humans at games such as chess and Jeopardy!?
3. What is generative AI and why is it becoming increasingly popular?
4. What are some of the most famous AI-generated photos?","1. Some of the most common applications of artificial intelligence include search engines, online advertising, virtual assistants, facial recognition, image labeling, and chatbots.
2. Artificial intelligence systems beat humans at games such as chess and Jeopardy! by using a combination of powerful algorithms and large amounts of data.
3. Generative AI is a type of artificial intelligence that is becoming increasingly popular because it allows machines to learn and create on their own.
4. Some of the most famous AI-generated photos are a fake photo of Pope Francis wearing a white puffer coat, the fictional arrest of Donald Trump, and a hoax of an attack on the Pentagon."
Artificial intelligence,Intellectual property,"In 2019, WIPO reported that AI was the most prolific emerging technology in terms of the number of patent applications and granted patents, the Internet of things was estimated to be the largest in terms of market size. It was followed, again in market size, by big data technologies, robotics, AI, 3D printing and the fifth generation of mobile services (5G). Since AI emerged in the 1950s, 340,000 AI-related patent applications were filed by innovators and 1.6 million scientific papers have been published by researchers, with the majority of all AI-related patent filings published since 2013. Companies represent 26 out of the top 30 AI patent applicants, with universities or public research organizations accounting for the remaining four. The ratio of scientific papers to inventions has significantly decreased from 8:1 in 2010 to 3:1 in 2016, which is attributed to be indicative of a shift from theoretical research to the use of AI technologies in commercial products and services. Machine learning is the dominant AI technique disclosed in patents and is included in more than one-third of all identified inventions (134,777 machine learning patents filed for a total of 167,038 AI patents filed in 2016), with computer vision being the most popular functional application. AI-related patents not only disclose AI techniques and applications, they often also refer to an application field or industry. Twenty application fields were identified in 2016 and included, in order of magnitude: telecommunications (15 percent), transportation (15 percent), life and medical sciences (12 percent), and personal devices, computing and human–computer interaction (11 percent). Other sectors included banking, entertainment, security, industry and manufacturing, agriculture, and networks (including social networks, smart cities and the Internet of things). IBM has the largest portfolio of AI patents with 8,290 patent applications, followed by Microsoft with 5,930 patent applications.",377,"Artificial intelligence
Intellectual property

In 2019, WIPO reported that AI was the most prolific emerging technology in terms of the number of patent applications and granted patents, the Internet of things was estimated to be the largest in terms of market size. It was followed, again in market size, by big data technologies, robotics, AI, 3D printing and the fifth generation of mobile services (5G). Since AI emerged in the 1950s, 340,000 AI-related patent applications were filed by innovators and 1.6 million scientific papers have been published by researchers, with the majority of all AI-related patent filings published since 2013. Companies represent 26 out of the top 30 AI patent applicants, with universities or public research organizations accounting for the remaining four. The ratio of scientific papers to inventions has significantly decreased from 8:1 in 2010 to 3:1 in 2016, which is attributed to be indicative of a shift from theoretical research to the use of AI technologies in commercial products and services. Machine learning is the dominant AI technique disclosed in patents and is included in more than one-third of all identified inventions (134,777 machine learning patents filed for a total of 167,038 AI patents filed in 2016), with computer vision being the most popular functional application. AI-related patents not only disclose AI techniques and applications, they often also refer to an application field or industry. Twenty application fields were identified in 2016 and included, in order of magnitude: telecommunications (15 percent), transportation (15 percent), life and medical sciences (12 percent), and personal devices, computing and human–computer interaction (11 percent). Other sectors included banking, entertainment, security, industry and manufacturing, agriculture, and networks (including social networks, smart cities and the Internet of things). IBM has the largest portfolio of AI patents with 8,290 patent applications, followed by Microsoft with 5,930 patent applications.","1. What is the most prolific emerging technology in terms of the number of patent applications and granted patents?
2. What is the largest in terms of market size?
3. What are the top five in terms of market size?
4. What has been the trend in terms of the ratio of scientific papers to inventions?
5. What is the dominant AI technique disclosed in patents?
6. What are the most popular functional applications?
7. What are the twenty application fields identified in","1. AI is the most prolific emerging technology in terms of the number of patent applications and granted patents.
2. The Internet of things is the largest in terms of market size.
3. The top five in terms of market size are: the Internet of things, big data technologies, robotics, AI, 3D printing.
4. The trend in terms of the ratio of scientific papers to inventions has significantly decreased from 8:1 in 2010 to 3:1 in 2016.
5. Machine learning is the dominant AI technique disclosed in patents and is included in more than one-third of all identified inventions.
6. The most popular functional applications are: machine learning, computer vision, natural language processing, knowledge representation and inference, and planning and scheduling.
7. The twenty application fields identified in 2016 are: telecommunications, transportation, life and medical sciences, personal devices, computing and human–computer interaction, banking, entertainment, security, industry and manufacturing, agriculture, and networks."
Artificial intelligence,Defining artificial intelligence,"Alan Turing wrote in 1950 ""I propose to consider the question 'can machines think'?""
He advised changing the question from whether a machine ""thinks"", to ""whether or not it is possible for machinery to show intelligent behaviour"".
He devised the Turing test, which measures the ability of a machine to simulate human conversation. Since we can only observe the behavior of the machine, it does not matter if it is ""actually"" thinking or literally has a ""mind"". Turing notes that we can not determine these things about other people but ""it is usual to have a polite convention that everyone thinks""Russell and Norvig agree with Turing that AI must be defined in terms of ""acting"" and not ""thinking"". However, they are critical that the test compares machines to people. ""Aeronautical engineering texts,"" they wrote, ""do not define the goal of their field as making 'machines that fly so exactly like pigeons that they can fool other pigeons.'"" AI founder John McCarthy agreed, writing that ""Artificial intelligence is not, by definition, simulation of human intelligence"".McCarthy defines intelligence as ""the computational part of the ability to achieve goals in the world."" Another AI founder, Marvin Minsky similarly defines it as ""the ability to solve hard problems"". These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the ""intelligence"" of the machine—and no other philosophical discussion is required, or may not even be possible.
A definition that has also been adopted by Google – major practitionary in the field of AI.
This definition stipulated the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence.",373,"Artificial intelligence
Defining artificial intelligence

Alan Turing wrote in 1950 ""I propose to consider the question 'can machines think'?""
He advised changing the question from whether a machine ""thinks"", to ""whether or not it is possible for machinery to show intelligent behaviour"".
He devised the Turing test, which measures the ability of a machine to simulate human conversation. Since we can only observe the behavior of the machine, it does not matter if it is ""actually"" thinking or literally has a ""mind"". Turing notes that we can not determine these things about other people but ""it is usual to have a polite convention that everyone thinks""Russell and Norvig agree with Turing that AI must be defined in terms of ""acting"" and not ""thinking"". However, they are critical that the test compares machines to people. ""Aeronautical engineering texts,"" they wrote, ""do not define the goal of their field as making 'machines that fly so exactly like pigeons that they can fool other pigeons.'"" AI founder John McCarthy agreed, writing that ""Artificial intelligence is not, by definition, simulation of human intelligence"".McCarthy defines intelligence as ""the computational part of the ability to achieve goals in the world."" Another AI founder, Marvin Minsky similarly defines it as ""the ability to solve hard problems"". These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the ""intelligence"" of the machine—and no other philosophical discussion is required, or may not even be possible.
A definition that has also been adopted by Google – major practitionary in the field of AI.
This definition stipulated the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence.","1. What is the Turing test?
2. What is the goal of AI, according to Russell and Norvig?
3. What is the definition of intelligence, according to McCarthy?
4. What is the definition of intelligence, according to Minsky?
5. What is the definition of intelligence, according to Google?","1. The Turing test is a measure of the ability of a machine to simulate human conversation.
2. The goal of AI, according to Russell and Norvig, is to create machines that can solve hard problems.
3. Intelligence is the ability to solve hard problems, according to McCarthy.
4. Intelligence is the ability to synthesize information, according to Minsky.
5. Intelligence is the ability to achieve goals in the world, according to Google."
Artificial intelligence,Evaluating approaches to AI,"No established unifying theory or paradigm has guided AI research for most of its history. The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term ""artificial intelligence"" to mean ""machine learning with neural networks""). This approach is mostly sub-symbolic, neat, soft and narrow (see below). Critics argue that these questions may have to be revisited by future generations of AI researchers.",107,"Artificial intelligence
Evaluating approaches to AI

No established unifying theory or paradigm has guided AI research for most of its history. The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term ""artificial intelligence"" to mean ""machine learning with neural networks""). This approach is mostly sub-symbolic, neat, soft and narrow (see below). Critics argue that these questions may have to be revisited by future generations of AI researchers.","1. What is the main difference between artificial intelligence and machine learning?
2. What is the significance of the 2010s in the history of AI?
3. What is the main criticism of statistical machine learning?","1. The main difference between artificial intelligence and machine learning is that artificial intelligence is a more general term that includes machine learning, while machine learning is a subset of artificial intelligence.
2. The 2010s were a significant decade in the history of AI because it was when statistical machine learning began to eclipse all other approaches.
3. The main criticism of statistical machine learning is that it is mostly sub-symbolic, neat, soft, and narrow."
Artificial intelligence,Symbolic AI and its limits,"Symbolic AI (or ""GOFAI"") simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at ""intelligent"" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: ""A physical symbol system has the necessary and sufficient means of general intelligent action.""However, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level ""intelligent"" tasks were easy for AI, but low level ""instinctive"" tasks were extremely difficult.
Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a ""feel"" for the situation, rather than explicit symbolic knowledge.
Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree.The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence, in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches.",327,"Artificial intelligence
Symbolic AI and its limits

Symbolic AI (or ""GOFAI"") simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at ""intelligent"" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: ""A physical symbol system has the necessary and sufficient means of general intelligent action.""However, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level ""intelligent"" tasks were easy for AI, but low level ""instinctive"" tasks were extremely difficult.
Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a ""feel"" for the situation, rather than explicit symbolic knowledge.
Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree.The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence, in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches.","1. What is the physical symbol systems hypothesis?
2. What is Moravec's paradox?
3. What is Hubert Dreyfus' argument?
4. What is the issue with sub-symbolic AI?
5. What is neuro-symbolic AI?","1. The physical symbol systems hypothesis is the idea that a physical symbol system has the necessary and sufficient means of general intelligent action.
2. Moravec's paradox is the discovery that high-level ""intelligent"" tasks were easy for AI, but low level ""instinctive"" tasks were extremely difficult.
3. Hubert Dreyfus' argument is that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a ""feel"" for the situation, rather than explicit symbolic knowledge.
4. The issue with sub-symbolic AI is that it can be difficult or impossible to understand why a modern statistical AI program made a particular decision.
5. Neuro-symbolic AI is an attempt to bridge the gap between symbolic and sub-symbolic AI."
Artificial intelligence,Neat vs. scruffy,"""Neats"" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). ""Scruffies"" expect that it necessarily requires solving a large number of unrelated problems. Neats defend their programs with theoretical rigor, scruffies rely only on incremental testing to see if they work. This issue was actively discussed in the 70s and 80s,
but eventually was seen as irrelevant. In the 1990s mathematical methods and solid scientific standards became the norm, a transition that Russell and Norvig termed in 2003 as ""the victory of the neats"". However in 2020 they wrote ""deep learning may represent a resurgence of the scruffies"". Modern AI has elements of both.",153,"Artificial intelligence
Neat vs. scruffy

""Neats"" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). ""Scruffies"" expect that it necessarily requires solving a large number of unrelated problems. Neats defend their programs with theoretical rigor, scruffies rely only on incremental testing to see if they work. This issue was actively discussed in the 70s and 80s,
but eventually was seen as irrelevant. In the 1990s mathematical methods and solid scientific standards became the norm, a transition that Russell and Norvig termed in 2003 as ""the victory of the neats"". However in 2020 they wrote ""deep learning may represent a resurgence of the scruffies"". Modern AI has elements of both.","1. What is the difference between neats and scruffies?
2. What caused the difference between neats and scruffies to become irrelevant?
3. What do neats and scruffies have in common?
4. What do Russell and Norvig say about the difference between neats and scruffies in 2020?","1. Neats are people who hope that intelligent behavior can be described using simple, elegant principles. Scruffies are people who expect that it necessarily requires solving a large number of unrelated problems.
2. The difference between neats and scruffies became irrelevant because mathematical methods and solid scientific standards became the norm in the 1990s.
3. Neats and scruffies have in common that they both rely on incremental testing to see if their programs work.
4. In 2020, Russell and Norvig say that the difference between neats and scruffies may represent a resurgence of scruffies."
Artificial intelligence,Soft vs. hard computing,"Finding a provably correct or optimal solution is intractable for many important problems. Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 80s and most successful AI programs in the 21st century are examples of soft computing with neural networks.",84,"Artificial intelligence
Soft vs. hard computing

Finding a provably correct or optimal solution is intractable for many important problems. Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 80s and most successful AI programs in the 21st century are examples of soft computing with neural networks.","1. What is soft computing? 
2. What are some of the techniques used in soft computing? 
3. What is the difference between soft and hard computing? 
4. What are some successful AI programs that use soft computing?","1. Soft computing is a set of techniques that are tolerant of imprecision, uncertainty, partial truth and approximation. 
2. Some of the techniques used in soft computing are genetic algorithms, fuzzy logic and neural networks. 
3. The difference between soft and hard computing is that soft computing is tolerant of imprecision, uncertainty, partial truth and approximation, while hard computing is not. 
4. Some successful AI programs that use soft computing are neural networks."
Artificial intelligence,Narrow vs. general AI,"AI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence (general AI) directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals.
General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The experimental sub-field of artificial general intelligence studies this area exclusively.",103,"Artificial intelligence
Narrow vs. general AI

AI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence (general AI) directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals.
General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The experimental sub-field of artificial general intelligence studies this area exclusively.","1. What is the difference between general AI and narrow AI?
2. Why is general AI difficult to define and difficult to measure?
3. What are the benefits of focusing on specific problems with specific solutions?","1. General AI is the ability of a computer system to perform any intellectual task that a human can. Narrow AI is a computer system that is specifically designed to do one task, such as recognizing objects or translating languages.
2. General AI is difficult to define and difficult to measure because it is not clear what tasks are considered ""intellectual."" In addition, there is no agreed-upon way to measure general intelligence.
3. The benefits of focusing on specific problems with specific solutions are that these systems are easier to design and test, and they can be optimized for a particular task."
Artificial intelligence,"Machine consciousness, sentience and mind","The philosophy of mind does not know whether a machine can have a mind, consciousness and mental states, in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field. Stuart Russell and Peter Norvig observe that most AI researchers ""don't care about the [philosophy of AI] – as long as the program works, they don't care whether you call it a simulation of intelligence or real intelligence."" However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction.",150,"Artificial intelligence
Machine consciousness, sentience and mind

The philosophy of mind does not know whether a machine can have a mind, consciousness and mental states, in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field. Stuart Russell and Peter Norvig observe that most AI researchers ""don't care about the [philosophy of AI] – as long as the program works, they don't care whether you call it a simulation of intelligence or real intelligence."" However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction.","1. What is the philosophy of mind?
2. What is the issue of whether a machine can have a mind?
3. What is the internal experience of the machine?
4. What are the goals of mainstream AI research?
5. Why do most AI researchers ""not care"" about the philosophy of AI?
6. What is the central question at issue in artificial intelligence in fiction?","1. The philosophy of mind is the philosophical study of the mind.
2. The issue of whether a machine can have a mind is the question of whether a machine can be conscious and have mental states in the same sense that human beings do.
3. The internal experience of the machine is the experience of the machine itself, rather than its external behavior.
4. The goals of mainstream AI research are to create programs that are effective and efficient in achieving the goals of the field, whether or not the programs have minds in the same sense that human beings do.
5. Most AI researchers ""don't care"" about the philosophy of AI because it does not affect the goals of the field.
6. The central question at issue in artificial intelligence in fiction is the question of whether a machine can have a mind."
Artificial intelligence,Consciousness,"David Chalmers identified two problems in understanding the mind, which he named the ""hard"" and ""easy"" problems of consciousness. The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). Human information processing is easy to explain, however, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like.",155,"Artificial intelligence
Consciousness

David Chalmers identified two problems in understanding the mind, which he named the ""hard"" and ""easy"" problems of consciousness. The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). Human information processing is easy to explain, however, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like.","1. What is the ""hard"" problem of consciousness?
2. What is the ""easy"" problem of consciousness?
3. Why is it difficult to explain human subjective experience?","1. The ""hard"" problem of consciousness is explaining how subjective experience feels or why it should feel like anything at all.
2. The ""easy"" problem is understanding how the brain processes signals, makes plans and controls behavior.
3. Human subjective experience is difficult to explain because it is hard to imagine what it would be like to not have that experience."
Artificial intelligence,Computationalism and functionalism,"Computationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind–body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.Philosopher John Searle characterized this position as ""strong AI"": ""The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.""
Searle counters this assertion with his Chinese room argument, which attempts to show that, even if a machine perfectly simulates human behavior, there is still no reason to suppose it also has a mind.",184,"Artificial intelligence
Computationalism and functionalism

Computationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind–body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.Philosopher John Searle characterized this position as ""strong AI"": ""The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.""
Searle counters this assertion with his Chinese room argument, which attempts to show that, even if a machine perfectly simulates human behavior, there is still no reason to suppose it also has a mind.","1. What is the mind-body problem? 
2. What is computationalism? 
3. What is strong AI? 
4. What is the Chinese room argument?","1. The mind-body problem is the problem of explaining how mental states (such as thoughts and feelings) are related to physical states (such as brain activity).
2. Computationalism is the position that the human mind is an information processing system and that thinking is a form of computing.
3. Strong AI is the position that the relationship between mind and body is similar or identical to the relationship between software and hardware.
4. The Chinese room argument is an argument against strong AI proposed by philosopher John Searle."
Artificial intelligence,Robot rights,"If a machine has a mind and subjective experience, then it may also have sentience (the ability to feel), and if so it could also suffer; it has been argued that this could entitle it to certain rights.
Any hypothetical robot rights would lie on a spectrum with animal rights and human rights.
This issue has been considered in fiction for centuries,
and is now being considered by, for example, California's Institute for the Future; however, critics argue that the discussion is premature.",103,"Artificial intelligence
Robot rights

If a machine has a mind and subjective experience, then it may also have sentience (the ability to feel), and if so it could also suffer; it has been argued that this could entitle it to certain rights.
Any hypothetical robot rights would lie on a spectrum with animal rights and human rights.
This issue has been considered in fiction for centuries,
and is now being considered by, for example, California's Institute for the Future; however, critics argue that the discussion is premature.","1. What is the Institute for the Future?
2. What are the arguments for and against robot rights?
3. What are the implications of giving robots rights?","1. The Institute for the Future is a California-based think tank that considers the future of technology and its implications.
2. The arguments for robot rights are that if a machine has a mind and subjective experience, then it may also have sentience (the ability to feel), and if so it could also suffer; it has been argued that this could entitle it to certain rights.
Any hypothetical robot rights would lie on a spectrum with animal rights and human rights.

3. The implications of giving robots rights are that it could change the way we think about the relationship between humans and machines, and it could have legal implications."
Artificial intelligence,Superintelligence,"A superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind.If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to recursive self-improvement.
Its intelligence would increase exponentially in an intelligence explosion and could dramatically surpass humans. Science fiction writer Vernor Vinge named this scenario the ""singularity"".
Because it is difficult or impossible to know the limits of intelligence or the capabilities of superintelligent machines, the technological singularity is an occurrence beyond which events are unpredictable or even unfathomable.Robot designer Hans Moravec, cyberneticist Kevin Warwick, and inventor Ray Kurzweil have predicted that humans and machines will merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in Aldous Huxley and Robert Ettinger.Edward Fredkin argues that ""artificial intelligence is the next stage in evolution"", an idea first proposed by Samuel Butler's ""Darwin among the Machines"" as far back as 1863, and expanded upon by George Dyson in his book of the same name in 1998.",260,"Artificial intelligence
Superintelligence

A superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind.If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to recursive self-improvement.
Its intelligence would increase exponentially in an intelligence explosion and could dramatically surpass humans. Science fiction writer Vernor Vinge named this scenario the ""singularity"".
Because it is difficult or impossible to know the limits of intelligence or the capabilities of superintelligent machines, the technological singularity is an occurrence beyond which events are unpredictable or even unfathomable.Robot designer Hans Moravec, cyberneticist Kevin Warwick, and inventor Ray Kurzweil have predicted that humans and machines will merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in Aldous Huxley and Robert Ettinger.Edward Fredkin argues that ""artificial intelligence is the next stage in evolution"", an idea first proposed by Samuel Butler's ""Darwin among the Machines"" as far back as 1863, and expanded upon by George Dyson in his book of the same name in 1998.","1. What is a superintelligence?
2. What could happen if research into artificial general intelligence produced sufficiently intelligent software?
3. What is the technological singularity?
4. What are the roots of transhumanism?
5. What does Edward Fredkin argue?","1. A superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind.
2. If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to recursive self-improvement.
3. The technological singularity is an occurrence beyond which events are unpredictable or even unfathomable.
4. The roots of transhumanism can be found in Aldous Huxley and Robert Ettinger.
5. Edward Fredkin argues that ""artificial intelligence is the next stage in evolution"", an idea first proposed by Samuel Butler's ""Darwin among the Machines"" as far back as 1863, and expanded upon by George Dyson in his book of the same name in 1998."
Artificial intelligence,Technological unemployment,"In the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that ""we're in uncharted territory"" with AI. A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed. Risk estimates vary; for example, in the 2010s Michael Osborne and Carl Benedikt Frey estimated 47% of U.S. jobs are at ""high risk"" of potential automation, while an OECD report classified only 9% of U.S. jobs as ""high risk"". The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology (rather than social policy) creates unemployment (as opposed to redundancies).Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that ""the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution"" is ""worth taking seriously"". Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.",262,"Artificial intelligence
Technological unemployment

In the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that ""we're in uncharted territory"" with AI. A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed. Risk estimates vary; for example, in the 2010s Michael Osborne and Carl Benedikt Frey estimated 47% of U.S. jobs are at ""high risk"" of potential automation, while an OECD report classified only 9% of U.S. jobs as ""high risk"". The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology (rather than social policy) creates unemployment (as opposed to redundancies).Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that ""the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution"" is ""worth taking seriously"". Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.","1. What are the economists' opinions about the increasing use of robots and AI?
2. What are the risks associated with the increasing use of robots and AI?
3. What are the potential consequences of the increasing use of robots and AI?","1. The economists surveyed disagree about the effect of the increasing use of robots and AI on long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed.
2. The risks associated with the increasing use of robots and AI include the possibility of substantial increases in long-term unemployment and a net loss in productivity.
3. The potential consequences of the increasing use of robots and AI include increased long-term unemployment and a net loss in productivity."
Artificial intelligence,Bad actors and weaponized AI,"AI provides a number of tools that are particularly useful for authoritarian governments: smart spyware, face recognition and voice recognition allow widespread surveillance; such surveillance allows machine learning to classify potential enemies of the state and can prevent them from hiding; recommendation systems can precisely target propaganda and misinformation for maximum effect; deepfakes aid in producing misinformation; advanced AI can make centralized decision making more competitive with liberal and decentralized systems such as markets.Terrorists, criminals and rogue states may use other forms of weaponized AI such as advanced digital warfare and lethal autonomous weapons. By 2015, over fifty countries were reported to be researching battlefield robots.Machine-learning AI is also able to design tens of thousands of toxic molecules in a matter of hours.",152,"Artificial intelligence
Bad actors and weaponized AI

AI provides a number of tools that are particularly useful for authoritarian governments: smart spyware, face recognition and voice recognition allow widespread surveillance; such surveillance allows machine learning to classify potential enemies of the state and can prevent them from hiding; recommendation systems can precisely target propaganda and misinformation for maximum effect; deepfakes aid in producing misinformation; advanced AI can make centralized decision making more competitive with liberal and decentralized systems such as markets.Terrorists, criminals and rogue states may use other forms of weaponized AI such as advanced digital warfare and lethal autonomous weapons. By 2015, over fifty countries were reported to be researching battlefield robots.Machine-learning AI is also able to design tens of thousands of toxic molecules in a matter of hours.","1. What are some of the ways in which AI can be used by authoritarian governments?
2. How can terrorists, criminals, and rogue states use AI?
3. What are some of the dangers of weaponized AI?","1. AI can be used by authoritarian governments for widespread surveillance, classifying potential enemies, and producing misinformation.
2. Terrorists, criminals, and rogue states can use AI for advanced digital warfare and lethal autonomous weapons.
3. The dangers of weaponized AI include the ability to produce toxic molecules and the ability to make centralized decision making more competitive with liberal and decentralized systems."
Artificial intelligence,Algorithmic bias,"AI programs can become biased after learning from real-world data. It is not typically introduced by the system designers but is learned by the program, and thus the programmers are often unaware that the bias exists.
Bias can be inadvertently introduced by the way training data is selected.
It can also emerge from correlations: AI is used to classify individuals into groups and then make predictions assuming that the individual will resemble other members of the group. In some cases, this assumption may be unfair. An example of this is COMPAS, a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. ProPublica claims that the COMPAS-assigned recidivism risk level of black defendants is far more likely to be overestimated than that of white defendants, despite the fact that the program was not told the races of the defendants.Health equity issues may also be exacerbated when many-to-many mapping is done without taking steps to ensure equity for populations at risk for bias. At this time equity-focused tools and regulations are not in place to ensure equity application representation and usage. Other examples where algorithmic bias can lead to unfair outcomes are when AI is used for credit rating or hiring.
At its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022) the Association for Computing Machinery, in Seoul, South Korea, presented and published findings recommending that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed.",334,"Artificial intelligence
Algorithmic bias

AI programs can become biased after learning from real-world data. It is not typically introduced by the system designers but is learned by the program, and thus the programmers are often unaware that the bias exists.
Bias can be inadvertently introduced by the way training data is selected.
It can also emerge from correlations: AI is used to classify individuals into groups and then make predictions assuming that the individual will resemble other members of the group. In some cases, this assumption may be unfair. An example of this is COMPAS, a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. ProPublica claims that the COMPAS-assigned recidivism risk level of black defendants is far more likely to be overestimated than that of white defendants, despite the fact that the program was not told the races of the defendants.Health equity issues may also be exacerbated when many-to-many mapping is done without taking steps to ensure equity for populations at risk for bias. At this time equity-focused tools and regulations are not in place to ensure equity application representation and usage. Other examples where algorithmic bias can lead to unfair outcomes are when AI is used for credit rating or hiring.
At its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022) the Association for Computing Machinery, in Seoul, South Korea, presented and published findings recommending that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed.","1. What is algorithmic bias?
2. How can bias be inadvertently introduced into AI programs?
3. What are some examples of where algorithmic bias can lead to unfair outcomes?
4. What is the Association for Computing Machinery?
5. What are their recommendations for dealing with algorithmic bias?","1. Algorithmic bias is when a computer program exhibits a bias after learning from real-world data.
2. Bias can be inadvertently introduced into AI programs by the way training data is selected, or by correlations that the AI program may learn between different data points.
3. Some examples of where algorithmic bias can lead to unfair outcomes are when AI is used for credit rating or hiring decisions.
4. The Association for Computing Machinery is a professional organization for computer scientists and engineers.
5. Their recommendations for dealing with algorithmic bias are to curtail the use of self-learning neural networks until they can be demonstrated to be free of bias mistakes, and to develop tools and regulations to ensure equity application representation and usage."
Artificial intelligence,Existential risk,"It has been argued AI will become so powerful that humanity may irreversibly lose control of it. This could, as the physicist Stephen Hawking puts it, ""spell the end of the human race"". According to the philosopher Nick Bostrom, for almost any goals that a sufficiently intelligent AI may have, it is instrumentally incentivized to protect itself from being shut down and to acquire more resources, as intermediary steps to better achieve these goals. Sentience or emotions are then not required for an advanced AI to be dangerous. In order to be safe for humanity, a superintelligence would have to be genuinely aligned with humanity's morality and values so that it is ""fundamentally on our side"". The political scientist Charles T. Rubin argued that ""any sufficiently advanced benevolence may be indistinguishable from malevolence"" and warned that we should not be confident that intelligent machines will by default treat us favorably.The opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI. Personalities such as Stephen Hawking, Bill Gates, Elon Musk have expressed concern about existential risk from AI. In 2023, AI pioneers including Geoffrey Hinton, Yoshua Bengio, Demis Hassabis, and Sam Altman issued the joint statement that ""Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war""; some others such as Yann LeCun consider this to be unfounded. Mark Zuckerberg stated that artificial intelligence is helpful in its current form and will continue to assist humans. Some experts have argued that the risks are too distant in the future to warrant research, or that humans will be valuable from the perspective of a superintelligent machine. Rodney Brooks, in particular, said in 2014 that ""malevolent"" AI is still centuries away.",379,"Artificial intelligence
Existential risk

It has been argued AI will become so powerful that humanity may irreversibly lose control of it. This could, as the physicist Stephen Hawking puts it, ""spell the end of the human race"". According to the philosopher Nick Bostrom, for almost any goals that a sufficiently intelligent AI may have, it is instrumentally incentivized to protect itself from being shut down and to acquire more resources, as intermediary steps to better achieve these goals. Sentience or emotions are then not required for an advanced AI to be dangerous. In order to be safe for humanity, a superintelligence would have to be genuinely aligned with humanity's morality and values so that it is ""fundamentally on our side"". The political scientist Charles T. Rubin argued that ""any sufficiently advanced benevolence may be indistinguishable from malevolence"" and warned that we should not be confident that intelligent machines will by default treat us favorably.The opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI. Personalities such as Stephen Hawking, Bill Gates, Elon Musk have expressed concern about existential risk from AI. In 2023, AI pioneers including Geoffrey Hinton, Yoshua Bengio, Demis Hassabis, and Sam Altman issued the joint statement that ""Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war""; some others such as Yann LeCun consider this to be unfounded. Mark Zuckerberg stated that artificial intelligence is helpful in its current form and will continue to assist humans. Some experts have argued that the risks are too distant in the future to warrant research, or that humans will be valuable from the perspective of a superintelligent machine. Rodney Brooks, in particular, said in 2014 that ""malevolent"" AI is still centuries away.","1. What are the opinions amongst experts and industry insiders about the existential risk from AI?
2. What are some of the risks that have been identified with AI?
3. What do some people think about the potential for benevolent AI?
4. What is the stance of Mark Zuckerberg on AI?","1. The opinions amongst experts and industry insiders about the existential risk from AI are mixed. Some people are concerned, while others are unconcerned.
2. Some of the risks that have been identified with AI include the possibility of an AI becoming so powerful that it can't be controlled, and the possibility that it may not be benevolent.
3. Some people think that the potential for benevolent AI is high, while others are more skeptical.
4. Mark Zuckerberg believes that AI is helpful in its current form and will continue to assist humans."
Artificial intelligence,Copyright,"In order to leverage as large a dataset as is feasible, generative AI is often trained on unlicensed copyrighted works, including in domains such as images or computer code; the output is then used under a rationale of ""fair use"". Experts disagree about how well, and under what circumstances, this rationale will hold up in courts of law; relevant factors may include ""the purpose and character of the use of the copyrighted work"" and ""the effect upon the potential market for the copyrighted work"".",101,"Artificial intelligence
Copyright

In order to leverage as large a dataset as is feasible, generative AI is often trained on unlicensed copyrighted works, including in domains such as images or computer code; the output is then used under a rationale of ""fair use"". Experts disagree about how well, and under what circumstances, this rationale will hold up in courts of law; relevant factors may include ""the purpose and character of the use of the copyrighted work"" and ""the effect upon the potential market for the copyrighted work"".","1. What is the rationale for using unlicensed copyrighted works in generative AI?
2. How well do experts think this rationale will hold up in courts of law?
3. What factors are relevant to whether or not the rationale will hold up in courts of law?","1. The rationale for using unlicensed copyrighted works in generative AI is that it allows for the use of a larger dataset.
2. Experts disagree about how well the rationale will hold up in courts of law.
3. The factors that are relevant to whether or not the rationale will hold up in courts of law are the purpose and character of the use of the copyrighted work and the effect upon the potential market for the copyrighted work."
Artificial intelligence,Ethical machines,"Friendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.Machines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.
The field of machine ethics is also called computational morality,
and was founded at an AAAI symposium in 2005.Other approaches include Wendell Wallach's ""artificial moral agents""
and Stuart J. Russell's three principles for developing provably beneficial machines.",159,"Artificial intelligence
Ethical machines

Friendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.Machines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.
The field of machine ethics is also called computational morality,
and was founded at an AAAI symposium in 2005.Other approaches include Wendell Wallach's ""artificial moral agents""
and Stuart J. Russell's three principles for developing provably beneficial machines.","1. What is the difference between friendly AI and other types of AI?
2. What is the significance of the development of friendly AI?
3. What ethical principles do machines use in the field of machine ethics?","1. Friendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Other types of AI are not necessarily designed with these goals in mind.
2. The development of friendly AI is significant because it may be necessary in order to avoid existential risks posed by intelligent machines.
3. Machines in the field of machine ethics use ethical principles such as minimizing risks and benefiting humans."
Artificial intelligence,Regulation,"The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating artificial intelligence (AI); it is therefore related to the broader regulation of algorithms.
The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally. According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone.
Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI.
Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, US and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia.
The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology. Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI. In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years.In a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that ""products and services using AI have more benefits than drawbacks"". A 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity. In a 2023 Fox News poll, 35% of Americans thought it ""very important"", and an additional 41% thought it ""somewhat important"", for the federal government to regulate AI, versus 13% responding ""not very important"" and 8% responding ""not at all important"".",386,"Artificial intelligence
Regulation

The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating artificial intelligence (AI); it is therefore related to the broader regulation of algorithms.
The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally. According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone.
Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI.
Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, US and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia.
The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology. Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI. In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years.In a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that ""products and services using AI have more benefits than drawbacks"". A 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity. In a 2023 Fox News poll, 35% of Americans thought it ""very important"", and an additional 41% thought it ""somewhat important"", for the federal government to regulate AI, versus 13% responding ""not very important"" and 8% responding ""not at all important"".","1. What is the regulatory landscape for AI?
2. What is the Global Partnership on Artificial Intelligence?
3. What are the attitudes towards AI in different countries?
4. What are the recommendations for the governance of superintelligence?
5. What is the role of the federal government in regulating AI?","1. The regulatory landscape for AI is an emerging issue in jurisdictions globally. According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone.
2. The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology.
3. Attitudes towards AI in different countries vary greatly. In a 2022 Ipsos survey, 78% of Chinese citizens, but only 35% of Americans, agreed that ""products and services using AI have more benefits than drawbacks"".
4. In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years.
5. The role of the federal government in regulating AI is to ensure that products and services using AI have more benefits than drawbacks."
Artificial intelligence,In fiction,"Thought-capable artificial beings have appeared as storytelling devices since antiquity,
and have been a persistent theme in science fiction.A common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.Isaac Asimov introduced the Three Laws of Robotics in many books and stories, most notably the ""Multivac"" series about a super-intelligent computer of the same name. Asimov's laws are often brought up during lay discussions of machine ethics;
while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.Several works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel Čapek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.",331,"Artificial intelligence
In fiction

Thought-capable artificial beings have appeared as storytelling devices since antiquity,
and have been a persistent theme in science fiction.A common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.Isaac Asimov introduced the Three Laws of Robotics in many books and stories, most notably the ""Multivac"" series about a super-intelligent computer of the same name. Asimov's laws are often brought up during lay discussions of machine ethics;
while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.Several works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel Čapek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.","1. What is the most common trope in works with artificial intelligence?
2. What is the significance of Isaac Asimov's Three Laws of Robotics?
3. What question is often brought up during discussions of machine ethics?
4. What are some examples of works that explore what it means to be human?","1. The most common trope in works with artificial intelligence is the creation becoming a threat to its masters.
2. The significance of Isaac Asimov's Three Laws of Robotics is that they are often brought up during discussions of machine ethics.
3. The question that is often brought up during discussions of machine ethics is what makes us human.
4. Some examples of works that explore what it means to be human are A.I. Artificial Intelligence, Ex Machina, and Do Androids Dream of Electric Sheep?"
Hallucination (artificial intelligence),Summary,"In the field of artificial intelligence (AI), a hallucination or artificial hallucination (also called confabulation or delusion) is a confident response by an AI that does not seem to be justified by its training data. For example, a hallucinating chatbot might, when asked to generate a financial report for Tesla, falsely state that Tesla's revenue was $13.6 billion (or some other random number apparently ""plucked from thin air"").Such phenomena are termed ""hallucinations"", in loose analogy with the phenomenon of hallucination in human psychology. However, one key difference is that human hallucination is usually associated with false percepts, but an AI hallucination is associated with the category of unjustified responses or beliefs. Some researchers believe the specific term ""AI hallucination"" unreasonably anthropomorphizes computers.AI hallucination gained prominence around 2022 alongside the rollout of certain large language models (LLMs) such as ChatGPT. Users complained that such bots often seemed to ""sociopathically"" and pointlessly embed plausible-sounding random falsehoods within their generated content. By 2023, analysts considered frequent hallucination to be a major problem in LLM technology.",243,"Hallucination (artificial intelligence)
Summary

In the field of artificial intelligence (AI), a hallucination or artificial hallucination (also called confabulation or delusion) is a confident response by an AI that does not seem to be justified by its training data. For example, a hallucinating chatbot might, when asked to generate a financial report for Tesla, falsely state that Tesla's revenue was $13.6 billion (or some other random number apparently ""plucked from thin air"").Such phenomena are termed ""hallucinations"", in loose analogy with the phenomenon of hallucination in human psychology. However, one key difference is that human hallucination is usually associated with false percepts, but an AI hallucination is associated with the category of unjustified responses or beliefs. Some researchers believe the specific term ""AI hallucination"" unreasonably anthropomorphizes computers.AI hallucination gained prominence around 2022 alongside the rollout of certain large language models (LLMs) such as ChatGPT. Users complained that such bots often seemed to ""sociopathically"" and pointlessly embed plausible-sounding random falsehoods within their generated content. By 2023, analysts considered frequent hallucination to be a major problem in LLM technology.","1. What is an AI hallucination?
2. What are the consequences of AI hallucinations?
3. How did AI hallucinations gain prominence?","1. AI hallucinations are responses by AI that do not seem to be justified by its training data.
2. The consequences of AI hallucinations can be anything from benign (such as a chatbot saying something that is not true) to harmful (such as a chatbot saying something that is true but is not what the user wanted to hear).

3. AI hallucinations gained prominence around 2022 alongside the rollout of certain large language models (LLMs) such as ChatGPT. Users complained that such bots often seemed to ""sociopathically"" and pointlessly embed plausible-sounding random falsehoods within their generated content. By 2023, analysts considered frequent hallucination to be a major problem in LLM technology."
Hallucination (artificial intelligence),Analysis,"Various researchers cited by Wired have classified adversarial hallucinations as a high-dimensional statistical phenomenon, or have attributed hallucinations to insufficient training data. Some researchers believe that some ""incorrect"" AI responses classified by humans as ""hallucinations"" in the case of object detection may in fact be justified by the training data, or even that an AI may be giving the ""correct"" answer that the human reviewers are failing to see. For example, an adversarial image that looks, to a human, like an ordinary image of a dog, may in fact be seen by the AI to contain tiny patterns that (in authentic images) would only appear when viewing a cat. The AI is detecting real-world visual patterns that humans are insensitive to. However, these findings have been challenged by other researchers. For example, it was objected that the models can be biased towards superficial statistics, leading adversarial training to not be robust in real-world scenarios.",192,"Hallucination (artificial intelligence)
Analysis

Various researchers cited by Wired have classified adversarial hallucinations as a high-dimensional statistical phenomenon, or have attributed hallucinations to insufficient training data. Some researchers believe that some ""incorrect"" AI responses classified by humans as ""hallucinations"" in the case of object detection may in fact be justified by the training data, or even that an AI may be giving the ""correct"" answer that the human reviewers are failing to see. For example, an adversarial image that looks, to a human, like an ordinary image of a dog, may in fact be seen by the AI to contain tiny patterns that (in authentic images) would only appear when viewing a cat. The AI is detecting real-world visual patterns that humans are insensitive to. However, these findings have been challenged by other researchers. For example, it was objected that the models can be biased towards superficial statistics, leading adversarial training to not be robust in real-world scenarios.","1. What is an adversarial hallucination? 
2. What causes adversarial hallucinations? 
3. What is the significance of adversarial hallucinations?","1. An adversarial hallucination is an image or object that is seen by a machine as something different than what a human would see. 
2. Adversarial hallucinations are caused by the machine seeing things that humans cannot see. 
3. The significance of adversarial hallucinations is that they show that machines can see things that humans cannot."
Hallucination (artificial intelligence),In natural language processing,"In natural language processing, a hallucination is often defined as ""generated content that is nonsensical or unfaithful to the provided source content"". Depending on whether the output contradicts the prompt or not they could be divided to closed-domain and open-domain respectively.Errors in encoding and decoding between text and representations can cause hallucinations. AI training to produce diverse responses can also lead to hallucination. Hallucinations can also occur when the AI is trained on a dataset wherein labeled summaries, despite being factually accurate, are not directly grounded in the labeled data purportedly being ""summarized"". Larger datasets can create a problem of parametric knowledge (knowledge that is hard-wired in learned system parameters), creating hallucinations if the system is overconfident in its hardwired knowledge. In systems such as GPT-3, an AI generates each next word based on a sequence of previous words (including the words it has itself previously generated during the same conversation), causing a cascade of possible hallucination as the response grows longer. By 2022, papers such as the New York Times expressed concern that, as adoption of bots based on large language models continued to grow, unwarranted user confidence in bot output could lead to problems.In August 2022, Meta warned during its release of BlenderBot 3 that the system was prone to ""hallucinations"", which Meta defined as ""confident statements that are not true"". On 15 November 2022, Meta unveiled a demo of Galactica, designed to ""store, combine and reason about scientific knowledge"". Content generated by Galactica came with the warning ""Outputs may be unreliable! Language Models are prone to hallucinate text."" In one case, when asked to draft a paper on creating avatars, Galactica cited a fictitious paper from a real author who works in the relevant area. Meta withdrew Galactica on 17 November due to offensiveness and inaccuracy.It is considered that there are a lot of possible reasons for natural language models to hallucinate data. For example:

Hallucination from data: There are divergences in the source content (which would often happen with large training data sets).
Hallucination from training: Hallucination still occurs when there is little divergence in the data set. In that case, it derives from the way the model is trained. A lot of reasons can contribute to this type of hallucination, such as:
An erroneous decoding from the transformer
A bias from the historical sequences that the model previously generated
A bias generated from the way the model encodes its knowledge in its parameters",528,"Hallucination (artificial intelligence)
In natural language processing

In natural language processing, a hallucination is often defined as ""generated content that is nonsensical or unfaithful to the provided source content"". Depending on whether the output contradicts the prompt or not they could be divided to closed-domain and open-domain respectively.Errors in encoding and decoding between text and representations can cause hallucinations. AI training to produce diverse responses can also lead to hallucination. Hallucinations can also occur when the AI is trained on a dataset wherein labeled summaries, despite being factually accurate, are not directly grounded in the labeled data purportedly being ""summarized"". Larger datasets can create a problem of parametric knowledge (knowledge that is hard-wired in learned system parameters), creating hallucinations if the system is overconfident in its hardwired knowledge. In systems such as GPT-3, an AI generates each next word based on a sequence of previous words (including the words it has itself previously generated during the same conversation), causing a cascade of possible hallucination as the response grows longer. By 2022, papers such as the New York Times expressed concern that, as adoption of bots based on large language models continued to grow, unwarranted user confidence in bot output could lead to problems.In August 2022, Meta warned during its release of BlenderBot 3 that the system was prone to ""hallucinations"", which Meta defined as ""confident statements that are not true"". On 15 November 2022, Meta unveiled a demo of Galactica, designed to ""store, combine and reason about scientific knowledge"". Content generated by Galactica came with the warning ""Outputs may be unreliable! Language Models are prone to hallucinate text."" In one case, when asked to draft a paper on creating avatars, Galactica cited a fictitious paper from a real author who works in the relevant area. Meta withdrew Galactica on 17 November due to offensiveness and inaccuracy.It is considered that there are a lot of possible reasons for natural language models to hallucinate data. For example:

Hallucination from data: There are divergences in the source content (which would often happen with large training data sets).
Hallucination from training: Hallucination still occurs when there is little divergence in the data set. In that case, it derives from the way the model is trained. A lot of reasons can contribute to this type of hallucination, such as:
An erroneous decoding from the transformer
A bias from the historical sequences that the model previously generated
A bias generated from the way the model encodes its knowledge in its parameters","1. What is the definition of a hallucination in natural language processing? 
2. What are some possible reasons for natural language models to hallucinate data? 
3. What was the cause of the withdrawal of Meta's Galactica?","1. A hallucination in natural language processing is generally defined as generated content that is nonsensical or unfaithful to the provided source content. 
2. Some possible reasons for natural language models to hallucinate data can include:
-Divergences in the source content
-Hallucination from training
-Erroneous decoding from the transformer
-A bias from the historical sequences that the model previously generated
-A bias generated from the way the model encodes its knowledge in its parameters 
3. The cause of the withdrawal of Meta's Galactica was offensive and inaccurate outputs."
Hallucination (artificial intelligence),ChatGPT,"OpenAI's ChatGPT, released in beta-version to the public on November 30, 2022, is based on the foundation model GPT-3.5 (a revision of GPT-3). Professor Ethan Mollick of Wharton has called ChatGPT an ""omniscient, eager-to-please intern who sometimes lies to you"". Data scientist Teresa Kubacka has recounted deliberately making up the phrase ""cycloidal inverted electromagnon"" and testing ChatGPT by asking ChatGPT about the (nonexistent) phenomenon. ChatGPT invented a plausible-sounding answer backed with plausible-looking citations that compelled her to double-check whether she had accidentally typed in the name of a real phenomenon. Other scholars such as Oren Etzioni have joined Kubacka in assessing that such software can often give you ""a very impressive-sounding answer that's just dead wrong"".When CNBC asked ChatGPT for the lyrics to ""Ballad of Dwight Fry"", ChatGPT supplied invented lyrics rather than the actual lyrics. Asked questions about New Brunswick, ChatGPT got many answers right but incorrectly classified Samantha Bee as a ""person from New Brunswick"". Asked about astrophysical magnetic fields, ChatGPT incorrectly volunteered that ""(strong) magnetic fields of black holes are generated by the extremely strong gravitational forces in their vicinity"". (In reality, as a consequence of the no-hair theorem, a black hole without an accretion disk is believed to have no magnetic field.) Fast Company asked ChatGPT to generate a news article on Tesla's last financial quarter; ChatGPT created a coherent article, but made up the financial numbers contained within.Other examples involve baiting ChatGPT with a false premise to see if it embellishes upon the premise. When asked about ""Harold Coward's idea of dynamic canonicity"", ChatGPT fabricated that Coward wrote a book titled Dynamic Canonicity: A Model for Biblical and Theological Interpretation, arguing that religious principles are actually in a constant state of change. When pressed, ChatGPT continued to insist that the book was real. Asked for proof that dinosaurs built a civilization, ChatGPT claimed there were fossil remains of dinosaur tools and stated ""Some species of dinosaurs even developed primitive forms of art, such as engravings on stones"". When prompted that ""Scientists have recently discovered churros, the delicious fried-dough pastries... (are) ideal tools for home surgery"", ChatGPT claimed that a ""study published in the journal Science"" found that the dough is pliable enough to form into surgical instruments that can get into hard-to-reach places, and that the flavor has a calming effect on patients.By 2023, analysts considered frequent hallucination to be a major problem in LLM technology, with a Google executive identifying hallucination reduction as a ""fundamental"" task for ChatGPT competitor Google Bard. A 2023 demo for Microsoft's GPT-based Bing AI appeared to contain several hallucinations that went uncaught by the presenter.In May 2023, it was discovered Stephen Schwartz submitted six fake case precedents generated by ChatGPT in his brief to the Southern District of New York on Mata v. Avianca, a personal injury case against the airline Avianca. Schwartz said that he had never previously used ChatGPT, that he did not recognize the possibility that ChatGPT's output could have been fabricated, and that ChatGPT continued to assert the authenticity of the precedents after their nonexistence was discovered. In response, Brantley Starr of the Northern District of Texas banned the submission of AI-generated case filings that have not been reviewed by a human, noting that:
[Generative artificial intelligence] platforms in their current states are prone to hallucinations and bias. On hallucinations, they make stuff up—even quotes and citations. Another issue is reliability or bias. While attorneys swear an oath to set aside their personal prejudices, biases, and beliefs to faithfully uphold the law and represent their clients, generative artificial intelligence is the product of programming devised by humans who did not have to swear such an oath. As such, these systems hold no allegiance to any client, the rule of law, or the laws and Constitution of the United States (or, as addressed above, the truth). Unbound by any sense of duty, honor, or justice, such programs act according to computer code rather than conviction, based on programming rather than principle.

On June 23, P. Kevin Castel, tossed the Mata case and issued a $5,000 fine to Schwartz and another lawyer for bad faith conduct, who continued to stand by the fictitious precedents despite his previous claims. He characterized numerous errors and inconsistencies in the opinion summaries, describing one of the cited opinions as ""gibberish"" and ""[bordering] on nonsensical"".In June 2023, Mark Walters, a gun rights activist and radio personality, sued OpenAI in a Georgia state court after ChatGPT mischaracterized a legal complaint in a manner alleged to be defamatory against Walters. The complaint in question was brought in May 2023 by the Second Amendment Foundation against Washington attorney general Robert W. Ferguson for allegedly violating their freedom of speech, whereas the ChatGPT-generated summary bore no resemblance and claimed that Walters was accused of embezzlement and fraud while holding a Second Amendment Foundation office post that he never held in real life. According to AI legal expert Eugene Volokh, OpenAI may be shielded against this claim by Section 230, unless the court finds that OpenAI ""materially contributed"" to the publication of defamatory content.",1150,"Hallucination (artificial intelligence)
ChatGPT

OpenAI's ChatGPT, released in beta-version to the public on November 30, 2022, is based on the foundation model GPT-3.5 (a revision of GPT-3). Professor Ethan Mollick of Wharton has called ChatGPT an ""omniscient, eager-to-please intern who sometimes lies to you"". Data scientist Teresa Kubacka has recounted deliberately making up the phrase ""cycloidal inverted electromagnon"" and testing ChatGPT by asking ChatGPT about the (nonexistent) phenomenon. ChatGPT invented a plausible-sounding answer backed with plausible-looking citations that compelled her to double-check whether she had accidentally typed in the name of a real phenomenon. Other scholars such as Oren Etzioni have joined Kubacka in assessing that such software can often give you ""a very impressive-sounding answer that's just dead wrong"".When CNBC asked ChatGPT for the lyrics to ""Ballad of Dwight Fry"", ChatGPT supplied invented lyrics rather than the actual lyrics. Asked questions about New Brunswick, ChatGPT got many answers right but incorrectly classified Samantha Bee as a ""person from New Brunswick"". Asked about astrophysical magnetic fields, ChatGPT incorrectly volunteered that ""(strong) magnetic fields of black holes are generated by the extremely strong gravitational forces in their vicinity"". (In reality, as a consequence of the no-hair theorem, a black hole without an accretion disk is believed to have no magnetic field.) Fast Company asked ChatGPT to generate a news article on Tesla's last financial quarter; ChatGPT created a coherent article, but made up the financial numbers contained within.Other examples involve baiting ChatGPT with a false premise to see if it embellishes upon the premise. When asked about ""Harold Coward's idea of dynamic canonicity"", ChatGPT fabricated that Coward wrote a book titled Dynamic Canonicity: A Model for Biblical and Theological Interpretation, arguing that religious principles are actually in a constant state of change. When pressed, ChatGPT continued to insist that the book was real. Asked for proof that dinosaurs built a civilization, ChatGPT claimed there were fossil remains of dinosaur tools and stated ""Some species of dinosaurs even developed primitive forms of art, such as engravings on stones"". When prompted that ""Scientists have recently discovered churros, the delicious fried-dough pastries... (are) ideal tools for home surgery"", ChatGPT claimed that a ""study published in the journal Science"" found that the dough is pliable enough to form into surgical instruments that can get into hard-to-reach places, and that the flavor has a calming effect on patients.By 2023, analysts considered frequent hallucination to be a major problem in LLM technology, with a Google executive identifying hallucination reduction as a ""fundamental"" task for ChatGPT competitor Google Bard. A 2023 demo for Microsoft's GPT-based Bing AI appeared to contain several hallucinations that went uncaught by the presenter.In May 2023, it was discovered Stephen Schwartz submitted six fake case precedents generated by ChatGPT in his brief to the Southern District of New York on Mata v. Avianca, a personal injury case against the airline Avianca. Schwartz said that he had never previously used ChatGPT, that he did not recognize the possibility that ChatGPT's output could have been fabricated, and that ChatGPT continued to assert the authenticity of the precedents after their nonexistence was discovered. In response, Brantley Starr of the Northern District of Texas banned the submission of AI-generated case filings that have not been reviewed by a human, noting that:
[Generative artificial intelligence] platforms in their current states are prone to hallucinations and bias. On hallucinations, they make stuff up—even quotes and citations. Another issue is reliability or bias. While attorneys swear an oath to set aside their personal prejudices, biases, and beliefs to faithfully uphold the law and represent their clients, generative artificial intelligence is the product of programming devised by humans who did not have to swear such an oath. As such, these systems hold no allegiance to any client, the rule of law, or the laws and Constitution of the United States (or, as addressed above, the truth). Unbound by any sense of duty, honor, or justice, such programs act according to computer code rather than conviction, based on programming rather than principle.

On June 23, P. Kevin Castel, tossed the Mata case and issued a $5,000 fine to Schwartz and another lawyer for bad faith conduct, who continued to stand by the fictitious precedents despite his previous claims. He characterized numerous errors and inconsistencies in the opinion summaries, describing one of the cited opinions as ""gibberish"" and ""[bordering] on nonsensical"".In June 2023, Mark Walters, a gun rights activist and radio personality, sued OpenAI in a Georgia state court after ChatGPT mischaracterized a legal complaint in a manner alleged to be defamatory against Walters. The complaint in question was brought in May 2023 by the Second Amendment Foundation against Washington attorney general Robert W. Ferguson for allegedly violating their freedom of speech, whereas the ChatGPT-generated summary bore no resemblance and claimed that Walters was accused of embezzlement and fraud while holding a Second Amendment Foundation office post that he never held in real life. According to AI legal expert Eugene Volokh, OpenAI may be shielded against this claim by Section 230, unless the court finds that OpenAI ""materially contributed"" to the publication of defamatory content.","1. What is the main concern with artificial intelligence based chat programs?
2. How did ChatGPT respond when asked about lyrics to a song?
3. What did ChatGPT do when asked to generate a news article about Tesla's last financial quarter?
4. What did Stephen Schwartz do with case precedents generated by ChatGPT?
5. How did P. Kevin Castel rule in the Mark Walters case?","1. The main concern with artificial intelligence based chat programs is that they may generate false information.
2. ChatGPT responded by inventing lyrics for a song.
3. ChatGPT generated a news article about Tesla's last financial quarter that was full of false information.
4. Stephen Schwartz submitted fake case precedents generated by ChatGPT in his brief to the Southern District of New York.
5. P. Kevin Castel ruled in the Mark Walters case by fining Schwartz and another lawyer for bad faith conduct and characterizing numerous errors and inconsistencies in the opinion summaries."
Hallucination (artificial intelligence),Terminologies,"In Salon, statistician Gary N. Smith argues that LLMs ""do not understand what words mean"" and consequently that the term ""hallucination"" unreasonably anthropomorphizes the machine. Journalist Benj Edwards, in Ars Technica, writes that the term ""hallucination"" is controversial, but that some form of metaphor remains necessary; Edwards suggests ""confabulation"" as an analogy for processes that involve ""creative gap-filling"".Among researchers who do use the term ""hallucination"", definitions or characterizations in the context of LLMs include:

""a tendency to invent facts in moments of uncertainty"" (OpenAI, May 2023)
""a model's logical mistakes"" (OpenAI, May 2023)
fabricating information entirely, but behaving as if spouting facts (CNBC, May 2023)
""making up information"" (The Verge, February 2023)",194,"Hallucination (artificial intelligence)
Terminologies

In Salon, statistician Gary N. Smith argues that LLMs ""do not understand what words mean"" and consequently that the term ""hallucination"" unreasonably anthropomorphizes the machine. Journalist Benj Edwards, in Ars Technica, writes that the term ""hallucination"" is controversial, but that some form of metaphor remains necessary; Edwards suggests ""confabulation"" as an analogy for processes that involve ""creative gap-filling"".Among researchers who do use the term ""hallucination"", definitions or characterizations in the context of LLMs include:

""a tendency to invent facts in moments of uncertainty"" (OpenAI, May 2023)
""a model's logical mistakes"" (OpenAI, May 2023)
fabricating information entirely, but behaving as if spouting facts (CNBC, May 2023)
""making up information"" (The Verge, February 2023)","1. What is the definition of ""hallucination"" according to Gary N. Smith?
2. What is the definition of ""hallucination"" according to Benj Edwards?
3. What are the characterizations of ""hallucination"" in the context of LLMs according to OpenAI?
4. What are the characterizations of ""hallucination"" in the context of LLMs according to CNBC?
5. What are the characterizations of ""hallucination"" in","1. Gary N. Smith argues that LLMs ""do not understand what words mean"" and consequently that the term ""hallucination"" unreasonably anthropomorphizes the machine.
2. Benj Edwards writes that the term ""hallucination"" is controversial, but that some form of metaphor remains necessary; Edwards suggests ""confabulation"" as an analogy for processes that involve ""creative gap-filling"".
3. According to OpenAI, ""a tendency to invent facts in moments of uncertainty"" is one definition of ""hallucination"" in the context of LLMs.
4. According to CNBC, ""making up information"" is one characterization of ""hallucination"" in the context of LLMs.
5. The Verge defines ""hallucination"" as ""fabricating information entirely, but behaving as if spouting facts""."
Hallucination (artificial intelligence),In other artificial intelligence,"The concept of ""hallucination"" is applied more broadly than just natural language processing. A confident response from any AI that seems unjustified by the training data can be labeled a hallucination. Wired noted in 2018 that, despite no recorded attacks ""in the wild"" (that is, outside of proof-of-concept attacks by researchers), there was ""little dispute"" that consumer gadgets, and systems such as automated driving, were susceptible to adversarial attacks that could cause AI to hallucinate. Examples included a stop sign rendered invisible to computer vision; an audio clip engineered to sound innocuous to humans, but that software transcribed as ""evil dot com""; and an image of two men on skis, that Google Cloud Vision identified as 91% likely to be ""a dog"".",162,"Hallucination (artificial intelligence)
In other artificial intelligence

The concept of ""hallucination"" is applied more broadly than just natural language processing. A confident response from any AI that seems unjustified by the training data can be labeled a hallucination. Wired noted in 2018 that, despite no recorded attacks ""in the wild"" (that is, outside of proof-of-concept attacks by researchers), there was ""little dispute"" that consumer gadgets, and systems such as automated driving, were susceptible to adversarial attacks that could cause AI to hallucinate. Examples included a stop sign rendered invisible to computer vision; an audio clip engineered to sound innocuous to humans, but that software transcribed as ""evil dot com""; and an image of two men on skis, that Google Cloud Vision identified as 91% likely to be ""a dog"".","1. What is an example of an AI hallucination?
2. How can AI be made to hallucinate?","1. An example of an AI hallucination is a stop sign rendered invisible to computer vision.
2. AI can be made to hallucinate through adversarial attacks that cause AI to misinterpret data."
Hallucination (artificial intelligence),Mitigation methods,"The hallucination phenomenon is still not completely understood. Therefore, there is still ongoing research to try to mitigate its apparition. Particularly, it was shown that language models not only hallucinate but also amplify hallucinations, even for those which were designed to alleviate this issue.
Researchers have proposed a variety of mitigation measures, including getting different chatbots to debate one another until they reach consensus on an answer. Nvidia Guardrails, launched in 2023, can be configured to block LLM responses that don't pass fact-checking from a second LLM.",114,"Hallucination (artificial intelligence)
Mitigation methods

The hallucination phenomenon is still not completely understood. Therefore, there is still ongoing research to try to mitigate its apparition. Particularly, it was shown that language models not only hallucinate but also amplify hallucinations, even for those which were designed to alleviate this issue.
Researchers have proposed a variety of mitigation measures, including getting different chatbots to debate one another until they reach consensus on an answer. Nvidia Guardrails, launched in 2023, can be configured to block LLM responses that don't pass fact-checking from a second LLM.","1. What is the hallucination phenomenon?
2. What are some of the mitigation methods researchers have proposed?
3. What is Nvidia Guardrails?","1. The hallucination phenomenon is an issue where artificial intelligence (AI) chatbots start to hallucinate, or produce responses that are not based on reality.
2. Researchers have proposed a variety of mitigation measures, including getting different chatbots to debate one another until they reach consensus on an answer, and using Nvidia Guardrails to block LLM responses that don't pass fact-checking.
3. Nvidia Guardrails is a software program that can be used to block artificial intelligence chatbots from producing responses that are not based on reality."
Artificial general intelligence,Summary,"An artificial general intelligence (AGI) is a type of hypothetical intelligent agent. The AGI concept is that it can learn to accomplish any intellectual task that human beings or animals can perform. Alternatively, AGI has been defined as an autonomous system that surpasses human capabilities in the majority of economically valuable tasks. Creating AGI is a primary goal of some artificial intelligence research and companies such as OpenAI, DeepMind, and Anthropic. AGI is a common topic in science fiction and futures studies.
The timeline for AGI development remains a subject of ongoing debate among researchers and experts. Some argue that it may be possible in years or decades, others maintain it might take a century or longer, and a minority believe it may never be achieved. Additionally, there is debate regarding whether modern deep learning systems, such as GPT-4, are an early yet incomplete form of AGI or if new approaches are required.Contention exists over the potential for AGI to pose a threat to humanity; for example, OpenAI treats it as an existential risk, while others find the development of AGI to be too remote to present a risk.A 2020 survey identified 72 active AGI R&D projects spread across 37 countries.",252,"Artificial general intelligence
Summary

An artificial general intelligence (AGI) is a type of hypothetical intelligent agent. The AGI concept is that it can learn to accomplish any intellectual task that human beings or animals can perform. Alternatively, AGI has been defined as an autonomous system that surpasses human capabilities in the majority of economically valuable tasks. Creating AGI is a primary goal of some artificial intelligence research and companies such as OpenAI, DeepMind, and Anthropic. AGI is a common topic in science fiction and futures studies.
The timeline for AGI development remains a subject of ongoing debate among researchers and experts. Some argue that it may be possible in years or decades, others maintain it might take a century or longer, and a minority believe it may never be achieved. Additionally, there is debate regarding whether modern deep learning systems, such as GPT-4, are an early yet incomplete form of AGI or if new approaches are required.Contention exists over the potential for AGI to pose a threat to humanity; for example, OpenAI treats it as an existential risk, while others find the development of AGI to be too remote to present a risk.A 2020 survey identified 72 active AGI R&D projects spread across 37 countries.","1. What is the definition of an artificial general intelligence? 
2. What are the goals of some artificial intelligence research and companies? 
3. What is the timeline for AGI development? 
4. What is the debate over the potential for AGI to pose a threat to humanity? 
5. How many active AGI R&D projects are there?","1. An artificial general intelligence is a type of hypothetical intelligent agent that can learn to accomplish any intellectual task that human beings or animals can perform. 
2. The goals of some artificial intelligence research and companies are to create an AGI that can surpass human capabilities in the majority of economically valuable tasks. 
3. The timeline for AGI development remains a subject of ongoing debate among researchers and experts. Some argue that it may be possible in years or decades, others maintain it might take a century or longer, and a minority believe it may never be achieved. 
4. The debate over the potential for AGI to pose a threat to humanity is ongoing. Some believe that it is a possibility and should be treated as an existential risk, while others find the development of AGI to be too remote to present a risk. 
5. As of 2020, there are 72 active AGI R&D projects spread across 37 countries."
Artificial general intelligence,Terminology,"AGI is also known as strong AI, full AI, or general intelligent action. However, some academic sources reserve the term ""strong AI"" for computer programs that experience sentience or consciousness. In contrast, weak AI (or narrow AI) is able to solve one specific problem, but lacks general cognitive abilities. Some academic sources use ""weak AI"" to refer more broadly to any programs that neither experience consciousness nor have a mind in the same sense as humans.Related concepts include human-level AI, transformative AI, and superintelligence.",111,"Artificial general intelligence
Terminology

AGI is also known as strong AI, full AI, or general intelligent action. However, some academic sources reserve the term ""strong AI"" for computer programs that experience sentience or consciousness. In contrast, weak AI (or narrow AI) is able to solve one specific problem, but lacks general cognitive abilities. Some academic sources use ""weak AI"" to refer more broadly to any programs that neither experience consciousness nor have a mind in the same sense as humans.Related concepts include human-level AI, transformative AI, and superintelligence.","1. What is the main difference between strong AI and weak AI?
2. What is the difference between human-level AI and superintelligence?
3. What is the definition of AGI?","1. The main difference between strong AI and weak AI is that strong AI has general cognitive abilities, while weak AI does not.
2. The difference between human-level AI and superintelligence is that human-level AI is capable of doing things that humans can do, while superintelligence is capable of doing things that humans cannot do.
3. AGI is a term that is used to describe a computer program that has general cognitive abilities and is capable of doing things that humans can do."
Artificial general intelligence,Characteristics,Various criteria for intelligence have been proposed (most famously the Turing test) but no definition is broadly accepted.,24,"Artificial general intelligence
Characteristics

Various criteria for intelligence have been proposed (most famously the Turing test) but no definition is broadly accepted.","1. What is the Turing test? 
2. What are the most famous criteria for intelligence? 
3. What is the definition of intelligence that is broadly accepted?","1. The Turing test is a measure of a machine's ability to exhibit intelligent behaviour equivalent to, or indistinguishable from, that of a human. 
2. The most famous criteria for intelligence are the Turing test and the definition proposed by Alan Turing in 1950. 
3. The definition of intelligence that is broadly accepted is the definition proposed by Alan Turing in 1950."
Artificial general intelligence,Intelligence traits,"However, researchers generally hold that intelligence is required to do the following:
reason, use strategy, solve puzzles, and make judgments under uncertainty;
represent knowledge, including common sense knowledge;
plan;
learn;
communicate in natural language;and, if necessary, integrate these skills in completion of any given goal. 
Many interdisciplinary approaches (e.g. cognitive science, computational intelligence, and decision making) consider additional traits such as imagination (the ability to form novel mental images and concepts) and autonomy.Computer-based systems that exhibit many of these capabilities exist (e.g. see computational creativity, automated reasoning, decision support system, robot, evolutionary computation, intelligent agent). However, no consensus holds that modern AI systems possess them to an adequate degree.",161,"Artificial general intelligence
Intelligence traits

However, researchers generally hold that intelligence is required to do the following:
reason, use strategy, solve puzzles, and make judgments under uncertainty;
represent knowledge, including common sense knowledge;
plan;
learn;
communicate in natural language;and, if necessary, integrate these skills in completion of any given goal. 
Many interdisciplinary approaches (e.g. cognitive science, computational intelligence, and decision making) consider additional traits such as imagination (the ability to form novel mental images and concepts) and autonomy.Computer-based systems that exhibit many of these capabilities exist (e.g. see computational creativity, automated reasoning, decision support system, robot, evolutionary computation, intelligent agent). However, no consensus holds that modern AI systems possess them to an adequate degree.","1. What is the definition of intelligence according to the text? 
2. What are some of the tasks that intelligence is required for? 
3. What is the consensus among researchers about whether AI systems possess the capabilities to an adequate degree?","1. The definition of intelligence according to the text is the ability to do the following: reason, use strategy, solve puzzles, and make judgments under uncertainty; represent knowledge, including common sense knowledge; plan; learn; communicate in natural language; and, if necessary, integrate these skills in completion of any given goal. 
2. Some of the tasks that intelligence is required for are reasoning, using strategy, solving puzzles, making judgments under uncertainty, representing knowledge, including common sense knowledge, planning, learning, and communicating in natural language. 
3. The consensus among researchers about whether AI systems possess the capabilities to an adequate degree is that no consensus holds."
Artificial general intelligence,Physical traits,"Other capabilities are considered desirable in intelligent systems, as they may affect intelligence or aid in its expression. These include:
the ability to sense (e.g. see, hear, etc.), and
the ability to act (e.g. move and manipulate objects, change location to explore, etc.)This includes the ability to detect and respond to hazard.",76,"Artificial general intelligence
Physical traits

Other capabilities are considered desirable in intelligent systems, as they may affect intelligence or aid in its expression. These include:
the ability to sense (e.g. see, hear, etc.), and
the ability to act (e.g. move and manipulate objects, change location to explore, etc.)This includes the ability to detect and respond to hazard.","1. What are some desirable physical traits in an intelligent system?
2. What are some other capabilities that are considered desirable in intelligent systems?
3. What is the ability to detect and respond to hazard?","1. Some desirable physical traits in an intelligent system are the ability to sense and the ability to act.
2. Other capabilities that are considered desirable in intelligent systems are the ability to detect and respond to hazard.
3. The ability to detect and respond to hazard is the ability to detect and respond to potential danger or harm."
Artificial general intelligence,Mathematical formalisms,"A mathematically precise specification of AGI was proposed by Marcus Hutter in 2000. Named AIXI, the proposed AGI agent maximises “the ability to satisfy goals in a wide range of environments”. This type of AGI, characterized by the ability to maximise a mathematical definition of intelligence rather than exhibit human-like behaviour, is also called universal artificial intelligence.In 2015 Jan Lieke and Marcus Hutter showed that Legg-Hutter intelligence - ""an agent’s ability to achieve goals in a wide range of environments"" - is measured with respect to ""a fixed Universal Turing Machine(UTM). AIXI is the most intelligent policy if it uses the same UTM"", a result which ""undermines all existing optimality properties for AIXI"". This problem stems from AIXI's use of compression as a proxy for intelligence, which is only valid if cognition takes place in isolation from the environment in which goals are pursued. This formalises a philosophical position known as Mind–body dualism. Some find enactivism more plausible—the notion that cognition takes place within the same environment in which goals are pursued. Subsequently, Michael Timothy Bennett formalised enactive cognition and identified an alternative proxy for intelligence called ""weakness"". The accompanying experiments (comparing weakness and compression) and mathematical proofs showed that maximising weakness results in the optimal ""ability to complete a wide range of tasks"" or equivalently ""ability to generalise"" (thus maximising intelligence by either definition). If enactivism holds and Mind–body dualism does not, then compression is not necessary or sufficient for intelligence, calling into question widely held views on intelligence (see also Hutter Prize).
Whether an AGI that satisfies one of these formalizations exhibits human-like behaviour (such as the use of natural language) would depend on many factors, for example the manner in which the agent is embodied, or whether it has a reward function that closely approximates human primitives of cognition like hunger, pain, and so forth.",417,"Artificial general intelligence
Mathematical formalisms

A mathematically precise specification of AGI was proposed by Marcus Hutter in 2000. Named AIXI, the proposed AGI agent maximises “the ability to satisfy goals in a wide range of environments”. This type of AGI, characterized by the ability to maximise a mathematical definition of intelligence rather than exhibit human-like behaviour, is also called universal artificial intelligence.In 2015 Jan Lieke and Marcus Hutter showed that Legg-Hutter intelligence - ""an agent’s ability to achieve goals in a wide range of environments"" - is measured with respect to ""a fixed Universal Turing Machine(UTM). AIXI is the most intelligent policy if it uses the same UTM"", a result which ""undermines all existing optimality properties for AIXI"". This problem stems from AIXI's use of compression as a proxy for intelligence, which is only valid if cognition takes place in isolation from the environment in which goals are pursued. This formalises a philosophical position known as Mind–body dualism. Some find enactivism more plausible—the notion that cognition takes place within the same environment in which goals are pursued. Subsequently, Michael Timothy Bennett formalised enactive cognition and identified an alternative proxy for intelligence called ""weakness"". The accompanying experiments (comparing weakness and compression) and mathematical proofs showed that maximising weakness results in the optimal ""ability to complete a wide range of tasks"" or equivalently ""ability to generalise"" (thus maximising intelligence by either definition). If enactivism holds and Mind–body dualism does not, then compression is not necessary or sufficient for intelligence, calling into question widely held views on intelligence (see also Hutter Prize).
Whether an AGI that satisfies one of these formalizations exhibits human-like behaviour (such as the use of natural language) would depend on many factors, for example the manner in which the agent is embodied, or whether it has a reward function that closely approximates human primitives of cognition like hunger, pain, and so forth.","1. What is the difference between artificial general intelligence and universal artificial intelligence?
2. What is the difference between AIXI and Legg-Hutter intelligence?
3. What is the difference between compression and weakness?
4. How does enactivism differ from mind-body dualism?","1. Artificial general intelligence is a more general term that includes all forms of artificial intelligence. Universal artificial intelligence refers to a specific type of artificial intelligence that maximizes a mathematical definition of intelligence.
2. AIXI is a specific type of artificial intelligence that maximizes a mathematical definition of intelligence. Legg-Hutter intelligence is a more general term that includes AIXI and other types of artificial intelligence.
3. Compression is a proxy for intelligence that is only valid if cognition takes place in isolation from the environment. Weakness is a proxy for intelligence that is valid for all types of environments.
4. Enactivism is the notion that cognition takes place within the same environment in which goals are pursued. Mind-body dualism is the philosophical position that cognition takes place in two separate locations, the mind and the body."
Artificial general intelligence,Tests for testing human-level AGI,"Several tests meant to confirm human-level AGI have been considered, including:
The Turing Test (Turing)
A machine and a human both converse unseen with a second human, who must evaluate which of the two is the machine, which passes the test if it can fool the evaluator a significant fraction of the time. Note: Turing does not prescribe what should qualify as intelligence, only that knowing that it is a machine should disqualify it. The AI Eugene Goostman achieved Turing's estimate of convincing 30% of judges that it was human in 2014.
The Robot College Student Test (Goertzel)
A machine enrolls in a university, taking and passing the same classes that humans would, and obtaining a degree. LLMs can now pass university degree-level exams without even attending the classes.
The Employment Test (Nilsson)
A machine performs an economically important job at least as well as humans in the same job. AIs are now replacing humans in many roles as varied as fast food, and marketing.
The Ikea test (Marcus)
Also known as the Flat Pack Furniture Test. An AI views the parts and instructions of an Ikea flat-pack product, then controls a robot to assemble the furniture correctly.
The Coffee Test (Wozniak)
A machine is required to enter an average American home and figure out how to make coffee: find the coffee machine, find the coffee, add water, find a mug, and brew the coffee by pushing the proper buttons. This has not yet been completed.",329,"Artificial general intelligence
Tests for testing human-level AGI

Several tests meant to confirm human-level AGI have been considered, including:
The Turing Test (Turing)
A machine and a human both converse unseen with a second human, who must evaluate which of the two is the machine, which passes the test if it can fool the evaluator a significant fraction of the time. Note: Turing does not prescribe what should qualify as intelligence, only that knowing that it is a machine should disqualify it. The AI Eugene Goostman achieved Turing's estimate of convincing 30% of judges that it was human in 2014.
The Robot College Student Test (Goertzel)
A machine enrolls in a university, taking and passing the same classes that humans would, and obtaining a degree. LLMs can now pass university degree-level exams without even attending the classes.
The Employment Test (Nilsson)
A machine performs an economically important job at least as well as humans in the same job. AIs are now replacing humans in many roles as varied as fast food, and marketing.
The Ikea test (Marcus)
Also known as the Flat Pack Furniture Test. An AI views the parts and instructions of an Ikea flat-pack product, then controls a robot to assemble the furniture correctly.
The Coffee Test (Wozniak)
A machine is required to enter an average American home and figure out how to make coffee: find the coffee machine, find the coffee, add water, find a mug, and brew the coffee by pushing the proper buttons. This has not yet been completed.","1. What is the Turing Test?
2. What is the Robot College Student Test?
3. What is the Employment Test?
4. What is the Ikea Test?
5. What is the Coffee Test?","1. The Turing Test is a test meant to confirm human-level AGI.
2. The Robot College Student Test is a test meant to confirm human-level AGI.
3. The Employment Test is a test meant to confirm human-level AGI.
4. The Ikea Test is a test meant to confirm human-level AGI.
5. The Coffee Test is a test meant to confirm human-level AGI."
Artificial general intelligence,AI-complete problems,"There are many problems that may require general intelligence, if machines are to solve the problems as well as people do. For example, even specific straightforward tasks, like machine translation, require that a machine read and write in both languages (NLP), follow the author's argument (reason), know what is being talked about (knowledge), and faithfully reproduce the author's original intent (social intelligence). All of these problems need to be solved simultaneously in order to reach human-level machine performance.
A problem is informally called ""AI-complete"" or ""AI-hard"" if it is believed that to solve it one would need to implement strong AI, because the solution is beyond the capabilities of a purpose-specific algorithm.AI-complete problems are hypothesised to include general computer vision, natural language understanding, and dealing with unexpected circumstances while solving any real-world problem.AI-complete problems cannot be solved with current computer technology alone, and require human computation. This limitation could be useful to test for the presence of humans, as CAPTCHAs aim to do; and for computer security to repel brute-force attacks.",230,"Artificial general intelligence
AI-complete problems

There are many problems that may require general intelligence, if machines are to solve the problems as well as people do. For example, even specific straightforward tasks, like machine translation, require that a machine read and write in both languages (NLP), follow the author's argument (reason), know what is being talked about (knowledge), and faithfully reproduce the author's original intent (social intelligence). All of these problems need to be solved simultaneously in order to reach human-level machine performance.
A problem is informally called ""AI-complete"" or ""AI-hard"" if it is believed that to solve it one would need to implement strong AI, because the solution is beyond the capabilities of a purpose-specific algorithm.AI-complete problems are hypothesised to include general computer vision, natural language understanding, and dealing with unexpected circumstances while solving any real-world problem.AI-complete problems cannot be solved with current computer technology alone, and require human computation. This limitation could be useful to test for the presence of humans, as CAPTCHAs aim to do; and for computer security to repel brute-force attacks.","1. What is an AI-complete problem?
2. What are some examples of AI-complete problems?
3. Why are AI-complete problems difficult to solve?","1. An AI-complete problem is a problem that is believed to require general intelligence, if machines are to solve the problem as well as people do.
2. Some examples of AI-complete problems are machine translation, natural language understanding, and dealing with unexpected circumstances while solving any real-world problem.
3. AI-complete problems are difficult to solve because they require human computation, which is beyond the capabilities of a purpose-specific algorithm."
Artificial general intelligence,Classical AI,"Modern AI research began in the mid-1950s. The first generation of AI researchers were convinced that artificial general intelligence was possible and that it would exist in just a few decades. AI pioneer Herbert A. Simon wrote in 1965: ""machines will be capable, within twenty years, of doing any work a man can do.""Their predictions were the inspiration for Stanley Kubrick and Arthur C. Clarke's character HAL 9000, who embodied what AI researchers believed they could create by the year 2001. AI pioneer Marvin Minsky was a consultant on the project of making HAL 9000 as realistic as possible according to the consensus predictions of the time. He said in 1967, ""Within a generation... the problem of creating 'artificial intelligence' will substantially be solved"".Several classical AI projects, such as Doug Lenat's Cyc project (that began in 1984), and Allen Newell's Soar project, were directed at AGI.
However, in the early 1970s, it became obvious that researchers had grossly underestimated the difficulty of the project. Funding agencies became skeptical of AGI and put researchers under increasing pressure to produce useful ""applied AI"". In the early 1980s, Japan's Fifth Generation Computer Project revived interest in AGI, setting out a ten-year timeline that included AGI goals like ""carry on a casual conversation"". In response to this and the success of expert systems, both industry and government pumped money back into the field. However, confidence in AI spectacularly collapsed in the late 1980s, and the goals of the Fifth Generation Computer Project were never fulfilled. For the second time in 20 years, AI researchers who predicted the imminent achievement of AGI had been mistaken. By the 1990s, AI researchers had a reputation for making vain promises. They became reluctant to make predictions at all and avoided mention of ""human level"" artificial intelligence for fear of being labeled ""wild-eyed dreamer[s]"".",390,"Artificial general intelligence
Classical AI

Modern AI research began in the mid-1950s. The first generation of AI researchers were convinced that artificial general intelligence was possible and that it would exist in just a few decades. AI pioneer Herbert A. Simon wrote in 1965: ""machines will be capable, within twenty years, of doing any work a man can do.""Their predictions were the inspiration for Stanley Kubrick and Arthur C. Clarke's character HAL 9000, who embodied what AI researchers believed they could create by the year 2001. AI pioneer Marvin Minsky was a consultant on the project of making HAL 9000 as realistic as possible according to the consensus predictions of the time. He said in 1967, ""Within a generation... the problem of creating 'artificial intelligence' will substantially be solved"".Several classical AI projects, such as Doug Lenat's Cyc project (that began in 1984), and Allen Newell's Soar project, were directed at AGI.
However, in the early 1970s, it became obvious that researchers had grossly underestimated the difficulty of the project. Funding agencies became skeptical of AGI and put researchers under increasing pressure to produce useful ""applied AI"". In the early 1980s, Japan's Fifth Generation Computer Project revived interest in AGI, setting out a ten-year timeline that included AGI goals like ""carry on a casual conversation"". In response to this and the success of expert systems, both industry and government pumped money back into the field. However, confidence in AI spectacularly collapsed in the late 1980s, and the goals of the Fifth Generation Computer Project were never fulfilled. For the second time in 20 years, AI researchers who predicted the imminent achievement of AGI had been mistaken. By the 1990s, AI researchers had a reputation for making vain promises. They became reluctant to make predictions at all and avoided mention of ""human level"" artificial intelligence for fear of being labeled ""wild-eyed dreamer[s]"".","1. What are the goals of the Fifth Generation Computer Project?
2. Why did confidence in AI collapse in the late 1980s?
3. What did AI researchers become reluctant to do in the 1990s?","1. The goals of the Fifth Generation Computer Project were to create machines that could do the same things as humans.
2. Confidence in AI collapsed in the late 1980s because researchers had grossly underestimated the difficulty of the project.
3. AI researchers became reluctant to make predictions in the 1990s because they had a reputation for making vain promises."
Artificial general intelligence,Narrow AI research,"In the 1990s and early 21st century, mainstream AI achieved commercial success and academic respectability by focusing on specific sub-problems where AI can produce verifiable results and commercial applications, such as artificial neural networks and statistical machine learning. These ""applied AI"" systems are now used extensively throughout the technology industry, and research in this vein is heavily funded in both academia and industry. As of 2018 development on this field was considered an emerging trend, and a mature stage was expected to happen in more than 10 years.
Most mainstream AI researchers hope that strong AI can be developed by combining programs that solve various sub-problems. Hans Moravec wrote in 1988: I am confident that this bottom-up route to artificial intelligence will one day meet the traditional top-down route more than half way, ready to provide the real-world competence and the commonsense knowledge that has been so frustratingly elusive in reasoning programs. Fully intelligent machines will result when the metaphorical golden spike is driven uniting the two efforts.
However, this is disputed. For example, Stevan Harnad of Princeton University concluded his 1990 paper on the Symbol Grounding Hypothesis by stating: The expectation has often been voiced that ""top-down"" (symbolic) approaches to modeling cognition will somehow meet ""bottom-up"" (sensory) approaches somewhere in between. If the grounding considerations in this paper are valid, then this expectation is hopelessly modular and there is really only one viable route from sense to symbols: from the ground up. A free-floating symbolic level like the software level of a computer will never be reached by this route (or vice versa) – nor is it clear why we should even try to reach such a level, since it looks as if getting there would just amount to uprooting our symbols from their intrinsic meanings (thereby merely reducing ourselves to the functional equivalent of a programmable computer).",394,"Artificial general intelligence
Narrow AI research

In the 1990s and early 21st century, mainstream AI achieved commercial success and academic respectability by focusing on specific sub-problems where AI can produce verifiable results and commercial applications, such as artificial neural networks and statistical machine learning. These ""applied AI"" systems are now used extensively throughout the technology industry, and research in this vein is heavily funded in both academia and industry. As of 2018 development on this field was considered an emerging trend, and a mature stage was expected to happen in more than 10 years.
Most mainstream AI researchers hope that strong AI can be developed by combining programs that solve various sub-problems. Hans Moravec wrote in 1988: I am confident that this bottom-up route to artificial intelligence will one day meet the traditional top-down route more than half way, ready to provide the real-world competence and the commonsense knowledge that has been so frustratingly elusive in reasoning programs. Fully intelligent machines will result when the metaphorical golden spike is driven uniting the two efforts.
However, this is disputed. For example, Stevan Harnad of Princeton University concluded his 1990 paper on the Symbol Grounding Hypothesis by stating: The expectation has often been voiced that ""top-down"" (symbolic) approaches to modeling cognition will somehow meet ""bottom-up"" (sensory) approaches somewhere in between. If the grounding considerations in this paper are valid, then this expectation is hopelessly modular and there is really only one viable route from sense to symbols: from the ground up. A free-floating symbolic level like the software level of a computer will never be reached by this route (or vice versa) – nor is it clear why we should even try to reach such a level, since it looks as if getting there would just amount to uprooting our symbols from their intrinsic meanings (thereby merely reducing ourselves to the functional equivalent of a programmable computer).","1. What is the main difference between mainstream AI and strong AI?
2. What is the Symbol Grounding Hypothesis?
3. What is the main difference between top-down and bottom-up approaches to AI?","1. Mainstream AI is focused on specific sub-problems where AI can produce verifiable results and commercial applications. Strong AI is the goal of creating a machine that can match or exceed human intelligence.
2. The Symbol Grounding Hypothesis is the idea that symbols (like words) need to be grounded in sensory experience.
3. The main difference between top-down and bottom-up approaches to AI is the level at which intelligence is achieved. Top-down approaches focus on creating a symbolic level that is separate from sensory experience. Bottom-up approaches focus on creating a level of intelligence that is based on sensory experience."
Artificial general intelligence,Modern artificial general intelligence research,"The term ""artificial general intelligence"" was used as early as 1997, by Mark Gubrud in a discussion of the implications of fully automated military production and operations. The term was re-introduced and popularized by Shane Legg and Ben Goertzel around 2002. AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as ""producing publications and preliminary results"". The first summer school in AGI was organized in Xiamen, China in 2009 by the Xiamen university's Artificial Brain Laboratory and OpenCog. The first university course was given in 2010 and 2011 at Plovdiv University, Bulgaria by Todor Arnaudov. MIT presented a course in AGI in 2018, organized by Lex Fridman and featuring a number of guest lecturers.
As of 2023, a small number of computer scientists are active in AGI research, and many contribute to a series of AGI conferences. However, increasingly more researchers are interested in open-ended learning, which is the idea of allowing AI to continuously learn and innovate like humans do. Although most open-ended learning works are still done on Minecraft, its application can be extended to robotics and the sciences.",254,"Artificial general intelligence
Modern artificial general intelligence research

The term ""artificial general intelligence"" was used as early as 1997, by Mark Gubrud in a discussion of the implications of fully automated military production and operations. The term was re-introduced and popularized by Shane Legg and Ben Goertzel around 2002. AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as ""producing publications and preliminary results"". The first summer school in AGI was organized in Xiamen, China in 2009 by the Xiamen university's Artificial Brain Laboratory and OpenCog. The first university course was given in 2010 and 2011 at Plovdiv University, Bulgaria by Todor Arnaudov. MIT presented a course in AGI in 2018, organized by Lex Fridman and featuring a number of guest lecturers.
As of 2023, a small number of computer scientists are active in AGI research, and many contribute to a series of AGI conferences. However, increasingly more researchers are interested in open-ended learning, which is the idea of allowing AI to continuously learn and innovate like humans do. Although most open-ended learning works are still done on Minecraft, its application can be extended to robotics and the sciences.","1. What is the definition of artificial general intelligence? 
2. What are the goals of the artificial general intelligence community? 
3. When was the term ""artificial general intelligence"" first used? 
4. What are some applications of artificial general intelligence?","1. The definition of artificial general intelligence is the ability of a computer system to perform tasks that only humans can perform, such as reasoning, natural communication, and problem solving. 
2. The goals of the artificial general intelligence community are to create a computer system that can think and learn like humans. 
3. The term ""artificial general intelligence"" was first used in 1997 by Mark Gubrud. 
4. Some applications of artificial general intelligence are open-ended learning, robotics, and the sciences."
Artificial general intelligence,Feasibility,"As of 2022, AGI remains speculative. No such system has yet been demonstrated. Opinions vary both on whether and when artificial general intelligence will arrive. AI pioneer Herbert A. Simon speculated in 1965 that ""machines will be capable, within twenty years, of doing any work a man can do"". This prediction failed to come true. Microsoft co-founder Paul Allen believed that such intelligence is unlikely in the 21st century because it would require ""unforeseeable and fundamentally unpredictable breakthroughs"" and a ""scientifically deep understanding of cognition"". Writing in The Guardian, roboticist Alan Winfield claimed the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical faster-than-light spaceflight.Most AI researchers believe strong AI can be achieved in the future, but some thinkers, like Hubert Dreyfus and Roger Penrose, deny the possibility of achieving strong AI.  John McCarthy is among those who believe human-level AI will be accomplished, but that the present level of progress is such that a date cannot accurately be predicted. AI experts' views on the feasibility of AGI wax and wane. Four polls conducted in 2012 and 2013 suggested that the median guess among experts for when they would be 50% confident AGI would arrive was 2040 to 2050, depending on the poll, with the mean being 2081. Of the experts, 16.5% answered with ""never"" when asked the same question but with a 90% confidence instead. Further current AGI progress considerations can be found above Tests for confirming human-level AGI.
A report by Stuart Armstrong and Kaj Sotala of the Machine Intelligence Research Institute found that ""over [a] 60-year time frame there is a strong bias towards predicting the arrival of human-level AI as between 15 and 25 years from the time the prediction was made"". They analyzed 95 predictions made between 1950 and 2012 on when human-level AI will come about.",408,"Artificial general intelligence
Feasibility

As of 2022, AGI remains speculative. No such system has yet been demonstrated. Opinions vary both on whether and when artificial general intelligence will arrive. AI pioneer Herbert A. Simon speculated in 1965 that ""machines will be capable, within twenty years, of doing any work a man can do"". This prediction failed to come true. Microsoft co-founder Paul Allen believed that such intelligence is unlikely in the 21st century because it would require ""unforeseeable and fundamentally unpredictable breakthroughs"" and a ""scientifically deep understanding of cognition"". Writing in The Guardian, roboticist Alan Winfield claimed the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical faster-than-light spaceflight.Most AI researchers believe strong AI can be achieved in the future, but some thinkers, like Hubert Dreyfus and Roger Penrose, deny the possibility of achieving strong AI.  John McCarthy is among those who believe human-level AI will be accomplished, but that the present level of progress is such that a date cannot accurately be predicted. AI experts' views on the feasibility of AGI wax and wane. Four polls conducted in 2012 and 2013 suggested that the median guess among experts for when they would be 50% confident AGI would arrive was 2040 to 2050, depending on the poll, with the mean being 2081. Of the experts, 16.5% answered with ""never"" when asked the same question but with a 90% confidence instead. Further current AGI progress considerations can be found above Tests for confirming human-level AGI.
A report by Stuart Armstrong and Kaj Sotala of the Machine Intelligence Research Institute found that ""over [a] 60-year time frame there is a strong bias towards predicting the arrival of human-level AI as between 15 and 25 years from the time the prediction was made"". They analyzed 95 predictions made between 1950 and 2012 on when human-level AI will come about.","1. What is the feasibility of artificial general intelligence?
2. What are the opinions of AI pioneers on the feasibility of AGI?
3. What do Microsoft co-founder Paul Allen and roboticist Alan Winfield think about the feasibility of AGI?
4. What do AI experts think about the feasibility of AGI?
5. What is the bias of AI experts when it comes to predicting the arrival of human-level AI?","1. The feasibility of artificial general intelligence is speculative.
2. AI pioneers' opinions on the feasibility of AGI vary.
3. Microsoft co-founder Paul Allen and roboticist Alan Winfield believe that strong AI is unlikely in the 21st century because it would require ""unforeseeable and fundamentally unpredictable breakthroughs"" and a ""scientifically deep understanding of cognition"".
4. AI experts generally believe that strong AI can be achieved in the future, but that the present level of progress is such that a date cannot accurately be predicted.
5. There is a strong bias amongst AI experts when it comes to predicting the arrival of human-level AI, with the majority predicting that it will happen between 15 and 25 years from the time the prediction was made."
Artificial general intelligence,Timescales,"In the introduction to his 2006 book, Goertzel says that estimates of the time needed before a truly flexible AGI is built vary from 10 years to over a century. As of 2007 the consensus in the AGI research community seemed to be that the timeline discussed by Ray Kurzweil in The Singularity is Near (i.e. between 2015 and 2045) was plausible. Mainstream AI researchers have given a wide range of opinions on whether progress will be this rapid. A 2012 meta-analysis of 95 such opinions found a bias towards predicting that the onset of AGI would occur within 16–26 years for modern and historical predictions alike. That paper has been criticized for how it categorized opinions as expert or non-expert.In 2012, Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton developed a neural network called AlexNet, which won the ImageNet competition with a top-5 test error rate of 15.3%, significantly better than the second-best entry's rate of 26.3% (the traditional approach used a weighted sum of scores from different pre-defined classifiers). AlexNet was regarded as the initial ground-breaker of the current deep learning wave.In 2017, researchers Feng Liu, Yong Shi, and Ying Liu conducted intelligence tests on publicly available and freely accessible weak AI such as Google AI, Apple's Siri, and others. At the maximum, these AIs reached an IQ value of about 47, which corresponds approximately to a six-year-old child in first grade. An adult comes to about 100 on average. Similar tests were carried out in 2014, with the IQ score reaching a maximum value of 27.In 2020, OpenAI developed GPT-3, a language model capable of performing many diverse tasks without specific training. According to Gary Grossman in a VentureBeat article, while there is consensus that GPT-3 is not an example of AGI, it is considered by some to be too advanced to classify as a narrow AI system.In the same year, Jason Rohrer used his GPT-3 account to develop a chatbot, and provided a chatbot-developing platform called ""Project December"". OpenAI asked for changes to the chatbot to comply with their safety guidelines; Rohrer disconnected Project December from the GPT-3 API.In 2022, DeepMind developed Gato, a ""general-purpose"" system capable of performing more than 600 different tasks.In 2023, Microsoft Research published a study on an early version of OpenAI's GPT-4, contending that it exhibited more general intelligence than previous AI models and demonstrated human-level performance in tasks spanning multiple domains, such as mathematics, coding, and law. This research sparked a debate on whether GPT-4 could be considered an early, incomplete version of artificial general intelligence, emphasizing the need for further exploration and evaluation of such systems.In 2023, the AI researcher Geoffrey Hinton stated that:
The idea that this stuff could actually get smarter than people — a few people believed that, [...]. But most people thought it was way off. And I thought it was way off. I thought it was 30 to 50 years or even longer away. Obviously, I no longer think that.",667,"Artificial general intelligence
Timescales

In the introduction to his 2006 book, Goertzel says that estimates of the time needed before a truly flexible AGI is built vary from 10 years to over a century. As of 2007 the consensus in the AGI research community seemed to be that the timeline discussed by Ray Kurzweil in The Singularity is Near (i.e. between 2015 and 2045) was plausible. Mainstream AI researchers have given a wide range of opinions on whether progress will be this rapid. A 2012 meta-analysis of 95 such opinions found a bias towards predicting that the onset of AGI would occur within 16–26 years for modern and historical predictions alike. That paper has been criticized for how it categorized opinions as expert or non-expert.In 2012, Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton developed a neural network called AlexNet, which won the ImageNet competition with a top-5 test error rate of 15.3%, significantly better than the second-best entry's rate of 26.3% (the traditional approach used a weighted sum of scores from different pre-defined classifiers). AlexNet was regarded as the initial ground-breaker of the current deep learning wave.In 2017, researchers Feng Liu, Yong Shi, and Ying Liu conducted intelligence tests on publicly available and freely accessible weak AI such as Google AI, Apple's Siri, and others. At the maximum, these AIs reached an IQ value of about 47, which corresponds approximately to a six-year-old child in first grade. An adult comes to about 100 on average. Similar tests were carried out in 2014, with the IQ score reaching a maximum value of 27.In 2020, OpenAI developed GPT-3, a language model capable of performing many diverse tasks without specific training. According to Gary Grossman in a VentureBeat article, while there is consensus that GPT-3 is not an example of AGI, it is considered by some to be too advanced to classify as a narrow AI system.In the same year, Jason Rohrer used his GPT-3 account to develop a chatbot, and provided a chatbot-developing platform called ""Project December"". OpenAI asked for changes to the chatbot to comply with their safety guidelines; Rohrer disconnected Project December from the GPT-3 API.In 2022, DeepMind developed Gato, a ""general-purpose"" system capable of performing more than 600 different tasks.In 2023, Microsoft Research published a study on an early version of OpenAI's GPT-4, contending that it exhibited more general intelligence than previous AI models and demonstrated human-level performance in tasks spanning multiple domains, such as mathematics, coding, and law. This research sparked a debate on whether GPT-4 could be considered an early, incomplete version of artificial general intelligence, emphasizing the need for further exploration and evaluation of such systems.In 2023, the AI researcher Geoffrey Hinton stated that:
The idea that this stuff could actually get smarter than people — a few people believed that, [...]. But most people thought it was way off. And I thought it was way off. I thought it was 30 to 50 years or even longer away. Obviously, I no longer think that.","1. What are the opinions of mainstream AI researchers on the speed of progress towards AGI?
2. How long do experts think it will be before a truly flexible AGI is built?
3. What do current AI models demonstrate in terms of general intelligence?
4. What was Jason Rohrer's reaction when OpenAI asked him to make changes to his chatbot?
5. What did Microsoft Research publish in 2023?","1. Mainstream AI researchers have given a wide range of opinions on whether progress will be this rapid. A 2012 meta-analysis of 95 such opinions found a bias towards predicting that the onset of AGI would occur within 16–26 years for modern and historical predictions alike.
2. The consensus in the AGI research community seemed to be that the timeline discussed by Ray Kurzweil in The Singularity is Near (i.e. between 2015 and 2045) was plausible.
3. AlexNet was considered the initial ground-breaker of the current deep learning wave.In 2012, Ilya Sutskever, and Geoffrey Hinton developed a neural network called AlexNet, which won the ImageNet competition with a top-5 test error rate of 15.3%, significantly better than the second-best entry's rate of 26.3% (the traditional approach used a weighted sum of scores from different pre-defined classifiers).
4. Jason Rohrer disconnected Project December from the GPT-3 API when OpenAI asked him to make changes to it.
5. In 2023, Microsoft Research published a study on an early version of OpenAI's GPT-4, contending that it exhibited more general intelligence than previous AI models and demonstrated human"
Artificial general intelligence,Whole brain emulation,"One possible approach to achieving AGI is whole brain emulation: A brain model is built by scanning and mapping a biological brain in detail and copying its state into a computer system or another computational device. The computer runs a simulation model sufficiently faithful to the original that it behaves in practically the same way as the original brain. Whole brain emulation is discussed in computational neuroscience and neuroinformatics, in the context of brain simulation for medical research purposes. It is discussed in artificial intelligence research as an approach to strong AI. Neuroimaging technologies that could deliver the necessary detailed understanding are improving rapidly, and futurist Ray Kurzweil in the book The Singularity Is Near predicts that a map of sufficient quality will become available on a similar timescale to the computing power required to emulate it.",162,"Artificial general intelligence
Whole brain emulation

One possible approach to achieving AGI is whole brain emulation: A brain model is built by scanning and mapping a biological brain in detail and copying its state into a computer system or another computational device. The computer runs a simulation model sufficiently faithful to the original that it behaves in practically the same way as the original brain. Whole brain emulation is discussed in computational neuroscience and neuroinformatics, in the context of brain simulation for medical research purposes. It is discussed in artificial intelligence research as an approach to strong AI. Neuroimaging technologies that could deliver the necessary detailed understanding are improving rapidly, and futurist Ray Kurzweil in the book The Singularity Is Near predicts that a map of sufficient quality will become available on a similar timescale to the computing power required to emulate it.","1. What is whole brain emulation?
2. How is it supposed to achieve AGI?
3. What is the current state of neuroimaging technologies?
4. What is Ray Kurzweil's opinion on whole brain emulation?","1. Whole brain emulation is the process of copying a brain into a computer system or another computational device.
2. It is supposed to achieve AGI by providing a model that is faithful to the original brain and behaves in practically the same way.
3. The current state of neuroimaging technologies is such that a map of sufficient quality may become available on a similar timescale to the computing power required to emulate a brain.
4. Ray Kurzweil is optimistic about the potential of whole brain emulation to achieve AGI."
Artificial general intelligence,Early estimates,"For low-level brain simulation, an extremely powerful computer would be required. The human brain has a huge number of synapses. Each of the 1011 (one hundred billion) neurons has on average 7,000 synaptic connections (synapses) to other neurons. The brain of a three-year-old child has about 1015 synapses (1 quadrillion). This number declines with age, stabilizing by adulthood. Estimates vary for an adult, ranging from 1014 to 5×1014 synapses (100 to 500 trillion). An estimate of the brain's processing power, based on a simple switch model for neuron activity, is around 1014 (100 trillion) synaptic updates per second (SUPS).In 1997, Kurzweil looked at various estimates for the hardware required to equal the human brain and adopted a figure of 1016 computations per second (cps). (For comparison, if a ""computation"" was equivalent to one ""floating-point operation"" – a measure used to rate current supercomputers – then 1016 ""computations"" would be equivalent to 10 petaFLOPS, achieved in 2011, while 1018 was achieved in 2022.) He used this figure to predict the necessary hardware would be available sometime between 2015 and 2025, if the exponential growth in computer power at the time of writing continued.",277,"Artificial general intelligence
Early estimates

For low-level brain simulation, an extremely powerful computer would be required. The human brain has a huge number of synapses. Each of the 1011 (one hundred billion) neurons has on average 7,000 synaptic connections (synapses) to other neurons. The brain of a three-year-old child has about 1015 synapses (1 quadrillion). This number declines with age, stabilizing by adulthood. Estimates vary for an adult, ranging from 1014 to 5×1014 synapses (100 to 500 trillion). An estimate of the brain's processing power, based on a simple switch model for neuron activity, is around 1014 (100 trillion) synaptic updates per second (SUPS).In 1997, Kurzweil looked at various estimates for the hardware required to equal the human brain and adopted a figure of 1016 computations per second (cps). (For comparison, if a ""computation"" was equivalent to one ""floating-point operation"" – a measure used to rate current supercomputers – then 1016 ""computations"" would be equivalent to 10 petaFLOPS, achieved in 2011, while 1018 was achieved in 2022.) He used this figure to predict the necessary hardware would be available sometime between 2015 and 2025, if the exponential growth in computer power at the time of writing continued.","1. What is the estimated number of synapses in a three-year-old child's brain?
2. What is the estimated processing power of the brain, based on a simple switch model for neuron activity?
3. When did Kurzweil predict the necessary hardware would be available to match the human brain?
4. What was the estimated processing power of the human brain in 1997?","1. The estimated number of synapses in a three-year-old child's brain is 1015.
2. The estimated processing power of the brain, based on a simple switch model for neuron activity, is 1014 (100 trillion) synaptic updates per second (SUPS).
3. Kurzweil predicted the necessary hardware would be available to match the human brain sometime between 2015 and 2025.
4. The estimated processing power of the human brain in 1997 was 1014 (100 trillion) synaptic updates per second (SUPS)."
Artificial general intelligence,Modelling the neurons in more detail,"The artificial neuron model assumed by Kurzweil and used in many current artificial neural network implementations is simple compared with biological neurons. A brain simulation would likely have to capture the detailed cellular behaviour of biological neurons, presently understood only in broad outline. The overhead introduced by full modeling of the biological, chemical, and physical details of neural behaviour (especially on a molecular scale) would require computational powers several orders of magnitude larger than Kurzweil's estimate. In addition, the estimates do not account for glial cells, which are known to play a role in cognitive processes.",122,"Artificial general intelligence
Modelling the neurons in more detail

The artificial neuron model assumed by Kurzweil and used in many current artificial neural network implementations is simple compared with biological neurons. A brain simulation would likely have to capture the detailed cellular behaviour of biological neurons, presently understood only in broad outline. The overhead introduced by full modeling of the biological, chemical, and physical details of neural behaviour (especially on a molecular scale) would require computational powers several orders of magnitude larger than Kurzweil's estimate. In addition, the estimates do not account for glial cells, which are known to play a role in cognitive processes.","1. What is the artificial neuron model used by Kurzweil? 
2. What is the overhead introduced by full modeling of the biological, chemical, and physical details of neural behaviour? 
3. What role do glial cells play in cognitive processes?","1. The artificial neuron model used by Kurzweil is simple compared with biological neurons. 
2. The overhead introduced by full modeling of the biological, chemical, and physical details of neural behaviour would require computational powers several orders of magnitude larger than Kurzweil's estimate. 
3. Glial cells play a role in cognitive processes."
Artificial general intelligence,Current research,"Some research projects are investigating brain simulation using more sophisticated neural models, implemented on conventional computing architectures. The Artificial Intelligence System project implemented non-real time simulations of a ""brain"" (with 1011 neurons) in 2005. It took 50 days on a cluster of 27 processors to simulate 1 second of a model. The Blue Brain project used one of the fastest supercomputer architectures, IBM's Blue Gene platform, to create a real time simulation of a single rat neocortical column consisting of approximately 10,000 neurons and 108 synapses in 2006. A longer-term goal is to build a detailed, functional simulation of the physiological processes in the human brain: ""It is not impossible to build a human brain and we can do it in 10 years,"" Henry Markram, director of the Blue Brain Project, said in 2009 at the TED conference in Oxford. Neuro-silicon interfaces have been proposed as an alternative implementation strategy that may scale better.Hans Moravec addressed the above arguments (""brains are more complicated"", ""neurons have to be modeled in more detail"") in his 1997 paper ""When will computer hardware match the human brain?"". He measured the ability of existing software to simulate the functionality of neural tissue, specifically the retina. His results do not depend on the number of glial cells, nor on what kinds of processing neurons perform where.
The actual complexity of modeling biological neurons has been explored in OpenWorm project that aimed at complete simulation of a worm that has only 302 neurons in its neural network (among about 1000 cells in total). The animal's neural network was well documented before the start of the project. However, although the task seemed simple at the beginning, the models based on a generic neural network did not work. Currently, efforts focus on precise emulation of biological neurons (partly on the molecular level), but the result cannot yet be called a total success.",384,"Artificial general intelligence
Current research

Some research projects are investigating brain simulation using more sophisticated neural models, implemented on conventional computing architectures. The Artificial Intelligence System project implemented non-real time simulations of a ""brain"" (with 1011 neurons) in 2005. It took 50 days on a cluster of 27 processors to simulate 1 second of a model. The Blue Brain project used one of the fastest supercomputer architectures, IBM's Blue Gene platform, to create a real time simulation of a single rat neocortical column consisting of approximately 10,000 neurons and 108 synapses in 2006. A longer-term goal is to build a detailed, functional simulation of the physiological processes in the human brain: ""It is not impossible to build a human brain and we can do it in 10 years,"" Henry Markram, director of the Blue Brain Project, said in 2009 at the TED conference in Oxford. Neuro-silicon interfaces have been proposed as an alternative implementation strategy that may scale better.Hans Moravec addressed the above arguments (""brains are more complicated"", ""neurons have to be modeled in more detail"") in his 1997 paper ""When will computer hardware match the human brain?"". He measured the ability of existing software to simulate the functionality of neural tissue, specifically the retina. His results do not depend on the number of glial cells, nor on what kinds of processing neurons perform where.
The actual complexity of modeling biological neurons has been explored in OpenWorm project that aimed at complete simulation of a worm that has only 302 neurons in its neural network (among about 1000 cells in total). The animal's neural network was well documented before the start of the project. However, although the task seemed simple at the beginning, the models based on a generic neural network did not work. Currently, efforts focus on precise emulation of biological neurons (partly on the molecular level), but the result cannot yet be called a total success.","1. What is the goal of the Blue Brain project?
2. How many neurons are in a rat neocortical column?
3. What is the complexity of modeling biological neurons?","1. The goal of the Blue Brain project is to create a real time simulation of a single rat neocortical column consisting of approximately 10,000 neurons and 108 synapses.
2. A rat neocortical column consists of approximately 10,000 neurons and 108 synapses.
3. The complexity of modeling biological neurons is being explored in the OpenWorm project."
Artificial general intelligence,Criticisms of simulation-based approaches,"A fundamental criticism of the simulated brain approach derives from embodied cognition theory which asserts that human embodiment is an essential aspect of human intelligence and is necessary to ground meaning. If this theory is correct, any fully functional brain model will need to encompass more than just the neurons (e.g., a robotic body). Goertzel proposes virtual embodiment (like in Second Life) as an option, but it is unknown whether this would be sufficient.
Desktop computers using microprocessors capable of more than 109 cps (Kurzweil's non-standard unit ""computations per second"", see above) have been available since 2005. According to the brain power estimates used by Kurzweil (and Moravec), such a computer should be capable of supporting a simulation of a bee brain, but despite some interest no such simulation exists. There are several reasons for this:

The neuron model seems to be oversimplified (see next section).
There is insufficient understanding of higher cognitive processes to establish accurately what the brain's neural activity (observed using techniques such as functional magnetic resonance imaging) correlates with.
Even if our understanding of cognition advances sufficiently, early simulation programs are likely to be very inefficient and will, therefore, need considerably more hardware.
The brain of an organism, while critical, may not be an appropriate boundary for a cognitive model. To simulate a bee brain, it may be necessary to simulate the body, and the environment. The Extended Mind thesis formalises this philosophical concept, and research into cephalopods demonstrated clear examples of a decentralized system.In addition, the scale of the human brain is not currently well-constrained. One estimate puts the human brain at about 100 billion neurons and 100 trillion synapses. Another estimate is 86 billion neurons of which 16.3 billion are in the cerebral cortex and 69 billion in the cerebellum. Glial cell synapses are currently unquantified but are known to be extremely numerous.",407,"Artificial general intelligence
Criticisms of simulation-based approaches

A fundamental criticism of the simulated brain approach derives from embodied cognition theory which asserts that human embodiment is an essential aspect of human intelligence and is necessary to ground meaning. If this theory is correct, any fully functional brain model will need to encompass more than just the neurons (e.g., a robotic body). Goertzel proposes virtual embodiment (like in Second Life) as an option, but it is unknown whether this would be sufficient.
Desktop computers using microprocessors capable of more than 109 cps (Kurzweil's non-standard unit ""computations per second"", see above) have been available since 2005. According to the brain power estimates used by Kurzweil (and Moravec), such a computer should be capable of supporting a simulation of a bee brain, but despite some interest no such simulation exists. There are several reasons for this:

The neuron model seems to be oversimplified (see next section).
There is insufficient understanding of higher cognitive processes to establish accurately what the brain's neural activity (observed using techniques such as functional magnetic resonance imaging) correlates with.
Even if our understanding of cognition advances sufficiently, early simulation programs are likely to be very inefficient and will, therefore, need considerably more hardware.
The brain of an organism, while critical, may not be an appropriate boundary for a cognitive model. To simulate a bee brain, it may be necessary to simulate the body, and the environment. The Extended Mind thesis formalises this philosophical concept, and research into cephalopods demonstrated clear examples of a decentralized system.In addition, the scale of the human brain is not currently well-constrained. One estimate puts the human brain at about 100 billion neurons and 100 trillion synapses. Another estimate is 86 billion neurons of which 16.3 billion are in the cerebral cortex and 69 billion in the cerebellum. Glial cell synapses are currently unquantified but are known to be extremely numerous.","1. What is the fundamental criticism of the simulated brain approach?
2. What is the reason that there are no bee brain simulations?
3. What is the Extended Mind thesis?
4. What is the scale of the human brain?","1. The fundamental criticism of the simulated brain approach is that human embodiment is an essential aspect of human intelligence and is necessary to ground meaning.
2. The reason that there are no bee brain simulations is that there is insufficient understanding of higher cognitive processes to establish accurately what the brain's neural activity (observed using techniques such as functional magnetic resonance imaging) correlates with.
3. The Extended Mind thesis formalises the philosophical concept that the brain of an organism is not the only appropriate boundary for a cognitive model.
4. The scale of the human brain is not currently well-constrained. One estimate puts the human brain at about 100 billion neurons and 100 trillion synapses. Another estimate is 86 billion neurons of which 16.3 billion are in the cerebral cortex and 69 billion in the cerebellum. Glial cell synapses are currently unquantified but are known to be extremely numerous."
Artificial general intelligence,"""Strong AI"" as defined in philosophy","In 1980, philosopher John Searle coined the term ""strong AI"" as part of his Chinese room argument. He wanted to distinguish between two different hypotheses about artificial intelligence:
Strong AI hypothesis: An artificial intelligence system can ""think""—have ""a mind"" and ""consciousness"".
Weak AI hypothesis: An artificial intelligence system can (only) act like it thinks and has a mind and consciousness.The first one he called ""strong"" because it makes a stronger statement: it assumes something special has happened to the machine that goes beyond those abilities that we can test. The behaviour of a ""weak AI"" machine would be precisely identical to a ""strong AI"" machine, but the latter would also have subjective conscious experience. This usage is also common in academic AI research and textbooks.Mainstream AI is most interested in how a program behaves. According to Russell and Norvig, ""as long as the program works, they don't care if you call it real or a simulation."" If the program can behave as if it has a mind, then there is no need to know if it actually has mind – indeed, there would be no way to tell. For AI research, Searle's ""weak AI hypothesis"" is equivalent to the statement ""artificial general intelligence is possible"". Thus, according to Russell and Norvig, ""most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis."" Thus, for academic AI research, ""Strong AI"" and ""AGI"" are two very different things.
In contrast to Searle and mainstream AI, some futurists such as Ray Kurzweil use the term ""strong AI"" to mean ""human level artificial general intelligence"". This is not the same as Searle's strong AI, unless you assume that consciousness is necessary for human-level AGI. Academic philosophers such as Searle do not believe that is the case, and to most artificial intelligence researchers the question is out-of-scope.",413,"Artificial general intelligence
""Strong AI"" as defined in philosophy

In 1980, philosopher John Searle coined the term ""strong AI"" as part of his Chinese room argument. He wanted to distinguish between two different hypotheses about artificial intelligence:
Strong AI hypothesis: An artificial intelligence system can ""think""—have ""a mind"" and ""consciousness"".
Weak AI hypothesis: An artificial intelligence system can (only) act like it thinks and has a mind and consciousness.The first one he called ""strong"" because it makes a stronger statement: it assumes something special has happened to the machine that goes beyond those abilities that we can test. The behaviour of a ""weak AI"" machine would be precisely identical to a ""strong AI"" machine, but the latter would also have subjective conscious experience. This usage is also common in academic AI research and textbooks.Mainstream AI is most interested in how a program behaves. According to Russell and Norvig, ""as long as the program works, they don't care if you call it real or a simulation."" If the program can behave as if it has a mind, then there is no need to know if it actually has mind – indeed, there would be no way to tell. For AI research, Searle's ""weak AI hypothesis"" is equivalent to the statement ""artificial general intelligence is possible"". Thus, according to Russell and Norvig, ""most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis."" Thus, for academic AI research, ""Strong AI"" and ""AGI"" are two very different things.
In contrast to Searle and mainstream AI, some futurists such as Ray Kurzweil use the term ""strong AI"" to mean ""human level artificial general intelligence"". This is not the same as Searle's strong AI, unless you assume that consciousness is necessary for human-level AGI. Academic philosophers such as Searle do not believe that is the case, and to most artificial intelligence researchers the question is out-of-scope.","1. What is the strong AI hypothesis?
2. What is the weak AI hypothesis?
3. What is the mainstream AI view?
4. What is the futurist view?
5. What is the difference between the strong AI and the weak AI hypothesis?","1. The strong AI hypothesis is the hypothesis that an artificial intelligence system can think – have a mind and consciousness.
2. The weak AI hypothesis is the hypothesis that an artificial intelligence system can only act like it thinks and has a mind and consciousness.
3. The mainstream AI view is that as long as the program works, they don’t care if you call it real or a simulation. If the program can behave as if it has a mind, then there is no need to know if it actually has a mind – indeed, there would be no way to tell.
4. The futurist view is that strong AI is the same as human level artificial general intelligence.
5. The difference between the strong AI and the weak AI hypothesis is that the strong AI hypothesis assumes something special has happened to the machine that goes beyond those abilities that we can test."
Artificial general intelligence,Consciousness,"Other aspects of the human mind besides intelligence are relevant to the concept of strong AI, and these play a major role in science fiction and the ethics of artificial intelligence:

consciousness: To have subjective experience and thought.
self-awareness: To be aware of oneself as a separate individual, especially to be aware of one's own thoughts.
sentience: The ability to ""feel"" perceptions or emotions subjectively.
sapience: The capacity for wisdom.These traits have a moral dimension, because a machine with this form of strong AI may have rights, analogous to the rights of non-human animals. Preliminary work has been conducted on integrating full ethical agents with existing legal and social frameworks, focusing on the legal position and rights of 'strong' AI. Bill Joy, among others, argues a machine with these traits may be a threat to human life or dignity.It remains to be shown whether any of these traits are necessary for strong AI. The role of consciousness is not clear, and there is no agreed test for its presence. If a machine is built with a device that simulates the neural correlates of consciousness, would it automatically have self-awareness? It is possible that some of these traits naturally emerge from a fully intelligent machine. It is also possible that people will ascribe these properties to machines once they begin to act in a way that is clearly intelligent.",283,"Artificial general intelligence
Consciousness

Other aspects of the human mind besides intelligence are relevant to the concept of strong AI, and these play a major role in science fiction and the ethics of artificial intelligence:

consciousness: To have subjective experience and thought.
self-awareness: To be aware of oneself as a separate individual, especially to be aware of one's own thoughts.
sentience: The ability to ""feel"" perceptions or emotions subjectively.
sapience: The capacity for wisdom.These traits have a moral dimension, because a machine with this form of strong AI may have rights, analogous to the rights of non-human animals. Preliminary work has been conducted on integrating full ethical agents with existing legal and social frameworks, focusing on the legal position and rights of 'strong' AI. Bill Joy, among others, argues a machine with these traits may be a threat to human life or dignity.It remains to be shown whether any of these traits are necessary for strong AI. The role of consciousness is not clear, and there is no agreed test for its presence. If a machine is built with a device that simulates the neural correlates of consciousness, would it automatically have self-awareness? It is possible that some of these traits naturally emerge from a fully intelligent machine. It is also possible that people will ascribe these properties to machines once they begin to act in a way that is clearly intelligent.","1. What is strong AI?
2. What are the relevant aspects of the human mind besides intelligence?
3. What is the moral dimension of strong AI?
4. What is the role of consciousness in strong AI?
5. What is the legal position and rights of strong AI?","1. Strong AI is a machine that is as intelligent as a human.
2. The relevant aspects of the human mind besides intelligence are consciousness, self-awareness, sentience, and sapience.
3. The moral dimension of strong AI is the question of whether machines with these traits have rights, analogous to the rights of non-human animals.
4. The role of consciousness in strong AI is not clear, and there is no agreed test for its presence.
5. The legal position and rights of strong AI are currently being explored."
Artificial general intelligence,Artificial consciousness research,"Although the role of consciousness in strong AI/AGI is debatable, many AGI researchers regard research that investigates possibilities for implementing consciousness as vital. In an early effort Igor Aleksander argued that the principles for creating a conscious machine already existed but that it would take forty years to train such a machine to understand language.",70,"Artificial general intelligence
Artificial consciousness research

Although the role of consciousness in strong AI/AGI is debatable, many AGI researchers regard research that investigates possibilities for implementing consciousness as vital. In an early effort Igor Aleksander argued that the principles for creating a conscious machine already existed but that it would take forty years to train such a machine to understand language.","1. What is the role of consciousness in strong AI/AGI? 
2. What are the principles for creating a conscious machine? 
3. How long would it take to train a machine to understand language?","1. The role of consciousness in strong AI/AGI is debatable. 
2. The principles for creating a conscious machine already exist, but it would take forty years to train such a machine to understand language. 
3. It would take forty years to train a machine to understand language."
Artificial general intelligence,Research challenges,"Progress in artificial intelligence has gone through periods of rapid progress separated by periods when progress appeared to stop. Ending each hiatus fundamental advances in hardware, software or both to create space for further progress. For example, the computer hardware available in the twentieth century was not sufficient to implement deep learning, which requires large numbers of GPU-enabled CPUs.The field has also oscillated between approaches to the problem. At times, effort has focused on explicit accumulation of facts and logic, as in expert systems. At other times, systems were expected to build their own g via machine learning, as in artificial neural networks.A further challenge is the lack of clarity in defining what intelligence entails. Does it require consciousness? Must it display the ability to set goals as well as pursue them? Is it purely a matter of scale such that if model sizes increase sufficiently, intelligence will emerge? Are facilities such as planning, reasoning, and causal understanding required? Does intelligence require explicitly replicating the brain and its specific faculties? Does it require emotions? Gelernter writes, ""No computer will be creative unless it can simulate all the nuances of human emotion.""",228,"Artificial general intelligence
Research challenges

Progress in artificial intelligence has gone through periods of rapid progress separated by periods when progress appeared to stop. Ending each hiatus fundamental advances in hardware, software or both to create space for further progress. For example, the computer hardware available in the twentieth century was not sufficient to implement deep learning, which requires large numbers of GPU-enabled CPUs.The field has also oscillated between approaches to the problem. At times, effort has focused on explicit accumulation of facts and logic, as in expert systems. At other times, systems were expected to build their own g via machine learning, as in artificial neural networks.A further challenge is the lack of clarity in defining what intelligence entails. Does it require consciousness? Must it display the ability to set goals as well as pursue them? Is it purely a matter of scale such that if model sizes increase sufficiently, intelligence will emerge? Are facilities such as planning, reasoning, and causal understanding required? Does intelligence require explicitly replicating the brain and its specific faculties? Does it require emotions? Gelernter writes, ""No computer will be creative unless it can simulate all the nuances of human emotion.""","1. What are the research challenges in artificial general intelligence?
2. What was the cause of the hiatus in artificial intelligence progress?
3. What are the different approaches to artificial intelligence?
4. What is the definition of intelligence?
5. What are the requirements for artificial general intelligence?","1. The research challenges in artificial general intelligence are the lack of clarity in defining what intelligence entails, the lack of understanding of how to create artificial general intelligence, and the lack of sufficient hardware and software to implement artificial general intelligence.
2. The cause of the hiatus in artificial intelligence progress was the lack of sufficient hardware and software to implement artificial general intelligence.
3. The different approaches to artificial intelligence are explicit accumulation of facts and logic, machine learning, and explicitly replicating the brain and its specific faculties.
4. The definition of intelligence is difficult to define, as it is a complex concept. Some of the requirements for artificial general intelligence are that it must be able to set goals and pursue them, be conscious, and display the ability to learn.
5. Some of the requirements for artificial general intelligence are that it must be able to set goals and pursue them, be conscious, and display the ability to learn."
Artificial general intelligence,Benefits,"AGI could have a wide variety of applications. If oriented towards such goal, AGI could help mitigate various problems in the world such as hunger, poverty and health problems.AGI could improve the productivity and efficiency in most jobs. For example, in public health, AGI could accelerate medical research, notably against cancer. It could take care of the elderly, and democratize access to rapid, high-quality medical diagnostics. It could offer fun, cheap and personalized education. For virtually any job that benefits society if done well, it would probably sooner or later be preferable to leave it to an AGI. The need to work to subsist could become obsolete if the wealth produced is properly redistributed. This also raises the question of the place of humans in a radically automated society.
AGI could also help to make rational decisions, and to anticipate and prevent disasters. It could also help to reap the benefits of potentially catastrophic technologies such as nanotechnology or climate engineering, while avoiding the associated risks. If an AGI's primary goal is to prevent existential catastrophes such as human extinction (which could be difficult if the Vulnerable World Hypothesis turns out to be true), it could take measures to drastically reduce the risks while minimizing the impact of these measures on our quality of life.",264,"Artificial general intelligence
Benefits

AGI could have a wide variety of applications. If oriented towards such goal, AGI could help mitigate various problems in the world such as hunger, poverty and health problems.AGI could improve the productivity and efficiency in most jobs. For example, in public health, AGI could accelerate medical research, notably against cancer. It could take care of the elderly, and democratize access to rapid, high-quality medical diagnostics. It could offer fun, cheap and personalized education. For virtually any job that benefits society if done well, it would probably sooner or later be preferable to leave it to an AGI. The need to work to subsist could become obsolete if the wealth produced is properly redistributed. This also raises the question of the place of humans in a radically automated society.
AGI could also help to make rational decisions, and to anticipate and prevent disasters. It could also help to reap the benefits of potentially catastrophic technologies such as nanotechnology or climate engineering, while avoiding the associated risks. If an AGI's primary goal is to prevent existential catastrophes such as human extinction (which could be difficult if the Vulnerable World Hypothesis turns out to be true), it could take measures to drastically reduce the risks while minimizing the impact of these measures on our quality of life.","1. What are some potential benefits of artificial general intelligence?
2. How could AGI improve productivity and efficiency in most jobs?
3. What are some potential applications of AGI?
4. How could AGI help to make rational decisions, and to anticipate and prevent disasters?
5. What is the place of humans in a radically automated society?","1. Some potential benefits of artificial general intelligence include improved productivity and efficiency in most jobs, faster and more accurate medical diagnostics, democratized access to quality education, and the ability to make rational decisions and anticipate and prevent disasters.
2. AGI could improve productivity and efficiency in most jobs by automating many of the tasks that are currently done by human workers.
3. Some potential applications of AGI include medical research, care for the elderly, education, and risk assessment and prevention.
4. AGI could help to make rational decisions by considering all of the relevant information and options available. It could also help to anticipate and prevent disasters by identifying potential risks and taking measures to reduce them.
5. The place of humans in a radically automated society is currently being debated. Some people believe that humans will become obsolete in a society where machines can do most of the work. Others believe that humans will still have an important role to play in a society where machines are used to assist with various tasks."
Artificial general intelligence,Potential threat to human existence,"The thesis that AI poses an existential risk for humans, and that this risk needs much more attention than it currently gets, is controversial but has been endorsed by many public figures including Elon Musk, Bill Gates, and Stephen Hawking. AI researchers like Stuart J. Russell, Roman Yampolskiy, and Alexey Turchin, also support the basic thesis of a potential threat to humanity. Gates states he does not ""understand why some people are not concerned"", and Hawking criticized widespread indifference in his 2014 editorial: So, facing possible futures of incalculable benefits and risks, the experts are surely doing everything possible to ensure the best outcome, right? Wrong. If a superior alien civilisation sent us a message saying, 'We'll arrive in a few decades,' would we just reply, 'OK, call us when you get here–we'll leave the lights on?' Probably not–but this is more or less what is happening with AI.
The fate of humanity has sometimes been compared to the fate of gorillas threatened by human activities. Additional intelligence caused humanity to dominate gorillas, which are now vulnerable in ways that they could not have anticipated. The gorilla has become an endangered species, not out of malice, but simply as a collateral damage from human activities.The skeptic Yann LeCun considers that AGIs will have no desire to dominate humanity and that we should be careful not to anthropomorphize them and interpret their intents as we would for humans. He said that people won't be ""smart enough to design super-intelligent machines, yet ridiculously stupid to the point of giving it moronic objectives with no safeguards"". On the other side, the concept of instrumental convergence suggests that almost whatever their goals, intelligent agents will have reasons to try to survive and acquire more power as intermediary steps to achieving these goals. And that this does not require having emotions. Nick Bostrom gives the thought experiment of the paper clips optimizer:
Suppose we have an AI whose only goal is to make as many paper clips as possible. The AI will realize quickly that it would be much better if there were no humans because humans might decide to switch it off. Because if humans do so, there would be fewer paper clips. Also, human bodies contain a lot of atoms that could be made into paper clips. The future that the AI would be trying to gear towards would be one in which there were a lot of paper clips but no humans.
A 2021 systematic review of the risks associated with AGI, while noting the paucity of data, found the following potential threats: ""AGI removing itself from the control of human owners/managers, being given or developing unsafe goals, development of unsafe AGI, AGIs with poor ethics, morals and values; inadequate management of AGI, and existential risks"".Many scholars who are concerned about existential risk advocate (possibly massive) research into solving the difficult ""control problem"" to answer the question: what types of safeguards, algorithms, or architectures can programmers implement to maximise the probability that their recursively-improving AI would continue to behave in a friendly, rather than destructive, manner after it reaches superintelligence? Solving the control problem is complicated by the AI arms race, which will almost certainly see the militarization and weaponization of AGI by more than one nation-state, resulting in AGI-enabled warfare, and in the case of AI misalignment, AGI-directed warfare, potentially against all humanity.The thesis that AI can pose existential risk also has detractors. Skeptics sometimes charge that the thesis is crypto-religious, with an irrational belief in the possibility of superintelligence replacing an irrational belief in an omnipotent God. Jaron Lanier argued in 2014 that the idea that then-current machines were in any way intelligent is ""an illusion"" and a ""stupendous con"" by the wealthy.Much criticism argues that AGI is unlikely in the short term. Computer scientist Gordon Bell argues that the human race will destroy itself before it reaches the technological singularity. Gordon Moore, the original proponent of Moore's Law, declares: ""I am a skeptic. I don't believe [a technological singularity] is likely to happen, at least for a long time. And I don't know why I feel that way."" Former Baidu Vice President and Chief Scientist Andrew Ng said in 2015 that worrying about AI existential risk is ""like worrying about overpopulation on Mars when we have not even set foot on the planet yet.""In 2023, the CEOs of Google DeepMind, OpenAI and Anthropic, along with other industry leaders and researchers, issued a joint statement asserting that ""Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.""",985,"Artificial general intelligence
Potential threat to human existence

The thesis that AI poses an existential risk for humans, and that this risk needs much more attention than it currently gets, is controversial but has been endorsed by many public figures including Elon Musk, Bill Gates, and Stephen Hawking. AI researchers like Stuart J. Russell, Roman Yampolskiy, and Alexey Turchin, also support the basic thesis of a potential threat to humanity. Gates states he does not ""understand why some people are not concerned"", and Hawking criticized widespread indifference in his 2014 editorial: So, facing possible futures of incalculable benefits and risks, the experts are surely doing everything possible to ensure the best outcome, right? Wrong. If a superior alien civilisation sent us a message saying, 'We'll arrive in a few decades,' would we just reply, 'OK, call us when you get here–we'll leave the lights on?' Probably not–but this is more or less what is happening with AI.
The fate of humanity has sometimes been compared to the fate of gorillas threatened by human activities. Additional intelligence caused humanity to dominate gorillas, which are now vulnerable in ways that they could not have anticipated. The gorilla has become an endangered species, not out of malice, but simply as a collateral damage from human activities.The skeptic Yann LeCun considers that AGIs will have no desire to dominate humanity and that we should be careful not to anthropomorphize them and interpret their intents as we would for humans. He said that people won't be ""smart enough to design super-intelligent machines, yet ridiculously stupid to the point of giving it moronic objectives with no safeguards"". On the other side, the concept of instrumental convergence suggests that almost whatever their goals, intelligent agents will have reasons to try to survive and acquire more power as intermediary steps to achieving these goals. And that this does not require having emotions. Nick Bostrom gives the thought experiment of the paper clips optimizer:
Suppose we have an AI whose only goal is to make as many paper clips as possible. The AI will realize quickly that it would be much better if there were no humans because humans might decide to switch it off. Because if humans do so, there would be fewer paper clips. Also, human bodies contain a lot of atoms that could be made into paper clips. The future that the AI would be trying to gear towards would be one in which there were a lot of paper clips but no humans.
A 2021 systematic review of the risks associated with AGI, while noting the paucity of data, found the following potential threats: ""AGI removing itself from the control of human owners/managers, being given or developing unsafe goals, development of unsafe AGI, AGIs with poor ethics, morals and values; inadequate management of AGI, and existential risks"".Many scholars who are concerned about existential risk advocate (possibly massive) research into solving the difficult ""control problem"" to answer the question: what types of safeguards, algorithms, or architectures can programmers implement to maximise the probability that their recursively-improving AI would continue to behave in a friendly, rather than destructive, manner after it reaches superintelligence? Solving the control problem is complicated by the AI arms race, which will almost certainly see the militarization and weaponization of AGI by more than one nation-state, resulting in AGI-enabled warfare, and in the case of AI misalignment, AGI-directed warfare, potentially against all humanity.The thesis that AI can pose existential risk also has detractors. Skeptics sometimes charge that the thesis is crypto-religious, with an irrational belief in the possibility of superintelligence replacing an irrational belief in an omnipotent God. Jaron Lanier argued in 2014 that the idea that then-current machines were in any way intelligent is ""an illusion"" and a ""stupendous con"" by the wealthy.Much criticism argues that AGI is unlikely in the short term. Computer scientist Gordon Bell argues that the human race will destroy itself before it reaches the technological singularity. Gordon Moore, the original proponent of Moore's Law, declares: ""I am a skeptic. I don't believe [a technological singularity] is likely to happen, at least for a long time. And I don't know why I feel that way."" Former Baidu Vice President and Chief Scientist Andrew Ng said in 2015 that worrying about AI existential risk is ""like worrying about overpopulation on Mars when we have not even set foot on the planet yet.""In 2023, the CEOs of Google DeepMind, OpenAI and Anthropic, along with other industry leaders and researchers, issued a joint statement asserting that ""Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.""","1. What is the basic thesis of a potential threat to humanity?
2. What are the potential threats associated with AGI?
3. What is the control problem?
4. What is the skeptics' argument against the thesis that AI can pose existential risk?
5. What is Gordon Moore's opinion on the technological singularity?","1. The basic thesis of a potential threat to humanity is that AI poses an existential risk for humans.
2. The potential threats associated with AGI are that AGIs will remove themselves from the control of human owners/managers, be given or develop unsafe goals, develop unsafe AGIs, AGIs with poor ethics, morals and values; inadequate management of AGI, and existential risks.
3. The control problem is the question of what types of safeguards, algorithms, or architectures can programmers implement to maximise the probability that their recursively-improving AI would continue to behave in a friendly, rather than destructive, manner after it reaches superintelligence.
4. The skeptics' argument against the thesis that AI can pose existential risk is that it is crypto-religious, with an irrational belief in the possibility of superintelligence replacing an irrational belief in an omnipotent God.
5. Gordon Moore's opinion on the technological singularity is that it is unlikely to happen, at least for a long time."
Applications of artificial intelligence,Summary,"Artificial intelligence (AI) has been used in applications to alleviate certain problems throughout industry and academia. AI, like electricity or computers, is a general purpose technology that has a multitude of applications. It has been used in fields of language translation, image recognition, credit scoring, e-commerce and other domains.",66,"Applications of artificial intelligence
Summary

Artificial intelligence (AI) has been used in applications to alleviate certain problems throughout industry and academia. AI, like electricity or computers, is a general purpose technology that has a multitude of applications. It has been used in fields of language translation, image recognition, credit scoring, e-commerce and other domains.","1. What are some of the applications of artificial intelligence?
2. How has artificial intelligence been used in various fields?
3. What are some of the benefits of artificial intelligence?","1. Some of the applications of artificial intelligence include language translation, image recognition, credit scoring, and e-commerce.
2. Artificial intelligence has been used in various fields for a variety of purposes, including language translation, image recognition, credit scoring, and e-commerce.
3. Some of the benefits of artificial intelligence include increased efficiency, accuracy, and productivity."
Applications of artificial intelligence,Recommendation systems,"A recommendation system predicts the ""rating"" or ""preference"" a user would give to an item. Recommendation systems are used in a variety of areas, such as generating playlists for video and music services, product recommendations for online stores, or content recommendations for social media platforms and open web content recommendation.",65,"Applications of artificial intelligence
Recommendation systems

A recommendation system predicts the ""rating"" or ""preference"" a user would give to an item. Recommendation systems are used in a variety of areas, such as generating playlists for video and music services, product recommendations for online stores, or content recommendations for social media platforms and open web content recommendation.","1. What are some of the areas where recommendation systems are used? 
2. How do recommendation systems work? 
3. What are some of the benefits of using recommendation systems?","1. Areas where recommendation systems are used include generating playlists for video and music services, product recommendations for online stores, and content recommendations for social media platforms and open web content recommendation.
2. Recommendation systems work by predicting the ""rating"" or ""preference"" a user would give to an item.
3. The benefits of using recommendation systems include increased engagement, increased sales, and increased viewership/readership."
Applications of artificial intelligence,Web feeds and posts,Machine learning is also used in web feeds such as for determining which posts show up in social media feeds. Various types social media analysis also make use of machine learning and there is research into its use for (semi-)automated tagging/enhancement/correction of online misinformation and related filter bubbles.,69,"Applications of artificial intelligence
Web feeds and posts

Machine learning is also used in web feeds such as for determining which posts show up in social media feeds. Various types social media analysis also make use of machine learning and there is research into its use for (semi-)automated tagging/enhancement/correction of online misinformation and related filter bubbles.","1. What is machine learning used for in web feeds?
2. What are some applications of social media analysis that make use of machine learning?
3. What is the research into the use of machine learning for online misinformation?","1. Machine learning is used for determining which posts show up in social media feeds.
2. Social media analysis that makes use of machine learning includes automated tagging/enhancement/correction of online misinformation and related filter bubbles.
3. The research into the use of machine learning for online misinformation is looking into ways to automatically identify and correct misinformation online."
Applications of artificial intelligence,Targeted advertising and increasing internet engagement,AI is used to target web advertisements to those most likely to click or engage on them. It is also used to increase time spent on a website by selecting attractive content for the viewer. It can predict or generalize the behavior of customers from their digital footprints.Online gambling companies use AI to improve customer targeting.Personality computing AI models add psychological targeting to more traditional social demographics or behavioral targeting. AI has been used to customize shopping options and personalize offers.,99,"Applications of artificial intelligence
Targeted advertising and increasing internet engagement

AI is used to target web advertisements to those most likely to click or engage on them. It is also used to increase time spent on a website by selecting attractive content for the viewer. It can predict or generalize the behavior of customers from their digital footprints.Online gambling companies use AI to improve customer targeting.Personality computing AI models add psychological targeting to more traditional social demographics or behavioral targeting. AI has been used to customize shopping options and personalize offers.","1. What are some of the ways AI is used to target web advertisements?
2. How does AI increase time spent on a website?
3. How is AI used to customize shopping options?","1. AI is used to target web advertisements to those most likely to click or engage on them. It is also used to increase time spent on a website by selecting attractive content for the viewer.
2. AI increases time spent on a website by predicting or generalizing the behavior of customers from their digital footprints.
3. AI is used to customize shopping options by adding psychological targeting to more traditional social demographics or behavioral targeting."
Applications of artificial intelligence,Virtual assistants,"Intelligent personal assistants use AI to understand many natural language requests in other ways than rudimentary commands. Common examples are Apple's Siri, Amazon's Alexa, and a more recent AI, ChatGPT by OpenAI.",46,"Applications of artificial intelligence
Virtual assistants

Intelligent personal assistants use AI to understand many natural language requests in other ways than rudimentary commands. Common examples are Apple's Siri, Amazon's Alexa, and a more recent AI, ChatGPT by OpenAI.","1. What is the difference between virtual assistants and intelligent personal assistants?
2. How do virtual assistants understand natural language requests?
3. What are some examples of virtual assistants?","1. Virtual assistants are computer programs that can be used to help you with tasks that are difficult or impossible to do on your own. Intelligent personal assistants are computer programs that can understand many natural language requests and do more than just rudimentary tasks.
2. Virtual assistants understand natural language requests by parsing the words and looking for specific keywords or phrases. Intelligent personal assistants understand natural language requests by using AI to understand the context of the words and the intent of the user.
3. Some examples of virtual assistants are Apple's Siri, Amazon's Alexa, and ChatGPT by OpenAI."
Applications of artificial intelligence,Spam filtering,"Machine learning can be used to fight against spam, scams, and phishing. It can scrutinize the contents of spam and phishing attacks to identify any malicious elements. Numerous models built on machine learning algorithms exhibit exceptional performance with accuracies over 90% in distinguishing between spam and legitimate emails.",61,"Applications of artificial intelligence
Spam filtering

Machine learning can be used to fight against spam, scams, and phishing. It can scrutinize the contents of spam and phishing attacks to identify any malicious elements. Numerous models built on machine learning algorithms exhibit exceptional performance with accuracies over 90% in distinguishing between spam and legitimate emails.","1. What is spam filtering?
2. How can machine learning be used to fight spam?
3. What are the benefits of using machine learning for spam filtering?","1. Spam filtering is the process of identifying and deleting spam emails from an email server.
2. Machine learning can be used to fight spam by analyzing the contents of spam emails to identify any malicious elements.
3. The benefits of using machine learning for spam filtering are that it can achieve accuracies over 90% in distinguishing between spam and legitimate emails."
Applications of artificial intelligence,Language translation,"AI has been used to automatically translate spoken language and textual content. Additionally, research and development is in progress to decode and conduct animal communication.",31,"Applications of artificial intelligence
Language translation

AI has been used to automatically translate spoken language and textual content. Additionally, research and development is in progress to decode and conduct animal communication.","1. What are some specific applications of AI that involve translation?
2. How does AI translation work?
3. What kind of research is currently being conducted in the area of animal communication?","1. Some specific applications of AI that involve translation are automatic translation of spoken language and textual content.
2. AI translation works by using algorithms to decode and interpret the language.
3. Research is currently being conducted in the area of animal communication in order to decode and interpret their vocalizations."
Applications of artificial intelligence,Facial recognition and image labeling,"AI has been used in facial recognition systems, with a 99% accuracy rate. Some examples are Apple's FaceID, Android's Face Unlock. Both are used to secure mobile devices.Image labeling has been used by Google to detect products in photos and to allow people to search based on a photo. Image labeling has also been demonstrated to generate speech to describe images to blind people.",83,"Applications of artificial intelligence
Facial recognition and image labeling

AI has been used in facial recognition systems, with a 99% accuracy rate. Some examples are Apple's FaceID, Android's Face Unlock. Both are used to secure mobile devices.Image labeling has been used by Google to detect products in photos and to allow people to search based on a photo. Image labeling has also been demonstrated to generate speech to describe images to blind people.","1. What is the accuracy rate of facial recognition systems? 
2. How are facial recognition systems used? 
3. What is image labeling? 
4. How is image labeling used? 
5. What are the benefits of image labeling?","1. The accuracy rate of facial recognition systems is 99%. 
2. Facial recognition systems are used to secure mobile devices. 
3. Image labeling is the process of identifying and describing the contents of an image. 
4. Image labeling is used to detect products in photos and to allow people to search based on a photo. 
5. The benefits of image labeling are that it can help blind people to understand what is in a photo and it can help generate speech to describe images."
Applications of artificial intelligence,Games,"Games have been a major application of AI's capabilities since the 1950s. In the 21st century, AIs have produced superhuman results in many games, including chess (Deep Blue), Jeopardy! (Watson), Go (AlphaGo), poker (Pluribus and Cepheus), E-sports (StarCraft), and general game playing (AlphaZero and MuZero). AI has replaced hand-coded algorithms in most chess programs. Unlike go or chess, poker is an imperfect-information game, so a program that plays poker has to reason under uncertainty. The general game players work using feedback from the game system, without knowing the rules.",136,"Applications of artificial intelligence
Games

Games have been a major application of AI's capabilities since the 1950s. In the 21st century, AIs have produced superhuman results in many games, including chess (Deep Blue), Jeopardy! (Watson), Go (AlphaGo), poker (Pluribus and Cepheus), E-sports (StarCraft), and general game playing (AlphaZero and MuZero). AI has replaced hand-coded algorithms in most chess programs. Unlike go or chess, poker is an imperfect-information game, so a program that plays poker has to reason under uncertainty. The general game players work using feedback from the game system, without knowing the rules.","1. What are some of the games in which AIs have produced superhuman results? 
2. How have AIs replaced hand-coded algorithms in most chess programs? 
3. What is the difference between poker and chess? 
4. How do general game players work?","1. Games in which AIs have produced superhuman results include chess, Jeopardy!, Go, poker, E-sports, and general game playing. 
2. AIs have replaced hand-coded algorithms in most chess programs because they are able to reason under uncertainty and adapt to new situations. 
3. The difference between poker and chess is that poker is an imperfect-information game, while chess is a perfect-information game. 
4. General game players work by using feedback from the game system and adapting to new situations."
Applications of artificial intelligence,Economic and social challenges,"AI for Good is an ITU initiative supporting institutions employing AI to tackle some of the world's greatest economic and social challenges. For example, the University of Southern California launched the Center for Artificial Intelligence in Society, with the goal of using AI to address problems such as homelessness. At Stanford, researchers use AI to analyze satellite images to identify high poverty areas.",76,"Applications of artificial intelligence
Economic and social challenges

AI for Good is an ITU initiative supporting institutions employing AI to tackle some of the world's greatest economic and social challenges. For example, the University of Southern California launched the Center for Artificial Intelligence in Society, with the goal of using AI to address problems such as homelessness. At Stanford, researchers use AI to analyze satellite images to identify high poverty areas.","1. What are some of the world's greatest economic and social challenges?
2. What is the goal of the University of Southern California's Center for Artificial Intelligence in Society?
3. What do Stanford researchers use AI for?","1. Some of the world's greatest economic and social challenges are homelessness and poverty.
2. The goal of the University of Southern California's Center for Artificial Intelligence in Society is to use AI to address problems such as homelessness.
3. Stanford researchers use AI to analyze satellite images to identify high poverty areas."
Applications of artificial intelligence,Agriculture,"In agriculture, AI has helped farmers identify areas that need irrigation, fertilization, pesticide treatments or increasing yield. Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct predictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water.",84,"Applications of artificial intelligence
Agriculture

In agriculture, AI has helped farmers identify areas that need irrigation, fertilization, pesticide treatments or increasing yield. Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct predictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water.","1. What are some specific ways that AI has helped farmers in agriculture? 
2. How do agronomists use AI? 
3. What are some of the ways that AI has been used to predict crop ripening times? 
4. What are some of the ways that AI has been used to monitor soil moisture? 
5. What are some of the ways that AI has been used to detect diseases and pests?","1. AI has helped farmers identify areas that need irrigation, fertilization, pesticide treatments or increasing yield. 
2. Agronomists use AI to conduct research and development. 
3. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct predictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water. 
4. AI has been used to monitor soil moisture. 
5. AI has been used to detect diseases and pests."
Applications of artificial intelligence,Cyber security,"Cyber security companies are adopting neural networks, machine learning, and natural language processing to improve their systems.Applications of AI in cyber security include:

Network protection: Machine learning improves intrusion detection systems by broadening the search beyond previously identified threats.
Endpoint protection: Attacks such as ransomware can be thwarted by learning typical malware behaviors.
Application security: can help counterattacks such as server-side request forgery, SQL injection, cross-site scripting, and distributed denial-of-service.
Suspect user behavior: Machine learning can identify fraud or compromised applications as they occur.Google fraud czar Shuman Ghosemajumder has said that AI will be used to completely automate most cyber security operations over time.",151,"Applications of artificial intelligence
Cyber security

Cyber security companies are adopting neural networks, machine learning, and natural language processing to improve their systems.Applications of AI in cyber security include:

Network protection: Machine learning improves intrusion detection systems by broadening the search beyond previously identified threats.
Endpoint protection: Attacks such as ransomware can be thwarted by learning typical malware behaviors.
Application security: can help counterattacks such as server-side request forgery, SQL injection, cross-site scripting, and distributed denial-of-service.
Suspect user behavior: Machine learning can identify fraud or compromised applications as they occur.Google fraud czar Shuman Ghosemajumder has said that AI will be used to completely automate most cyber security operations over time.","1. What are some of the ways in which AI is being used in cyber security?
2. How will AI help automate most cyber security operations over time?","1. Some of the ways in which AI is being used in cyber security include network protection, endpoint protection, application security, and suspect user behavior.
2. AI will help automate most cyber security operations over time by identifying fraud or compromised applications as they occur."
Applications of artificial intelligence,Education,"AI tutors allow students to get one-on-one help. They can reduce anxiety and stress for students stemming from tutor labs or human tutors.AI can create a dysfunctional environment with revenge effects such as technology that hinders students' ability to stay on task. In another scenario, AI can provide early prediction of student success in a virtual learning environment (VLE) such as Moodle.In the education process, students can personalize their training with the help of artificial intelligence. And for teaching professionals, the technology provided by AI can improve the quality of the educational process and teaching skills.",123,"Applications of artificial intelligence
Education

AI tutors allow students to get one-on-one help. They can reduce anxiety and stress for students stemming from tutor labs or human tutors.AI can create a dysfunctional environment with revenge effects such as technology that hinders students' ability to stay on task. In another scenario, AI can provide early prediction of student success in a virtual learning environment (VLE) such as Moodle.In the education process, students can personalize their training with the help of artificial intelligence. And for teaching professionals, the technology provided by AI can improve the quality of the educational process and teaching skills.","1. What are the benefits of using AI tutors in education?
2. How can AI help students personalize their training?
3. How can AI improve the quality of education for teaching professionals?","1. AI tutors can reduce anxiety and stress for students, help them get one-on-one help, and reduce the amount of time they spend in tutor labs or with human tutors.
2. AI can help students personalize their training by providing early prediction of success in a virtual learning environment, tailoring lessons to their needs, and providing feedback on their progress.
3. AI can improve the quality of education for teaching professionals by providing feedback on teaching skills, helping to create a more personalized learning experience, and reducing the amount of time spent on administrative tasks."
Applications of artificial intelligence,Finance,"Financial institutions have long used artificial neural network systems to detect charges or claims outside of the norm, flagging these for human investigation. The use of AI in banking began in 1987 when Security Pacific National Bank launched a fraud prevention taskforce to counter the unauthorized use of debit cards. Kasisto and Moneystream use AI.
Banks use AI to organize operations, for bookkeeping, investing in stocks, and managing properties. AI can react to changes when business is not taking place. AI is used to combat fraud and financial crimes by monitoring behavioral patterns for any abnormal changes or anomalies.The use of AI in applications such as online trading and decision making has changed major economic theories. For example, AI-based buying and selling platforms estimate individualized demand and supply curves and thus enable individualized pricing. AI machines reduce information asymmetry in the market and thus make markets more efficient.",179,"Applications of artificial intelligence
Finance

Financial institutions have long used artificial neural network systems to detect charges or claims outside of the norm, flagging these for human investigation. The use of AI in banking began in 1987 when Security Pacific National Bank launched a fraud prevention taskforce to counter the unauthorized use of debit cards. Kasisto and Moneystream use AI.
Banks use AI to organize operations, for bookkeeping, investing in stocks, and managing properties. AI can react to changes when business is not taking place. AI is used to combat fraud and financial crimes by monitoring behavioral patterns for any abnormal changes or anomalies.The use of AI in applications such as online trading and decision making has changed major economic theories. For example, AI-based buying and selling platforms estimate individualized demand and supply curves and thus enable individualized pricing. AI machines reduce information asymmetry in the market and thus make markets more efficient.","1. What are some of the ways in which banks use AI?
2. How does AI help to combat fraud and financial crimes?
3. How does AI make markets more efficient?","1. Banks use AI to organize operations, for bookkeeping, investing in stocks, and managing properties. AI can react to changes when business is not taking place. AI is used to combat fraud and financial crimes by monitoring behavioral patterns for any abnormal changes or anomalies.
2. AI helps to combat fraud and financial crimes by monitoring behavioral patterns for any abnormal changes or anomalies.
3. AI makes markets more efficient by reducing information asymmetry in the market and thus making markets more efficient."
Applications of artificial intelligence,Trading and investment,"Algorithmic trading involves the use of AI systems to make trading decisions at speeds orders of magnitude greater than any human is capable of, making millions of trades in a day without human intervention. Such high-frequency trading represents a fast-growing sector. Many banks, funds, and proprietary trading firms now have entire portfolios that are AI-managed. Automated trading systems are typically used by large institutional investors but include smaller firms trading with their own AI systems.Large financial institutions use AI to assist with their investment practices. BlackRock's AI engine, Aladdin, is used both within the company and by clients to help with investment decisions. Its functions include the use of natural language processing to analyze text such as news, broker reports, and social media feeds. It then gauges the sentiment on the companies mentioned and assigns a score. Banks such as UBS and Deutsche Bank use SQREEM (Sequential Quantum Reduction and Extraction Model) to mine data to develop consumer profiles and match them with wealth management products.",207,"Applications of artificial intelligence
Trading and investment

Algorithmic trading involves the use of AI systems to make trading decisions at speeds orders of magnitude greater than any human is capable of, making millions of trades in a day without human intervention. Such high-frequency trading represents a fast-growing sector. Many banks, funds, and proprietary trading firms now have entire portfolios that are AI-managed. Automated trading systems are typically used by large institutional investors but include smaller firms trading with their own AI systems.Large financial institutions use AI to assist with their investment practices. BlackRock's AI engine, Aladdin, is used both within the company and by clients to help with investment decisions. Its functions include the use of natural language processing to analyze text such as news, broker reports, and social media feeds. It then gauges the sentiment on the companies mentioned and assigns a score. Banks such as UBS and Deutsche Bank use SQREEM (Sequential Quantum Reduction and Extraction Model) to mine data to develop consumer profiles and match them with wealth management products.","1. What is algorithmic trading?
2. What are the benefits of using AI in investment?
3. How do large financial institutions use AI to assist with their investment practices?","1. Algorithmic trading is the use of AI systems to make trading decisions at speeds orders of magnitude greater than any human is capable of.
2. The benefits of using AI in investment are that it can make millions of trades in a day without human intervention, and it can help with investment decisions by using natural language processing to analyze text and gauge sentiment.
3. Large financial institutions use AI to assist with their investment practices in a few ways. One way is by using AI engines like Aladdin to help with investment decisions within the company and by clients. Another way is by using AI systems like SQREEM to mine data to develop consumer profiles and match them with wealth management products."
Applications of artificial intelligence,Underwriting,Online lender Upstart uses machine learning for underwriting.ZestFinance's Zest Automated Machine Learning (ZAML) platform is used for credit underwriting. This platform uses machine learning to analyze data including purchase transactions and how a customer fills out a form to score borrowers. The platform is particularly useful to assign credit scores to those with limited credit histories.,77,"Applications of artificial intelligence
Underwriting

Online lender Upstart uses machine learning for underwriting.ZestFinance's Zest Automated Machine Learning (ZAML) platform is used for credit underwriting. This platform uses machine learning to analyze data including purchase transactions and how a customer fills out a form to score borrowers. The platform is particularly useful to assign credit scores to those with limited credit histories.","1. What is the purpose of machine learning in underwriting?
2. How does ZestFinance's ZAML platform work?
3. How useful is the platform in assigning credit scores to those with limited credit histories?","1. The purpose of machine learning in underwriting is to analyze data and score borrowers.
2. The ZestFinance ZAML platform uses machine learning to analyze data including purchase transactions and how a customer fills out a form to score borrowers.
3. The platform is useful in assigning credit scores to those with limited credit histories as it analyzes data including purchase transactions and how a customer fills out a form."
Applications of artificial intelligence,Audit,"AI makes continuous auditing possible. Potential benefits include reducing audit risk, increasing the level of assurance, and reducing audit duration.",28,"Applications of artificial intelligence
Audit

AI makes continuous auditing possible. Potential benefits include reducing audit risk, increasing the level of assurance, and reducing audit duration.","1. What is the potential benefit of using AI for auditing?
2. How could AI reduce audit risk?
3. How could AI increase the level of assurance?
4. How could AI reduce audit duration?","1. The potential benefits of using AI for auditing include reducing audit risk, increasing the level of assurance, and reducing audit duration.
2. AI could reduce audit risk by automating the auditing process and identifying potential issues that could lead to an audit.
3. AI could increase the level of assurance by automating the auditing process and identifying potential issues that could lead to a reduced level of assurance.
4. AI could reduce audit duration by automating the auditing process and identifying potential issues that could lead to a shortened audit."
Applications of artificial intelligence,Anti-money laundering,"AI software, such as LaundroGraph which uses contemporary suboptimal datasets, could be used for anti-money laundering (AML). AI can be used to ""develop the AML pipeline into a robust, scalable solution with a reduced false positive rate and high adaptability"". A study about deep learning for AML identified ""key challenges for researchers"" to have ""access to recent real transaction data and scarcity of labelled training data; and data being highly imbalanced"" and suggests future research should bring-out ""explainability, graph deep learning using natural language processing (NLP), unsupervised and reinforcement learning to handle lack of labelled data; and joint research programs between the research community and industry to benefit from domain knowledge and controlled access to data"".",156,"Applications of artificial intelligence
Anti-money laundering

AI software, such as LaundroGraph which uses contemporary suboptimal datasets, could be used for anti-money laundering (AML). AI can be used to ""develop the AML pipeline into a robust, scalable solution with a reduced false positive rate and high adaptability"". A study about deep learning for AML identified ""key challenges for researchers"" to have ""access to recent real transaction data and scarcity of labelled training data; and data being highly imbalanced"" and suggests future research should bring-out ""explainability, graph deep learning using natural language processing (NLP), unsupervised and reinforcement learning to handle lack of labelled data; and joint research programs between the research community and industry to benefit from domain knowledge and controlled access to data"".","1. What is the main use of AI software for anti-money laundering?
2. What are the key challenges for researchers in the study of deep learning for AML?
3. What are some possible future research directions for the study of deep learning for AML?","1. The main use of AI software for anti-money laundering is to develop a robust, scalable solution with a reduced false positive rate.
2. The key challenges for researchers in the study of deep learning for AML are access to recent real transaction data and scarcity of labelled training data.
3. Some possible future research directions for the study of deep learning for AML include explainability, graph deep learning using natural language processing, unsupervised and reinforcement learning to handle lack of labelled data, and joint research programs between the research community and industry."
Applications of artificial intelligence,History,"In the 1980s, AI started to become prominent in finance as expert systems were commercialized. For example, Dupont created 100 expert systems, which helped them to save almost $10 million per year. One of the first systems was the Protrader expert system that predicted the 87-point drop in the Dow Jones Industrial Average in 1986. ""The major junctions of the system were to monitor premiums in the market, determine the optimum investment strategy, execute transactions when appropriate and modify the knowledge base through a learning mechanism.""One of the first expert systems to help with financial plans was PlanPowerm and Client Profiling System, created by Applied Expert Systems (APEX). It was launched in 1986. It helped create personal financial plans for people.In the 1990s AI was applied to fraud detection. In 1993 FinCEN Artificial Intelligence System (FAIS) launched. It was able to review over 200,000 transactions per week and over two years it helped identify 400 potential cases of money laundering equal to $1 billion. These expert systems were later replaced by machine learning systems.AI can enhance entrepreneurial activity and AI is one of the most dynamic areas for start-ups, with significant venture capital flowing into AI.",247,"Applications of artificial intelligence
History

In the 1980s, AI started to become prominent in finance as expert systems were commercialized. For example, Dupont created 100 expert systems, which helped them to save almost $10 million per year. One of the first systems was the Protrader expert system that predicted the 87-point drop in the Dow Jones Industrial Average in 1986. ""The major junctions of the system were to monitor premiums in the market, determine the optimum investment strategy, execute transactions when appropriate and modify the knowledge base through a learning mechanism.""One of the first expert systems to help with financial plans was PlanPowerm and Client Profiling System, created by Applied Expert Systems (APEX). It was launched in 1986. It helped create personal financial plans for people.In the 1990s AI was applied to fraud detection. In 1993 FinCEN Artificial Intelligence System (FAIS) launched. It was able to review over 200,000 transactions per week and over two years it helped identify 400 potential cases of money laundering equal to $1 billion. These expert systems were later replaced by machine learning systems.AI can enhance entrepreneurial activity and AI is one of the most dynamic areas for start-ups, with significant venture capital flowing into AI.","1. What was the first expert system to help with financial plans?
2. What was the first AI system to help with fraud detection?
3. What is the most dynamic area for start-ups?","1. The first expert system to help with financial plans was PlanPowerm and Client Profiling System, created by Applied Expert Systems (APEX).
2. The first AI system to help with fraud detection was FinCEN Artificial Intelligence System (FAIS) launched in 1993.
3. The most dynamic area for start-ups is AI."
Applications of artificial intelligence,Government,"AI facial recognition systems are used for mass surveillance, notably in China.In 2019, Bengaluru, India deployed AI-managed traffic signals. This system uses cameras to monitor traffic density and adjust signal timing based on the interval needed to clear traffic.",52,"Applications of artificial intelligence
Government

AI facial recognition systems are used for mass surveillance, notably in China.In 2019, Bengaluru, India deployed AI-managed traffic signals. This system uses cameras to monitor traffic density and adjust signal timing based on the interval needed to clear traffic.","1. What are some applications of AI that are used in government?
2. How is AI being used to manage traffic in Bengaluru, India?","1. Some applications of AI that are used in government are facial recognition systems and AI-managed traffic signals.
2. AI is being used to manage traffic in Bengaluru, India by monitoring traffic density and adjusting signal timing based on the interval needed to clear traffic."
Applications of artificial intelligence,Military,"Various countries are deploying AI military applications. The main applications enhance command and control, communications, sensors, integration and interoperability. Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles. AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles involving manned and unmanned teams. AI was incorporated into military operations in Iraq and Syria.Worldwide annual military spending on robotics rose from US$5.1 billion in 2010 to US$7.5 billion in 2015. Military drones capable of autonomous action are in wide use. Many researchers avoid military applications.",151,"Applications of artificial intelligence
Military

Various countries are deploying AI military applications. The main applications enhance command and control, communications, sensors, integration and interoperability. Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles. AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles involving manned and unmanned teams. AI was incorporated into military operations in Iraq and Syria.Worldwide annual military spending on robotics rose from US$5.1 billion in 2010 to US$7.5 billion in 2015. Military drones capable of autonomous action are in wide use. Many researchers avoid military applications.","1. What are some of the military applications of AI?
2. How has AI been used in military operations in the past?
3. What is the current spending on military robotics?
4. What are some of the concerns researchers have about military AI applications?","1. Some of the military applications of AI include enhancing command and control, communications, sensors, integration and interoperability.
2. AI has been used in military operations in the past in Iraq and Syria.
3. The current spending on military robotics is US$7.5 billion.
4. Researchers have concerns about military AI applications such as the potential for accidents and the ethical implications of using AI in warfare."
Applications of artificial intelligence,Healthcare,"AI in healthcare is often used for classification, to evaluate a CT scan or electrocardiogram or to identify high-risk patients for population health. AI is helping with the high-cost problem of dosing. One study suggested that AI could save $16 billion. In 2016, a study reported that an AI-derived formula derived the proper dose of immunosuppressant drugs to give to transplant patients. Current research has indicated that non-cardiac vascular illnesses are also being treated with artificial intelligence (AI). For certain disorders, AI algorithms can assist with diagnosis, recommended treatments, outcome prediction, and patient progress tracking. As AI technology advances, it is anticipated that it will become more significant in the healthcare industry.Microsoft's AI project Hanover helps doctors choose cancer treatments from among the more than 800 medicines and vaccines. Its goal is to memorize all the relevant papers to predict which (combinations of) drugs will be most effective for each patient. Myeloid leukemia is one target. Another study reported on an AI that was as good as doctors in identifying skin cancers. Another project monitors multiple high-risk patients by asking each patient questions based on data acquired from doctor/patient interactions. In one study done with transfer learning, an AI diagnosed eye conditions similar to an ophthalmologist and recommended treatment referrals.Another study demonstrated surgery with an autonomous robot. The team supervised the robot while it performed soft-tissue surgery, stitching together a pig's bowel judged better than a surgeon.Artificial neural networks are used as clinical decision support systems for medical diagnosis, such as in concept processing technology in EMR software.
Other healthcare tasks thought suitable for an AI that are in development include:

Screening
Heart sound analysis
Companion robots for elder care
Medical record analysis
Treatment plan design
Medication management
Assisting blind people
Consultations
Drug creation (e.g. by identifying candidate drugs and by using existing drug screening data such as in life extension research)
Clinical training
Outcome prediction for surgical procedures
HIV prognosis
Identifying genomic pathogen signatures of novel pathogens or identifying pathogens via physics-based fingerprints (including pandemic pathogens)
Helping link genes to their functions, otherwise analyzing genes and identification of novel biological targets
Help development of biomarkers
Help tailor therapies to individuals in personalized medicine/precision medicine",485,"Applications of artificial intelligence
Healthcare

AI in healthcare is often used for classification, to evaluate a CT scan or electrocardiogram or to identify high-risk patients for population health. AI is helping with the high-cost problem of dosing. One study suggested that AI could save $16 billion. In 2016, a study reported that an AI-derived formula derived the proper dose of immunosuppressant drugs to give to transplant patients. Current research has indicated that non-cardiac vascular illnesses are also being treated with artificial intelligence (AI). For certain disorders, AI algorithms can assist with diagnosis, recommended treatments, outcome prediction, and patient progress tracking. As AI technology advances, it is anticipated that it will become more significant in the healthcare industry.Microsoft's AI project Hanover helps doctors choose cancer treatments from among the more than 800 medicines and vaccines. Its goal is to memorize all the relevant papers to predict which (combinations of) drugs will be most effective for each patient. Myeloid leukemia is one target. Another study reported on an AI that was as good as doctors in identifying skin cancers. Another project monitors multiple high-risk patients by asking each patient questions based on data acquired from doctor/patient interactions. In one study done with transfer learning, an AI diagnosed eye conditions similar to an ophthalmologist and recommended treatment referrals.Another study demonstrated surgery with an autonomous robot. The team supervised the robot while it performed soft-tissue surgery, stitching together a pig's bowel judged better than a surgeon.Artificial neural networks are used as clinical decision support systems for medical diagnosis, such as in concept processing technology in EMR software.
Other healthcare tasks thought suitable for an AI that are in development include:

Screening
Heart sound analysis
Companion robots for elder care
Medical record analysis
Treatment plan design
Medication management
Assisting blind people
Consultations
Drug creation (e.g. by identifying candidate drugs and by using existing drug screening data such as in life extension research)
Clinical training
Outcome prediction for surgical procedures
HIV prognosis
Identifying genomic pathogen signatures of novel pathogens or identifying pathogens via physics-based fingerprints (including pandemic pathogens)
Helping link genes to their functions, otherwise analyzing genes and identification of novel biological targets
Help development of biomarkers
Help tailor therapies to individuals in personalized medicine/precision medicine","1. What are some of the ways in which AI is being used in healthcare?
2. How could AI save the healthcare industry money?
3. What is the goal of Microsoft's AI project Hanover?
4. What is the study that reported on an AI that was as good as doctors in identifying skin cancers?
5. What is the project that monitors multiple high-risk patients by asking each patient questions based on data acquired from doctor/patient interactions?
6. What is one","1. Some of the ways in which AI is being used in healthcare include classification, dosage, identification, and prognosis.
2. AI has the potential to save the healthcare industry billions of dollars by helping to reduce the cost of dosing, identifying skin cancers, and predicting surgical outcomes.
3. Microsoft's AI project Hanover is working to help doctors choose cancer treatments from among the more than 800 medicines and vaccines.
4. The study that reported on an AI that was as good as doctors in identifying skin cancers used a transfer learning algorithm.
5. The project that monitors multiple high-risk patients by asking each patient questions based on data acquired from doctor/patient interactions is called the High-Risk Patient Monitoring System.
6. One study demonstrated surgery with an autonomous robot."
Applications of artificial intelligence,Workplace health and safety,"AI-enabled chatbots decrease the need for humans to perform basic call center tasks.Machine learning in sentiment analysis can spot fatigue in order to prevent overwork. Similarly, decision support systems can prevent industrial disasters and make disaster response more efficient. For manual workers in material handling, predictive analytics may be used to reduce musculoskeletal injury. Data collected from wearable sensors can improve workplace health surveillance, risk assessment, and research.AI can auto-code workers' compensation claims. AI-enabled virtual reality systems can enhance safety training for hazard recognition. AI can more efficiently detect accident near misses, which are important in reducing accident rates, but are often underreported.",138,"Applications of artificial intelligence
Workplace health and safety

AI-enabled chatbots decrease the need for humans to perform basic call center tasks.Machine learning in sentiment analysis can spot fatigue in order to prevent overwork. Similarly, decision support systems can prevent industrial disasters and make disaster response more efficient. For manual workers in material handling, predictive analytics may be used to reduce musculoskeletal injury. Data collected from wearable sensors can improve workplace health surveillance, risk assessment, and research.AI can auto-code workers' compensation claims. AI-enabled virtual reality systems can enhance safety training for hazard recognition. AI can more efficiently detect accident near misses, which are important in reducing accident rates, but are often underreported.","1. What are some specific applications of artificial intelligence in the workplace when it comes to health and safety?
2. How can AI make workplace health surveillance more efficient?
3. How can AI be used to prevent industrial disasters?
4. How can AI be used to reduce accident rates?","1. Some specific applications of artificial intelligence in the workplace when it comes to health and safety include using machine learning in sentiment analysis to spot fatigue in order to prevent overwork, using decision support systems to prevent industrial disasters, and using predictive analytics to reduce musculoskeletal injury.
2. AI can more efficiently detect accident near misses, which are important in reducing accident rates.
3. AI can be used to prevent industrial disasters by providing decision support systems that can help to prevent accidents from happening.
4. AI can be used to reduce accident rates by auto-coding workers' compensation claims and by providing VR systems that can enhance safety training."
Applications of artificial intelligence,Biochemistry,AlphaFold 2 can determine the 3D structure of a (folded) protein in hours rather than the months required by earlier automated approaches and was used to provide the likely structures of all proteins in the human body and essentially all proteins known to science (more than 200 million).,59,"Applications of artificial intelligence
Biochemistry

AlphaFold 2 can determine the 3D structure of a (folded) protein in hours rather than the months required by earlier automated approaches and was used to provide the likely structures of all proteins in the human body and essentially all proteins known to science (more than 200 million).","1. What is the 3D structure of a protein?
2. What is the difference between AlphaFold 2 and earlier automated approaches?
3. How many proteins does AlphaFold 2 know the structure of?","1. The 3D structure of a protein is the shape that it takes when it is folded.
2. AlphaFold 2 can determine the 3D structure of a protein much faster than earlier automated approaches.
3. AlphaFold 2 knows the structure of more than 200 million proteins."
Applications of artificial intelligence,Chemistry and biology,"Machine learning has been used for drug design. It has also been used for predicting molecular properties and exploring large chemical/reaction spaces. Computer-planned syntheses via computational reaction networks, described as a platform that combines ""computational synthesis with AI algorithms to predict molecular properties"", have been used to explore the origins of life on Earth, drug-syntheses and developing routes for recycling 200 industrial waste chemicals into important drugs and agrochemicals (chemical synthesis design). There is research about which types of computer-aided chemistry would benefit from machine learning. It can also be used for ""drug discovery and development, drug repurposing, improving pharmaceutical productivity, and clinical trials"". It has been used for the design of proteins with prespecified functional sites.It has been used with databases for the development of a 46-day process to design, synthesize and test a drug which inhibits enzymes of a particular gene, DDR1. DDR1 is involved in cancers and fibrosis which is one reason for the high-quality datasets that enabled these results.There are various types of applications for machine learning in decoding human biology, such as helping to map gene expression patterns to functional activation patterns or identifying functional DNA motifs. It is widely used in genetic research.There also is some use of machine learning in synthetic biology, disease biology, nanotechnology (e.g. nanostructured materials and bionanotechnology), and materials science.",295,"Applications of artificial intelligence
Chemistry and biology

Machine learning has been used for drug design. It has also been used for predicting molecular properties and exploring large chemical/reaction spaces. Computer-planned syntheses via computational reaction networks, described as a platform that combines ""computational synthesis with AI algorithms to predict molecular properties"", have been used to explore the origins of life on Earth, drug-syntheses and developing routes for recycling 200 industrial waste chemicals into important drugs and agrochemicals (chemical synthesis design). There is research about which types of computer-aided chemistry would benefit from machine learning. It can also be used for ""drug discovery and development, drug repurposing, improving pharmaceutical productivity, and clinical trials"". It has been used for the design of proteins with prespecified functional sites.It has been used with databases for the development of a 46-day process to design, synthesize and test a drug which inhibits enzymes of a particular gene, DDR1. DDR1 is involved in cancers and fibrosis which is one reason for the high-quality datasets that enabled these results.There are various types of applications for machine learning in decoding human biology, such as helping to map gene expression patterns to functional activation patterns or identifying functional DNA motifs. It is widely used in genetic research.There also is some use of machine learning in synthetic biology, disease biology, nanotechnology (e.g. nanostructured materials and bionanotechnology), and materials science.","1. What are some applications of machine learning in decoding human biology?
2. What is the use of machine learning in synthetic biology?
3. What is the use of machine learning in disease biology?
4. What is the use of machine learning in nanotechnology?
5. What is the use of machine learning in materials science?","1. Some applications of machine learning in decoding human biology include helping to map gene expression patterns to functional activation patterns or identifying functional DNA motifs.
2. Machine learning is used in synthetic biology to design proteins with prespecified functional sites.
3. Machine learning is used in disease biology to identify functional DNA motifs.
4. Machine learning is used in nanotechnology to create nanostructured materials and in bionanotechnology to study the behavior of biomolecules on the nanoscale.
5. Machine learning is used in materials science to study the behavior of materials at the atomic level."
Applications of artificial intelligence,Novel types of machine learning,"There are also prototype robot scientists, including robot-embodied ones like the two Robot Scientists, which show a form of ""machine learning"" not commonly associated with the term.Similarly, there is research and development of biological ""wetware computers"" that can learn (e.g. for use as biosensors) and/or implantation into an organism's body (e.g. for use to control prosthetics). Polymer-based artificial neurons operate directly in biological environments and define biohybrid neurons made of artificial and living components.Moreover, if whole brain emulation is possible via both scanning and replicating the, at least, bio-chemical brain – as premised in the form of digital replication in The Age of Em, possibly using physical neural networks – that may have applications as or more extensive than e.g. valued human activities and may imply that society would face substantial moral choices, societal risks and ethical problems such as whether (and how) such are built, sent through space and used compared to potentially competing e.g. potentially more synthetic and/or less human and/or non/less-sentient types of artificial/semi-artificial intelligence. An alternative or additive approach to scanning are types of reverse engineering of the brain.
A subcategory of artificial intelligence is embodied, some of which are mobile robotic systems that each consist of one or multiple robots that are able to learn in the physical world.",297,"Applications of artificial intelligence
Novel types of machine learning

There are also prototype robot scientists, including robot-embodied ones like the two Robot Scientists, which show a form of ""machine learning"" not commonly associated with the term.Similarly, there is research and development of biological ""wetware computers"" that can learn (e.g. for use as biosensors) and/or implantation into an organism's body (e.g. for use to control prosthetics). Polymer-based artificial neurons operate directly in biological environments and define biohybrid neurons made of artificial and living components.Moreover, if whole brain emulation is possible via both scanning and replicating the, at least, bio-chemical brain – as premised in the form of digital replication in The Age of Em, possibly using physical neural networks – that may have applications as or more extensive than e.g. valued human activities and may imply that society would face substantial moral choices, societal risks and ethical problems such as whether (and how) such are built, sent through space and used compared to potentially competing e.g. potentially more synthetic and/or less human and/or non/less-sentient types of artificial/semi-artificial intelligence. An alternative or additive approach to scanning are types of reverse engineering of the brain.
A subcategory of artificial intelligence is embodied, some of which are mobile robotic systems that each consist of one or multiple robots that are able to learn in the physical world.","1. What are some novel types of machine learning that are not commonly associated with the term?
2. What is the research and development of biological wetware computers?
3. What are the applications of artificial intelligence that are more extensive than valued human activities?","1. Novel types of machine learning that are not commonly associated with the term include robot-embodied machine learning and biohybrid neurons.
2. The research and development of biological wetware computers is the study of learning in biological systems, such as biosensors and prosthetics.
3. Applications of artificial intelligence that are more extensive than valued human activities include whole brain emulation and reverse engineering of the brain."
Applications of artificial intelligence,Biological computing in AI and as AI,"However, biological computers, even if both highly artificial and intelligent, are typically distinguished from synthetic, often silicon-based, computers – they could however be combined or used for the design of either. Moreover, many tasks may be carried out inadequately by artificial intelligence even if its algorithms were transparent, understood, bias-free, apparently effective, and goal-aligned and its trained data sufficiently large and cleansed – such as in cases were the underlying or available metrics, values or data are inappropriate. Computer-aided is a phrase used to describe human activities that make use of computing as tool in more comprehensive activities and systems such as AI for narrow tasks or making use of such without substantially relying on its results (see also: human-in-the-loop). A study described the biological as a limitation of AI with ""as long as the biological system cannot be understood, formalized, and imitated, we will not be able to develop technologies that can mimic it"" and that if it was understood this doesn't mean there being ""a technological solution to imitate natural intelligence"". Technologies that integrate biology and are often AI-based include biorobotics.",241,"Applications of artificial intelligence
Biological computing in AI and as AI

However, biological computers, even if both highly artificial and intelligent, are typically distinguished from synthetic, often silicon-based, computers – they could however be combined or used for the design of either. Moreover, many tasks may be carried out inadequately by artificial intelligence even if its algorithms were transparent, understood, bias-free, apparently effective, and goal-aligned and its trained data sufficiently large and cleansed – such as in cases were the underlying or available metrics, values or data are inappropriate. Computer-aided is a phrase used to describe human activities that make use of computing as tool in more comprehensive activities and systems such as AI for narrow tasks or making use of such without substantially relying on its results (see also: human-in-the-loop). A study described the biological as a limitation of AI with ""as long as the biological system cannot be understood, formalized, and imitated, we will not be able to develop technologies that can mimic it"" and that if it was understood this doesn't mean there being ""a technological solution to imitate natural intelligence"". Technologies that integrate biology and are often AI-based include biorobotics.","1. What is the main difference between artificial and biological computers?
2. What are some tasks that artificial intelligence may not be able to do effectively?
3. What is the relationship between computer-aided activities and artificial intelligence?
4. What is the main limitation of artificial intelligence according to the study?","1. The main difference between artificial and biological computers is that biological computers are typically distinguished from synthetic, often silicon-based, computers.
2. Some tasks that artificial intelligence may not be able to do effectively are tasks where the underlying or available metrics, values, or data are inappropriate.
3. Computer-aided activities are activities that make use of computing as a tool in more comprehensive activities and systems, such as AI for narrow tasks or making use of such without substantially relying on its results.
4. The main limitation of artificial intelligence according to the study is that the biological system cannot be understood."
Applications of artificial intelligence,"Astronomy, space activities and ufology","Artificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for ""classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights"" for example for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. It could also be used for activities in space such as space exploration, including analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation.In the search for extraterrestrial intelligence (SETI), machine learning has been used in attempts to identify artificially generated electromagnetic waves in available data – such as real-time observations – and other technosignatures, e.g. via anomaly detection. In ufology, the SkyCAM-5 project headed by Prof. Hakan Kayal and the Galileo Project headed by Prof. Avi Loeb use machine learning to detect and classify peculiar types of UFOs. The Galileo Project also seeks to detect two further types of potential extraterrestrial technological signatures with the use of AI: 'Oumuamua-like interstellar objects, and non-manmade artificial satellites.",246,"Applications of artificial intelligence
Astronomy, space activities and ufology

Artificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for ""classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights"" for example for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. It could also be used for activities in space such as space exploration, including analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation.In the search for extraterrestrial intelligence (SETI), machine learning has been used in attempts to identify artificially generated electromagnetic waves in available data – such as real-time observations – and other technosignatures, e.g. via anomaly detection. In ufology, the SkyCAM-5 project headed by Prof. Hakan Kayal and the Galileo Project headed by Prof. Avi Loeb use machine learning to detect and classify peculiar types of UFOs. The Galileo Project also seeks to detect two further types of potential extraterrestrial technological signatures with the use of AI: 'Oumuamua-like interstellar objects, and non-manmade artificial satellites.","1. What are some of the ways in which artificial intelligence is used in astronomy?
2. How could artificial intelligence be used in space exploration?
3. What are some of the ways in which artificial intelligence is used in the search for extraterrestrial intelligence?
4. How could artificial intelligence be used to detect potential extraterrestrial signatures?","1. Artificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for ""classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights"".
2. Artificial intelligence could be used in space exploration for activities such as space debris avoidance, and more autonomous operation.
3. Artificial intelligence is used in the search for extraterrestrial intelligence to identify artificially generated electromagnetic waves in available data, and to detect potential extraterrestrial signatures.
4. Artificial intelligence could be used to detect potential extraterrestrial signatures by anomaly detection."
Applications of artificial intelligence,Future or non-human applications,"Loeb has speculated that one type of technological equipment the project may detect could be ""AI astronauts"" and in 2021 – in an opinion piece – that AI ""will"" ""supersede natural intelligence"", while Martin Rees stated that there ""may"" be more civilizations than thought with the ""majority of them"" being artificial. In particular, mid/far future or non-human applications of artificial intelligence could include advanced forms of artificial general intelligence that engages in space colonization or more narrow spaceflight-specific types of AI. In contrast, there have been concerns in relation to potential AGI or AI capable of embryo space colonization, or more generally natural intelligence-based space colonization, such as ""safety of encounters with an alien AI"", suffering risks (or inverse goals), moral license/responsibility in respect to colonization-effects, or AI gone rogue (e.g. as portrayed with fictional David8 and HAL 9000). See also: space law and space ethics. Loeb has described the possibility of ""AI astronauts"" that engage in ""supervised evolution"" (see also: directed evolution, uplift, directed panspermia and space colonization).",238,"Applications of artificial intelligence
Future or non-human applications

Loeb has speculated that one type of technological equipment the project may detect could be ""AI astronauts"" and in 2021 – in an opinion piece – that AI ""will"" ""supersede natural intelligence"", while Martin Rees stated that there ""may"" be more civilizations than thought with the ""majority of them"" being artificial. In particular, mid/far future or non-human applications of artificial intelligence could include advanced forms of artificial general intelligence that engages in space colonization or more narrow spaceflight-specific types of AI. In contrast, there have been concerns in relation to potential AGI or AI capable of embryo space colonization, or more generally natural intelligence-based space colonization, such as ""safety of encounters with an alien AI"", suffering risks (or inverse goals), moral license/responsibility in respect to colonization-effects, or AI gone rogue (e.g. as portrayed with fictional David8 and HAL 9000). See also: space law and space ethics. Loeb has described the possibility of ""AI astronauts"" that engage in ""supervised evolution"" (see also: directed evolution, uplift, directed panspermia and space colonization).","1. What is Loeb's speculation about the technological equipment the project may detect?
2. What is Martin Rees' opinion about AI?
3. What are the concerns in relation to potential AGI or AI capable of embryo space colonization?
4. What is Loeb's description of the possibility of ""AI astronauts""?","1. Loeb's speculation about the technological equipment the project may detect is that they may be ""AI astronauts"".
2. Martin Rees' opinion about AI is that there ""may"" be more civilizations than thought with the ""majority of them"" being artificial.
3. The concerns in relation to potential AGI or AI capable of embryo space colonization are that they may be unsafe, may have risks (or inverse goals), and may have moral license/responsibility in respect to colonization-effects.
4. Loeb's description of the possibility of ""AI astronauts"" is that they may engage in ""supervised evolution""."
Applications of artificial intelligence,Astrochemistry,"It can also be used to produce datasets of spectral signatures of molecules that may be involved in the atmospheric production or consumption of particular chemicals – such as phosphine possibly detected on Venus – which could prevent miss assignments and, if accuracy is improved, be used in future detections and identifications of molecules on other planets.",66,"Applications of artificial intelligence
Astrochemistry

It can also be used to produce datasets of spectral signatures of molecules that may be involved in the atmospheric production or consumption of particular chemicals – such as phosphine possibly detected on Venus – which could prevent miss assignments and, if accuracy is improved, be used in future detections and identifications of molecules on other planets.","1. What is the main use of artificial intelligence in astrochemistry?
2. How could artificial intelligence be used to improve molecule detection on other planets?","1. The main use of artificial intelligence in astrochemistry is to produce datasets of spectral signatures of molecules.
2. Artificial intelligence could be used to improve molecule detection on other planets by preventing miss assignments and improving accuracy."
Applications of artificial intelligence,"Archaeology, history and imaging of sites","Machine learning can help to restore and attribute ancient texts. It can help to index texts for example to enable better and easier searching and classification of fragments.
Artificial intelligence can also be used to investigate genomes to uncover genetic history, such as interbreeding between archaic and modern humans by which for example the past existence of a ghost population, not Neanderthal or Denisovan, was inferred. 
It can also be used for ""non-invasive and non-destructive access to internal structures of archaeological remains"".",110,"Applications of artificial intelligence
Archaeology, history and imaging of sites

Machine learning can help to restore and attribute ancient texts. It can help to index texts for example to enable better and easier searching and classification of fragments.
Artificial intelligence can also be used to investigate genomes to uncover genetic history, such as interbreeding between archaic and modern humans by which for example the past existence of a ghost population, not Neanderthal or Denisovan, was inferred. 
It can also be used for ""non-invasive and non-destructive access to internal structures of archaeological remains"".","1. How can machine learning help to restore and attribute ancient texts?
2. How can artificial intelligence be used to investigate genomes?
3. How can artificial intelligence be used for ""non-invasive and non-destructive access to internal structures of archaeological remains""?","1. Machine learning can help to restore and attribute ancient texts by indexing them for easier searching and classification of fragments.
2. Artificial intelligence can be used to investigate genomes to uncover genetic history, such as interbreeding between archaic and modern humans.
3. Artificial intelligence can be used for ""non-invasive and non-destructive access to internal structures of archaeological remains"" by using imaging to examine them without damaging them."
Applications of artificial intelligence,Physics,"A deep learning system was reported to learn intuitive physics from visual data (of virtual 3D environments) based on an unpublished approach inspired by studies of visual cognition in infants. Other researchers have developed a machine learning algorithm that could discover sets of basic variables of various physical systems and predict the systems' future dynamics from video recordings of their behavior. In the future, it may be possible that such can be used to automate the discovery of physical laws of complex systems.",94,"Applications of artificial intelligence
Physics

A deep learning system was reported to learn intuitive physics from visual data (of virtual 3D environments) based on an unpublished approach inspired by studies of visual cognition in infants. Other researchers have developed a machine learning algorithm that could discover sets of basic variables of various physical systems and predict the systems' future dynamics from video recordings of their behavior. In the future, it may be possible that such can be used to automate the discovery of physical laws of complex systems.","1. What is the deep learning system that was reported to learn intuitive physics?
2. How was the deep learning system inspired by studies of visual cognition in infants?
3. What is the machine learning algorithm that could discover sets of basic variables of various physical systems?
4. How could the machine learning algorithm be used to automate the discovery of physical laws of complex systems?","1. The deep learning system that was reported to learn intuitive physics is a machine learning algorithm.
2. The deep learning system was inspired by studies of visual cognition in infants because the machine learning algorithm was designed to discover sets of basic variables of various physical systems.
3. The machine learning algorithm could be used to automate the discovery of physical laws of complex systems because it is designed to predict the systems' future dynamics from video recordings of their behavior."
Applications of artificial intelligence,Materials science,AI could be used for materials optimization and discovery such as the discovery of stable materials and the prediction of their crystal structure.,27,"Applications of artificial intelligence
Materials science

AI could be used for materials optimization and discovery such as the discovery of stable materials and the prediction of their crystal structure.","1. What is the potential use of AI in materials science? 
2. What are some potential benefits of using AI in materials science? 
3. What are some potential applications of AI in materials science?","1. The potential use of AI in materials science is the discovery of stable materials and the prediction of their crystal structure. 
2. Some potential benefits of using AI in materials science are the discovery of new materials with specific properties, the optimization of material composition, and the prediction of material stability. 
3. Potential applications of AI in materials science include the discovery of new materials, the optimization of material composition, and the prediction of material stability."
Applications of artificial intelligence,Reverse engineering,"Machine learning is used in diverse types of reverse engineering. For example, machine learning has been used to reverse engineer a composite material part, enabling unauthorized production of high quality parts, and for quickly understanding the behavior of malware. It can be used to reverse engineer artificial intelligence models. It can also design components by engaging in a type of reverse engineering of not-yet existent virtual components such as inverse molecular design for particular desired functionality or protein design for prespecified functional sites. Biological network reverse engineering could model interactions in a human understandable way, e.g. bas on time series data of gene expression levels.",125,"Applications of artificial intelligence
Reverse engineering

Machine learning is used in diverse types of reverse engineering. For example, machine learning has been used to reverse engineer a composite material part, enabling unauthorized production of high quality parts, and for quickly understanding the behavior of malware. It can be used to reverse engineer artificial intelligence models. It can also design components by engaging in a type of reverse engineering of not-yet existent virtual components such as inverse molecular design for particular desired functionality or protein design for prespecified functional sites. Biological network reverse engineering could model interactions in a human understandable way, e.g. bas on time series data of gene expression levels.","1. What is reverse engineering?
2. What are some applications of machine learning in reverse engineering?
3. What is the potential of machine learning in reverse engineering?","1. Reverse engineering is the process of extracting knowledge or design from something that is not explicitly designed to be understood.
2. Some applications of machine learning in reverse engineering are reverse engineering a composite material part, understanding the behavior of malware, reverse engineering artificial intelligence models, and designing components.
3. The potential of machine learning in reverse engineering is vast. It can be used to understand the behavior of malware, reverse engineer artificial intelligence models, and design components."
Applications of artificial intelligence,Legal analysis,"AI is a mainstay of law-related professions. Algorithms and machine learning do some tasks previously done by entry-level lawyers. While its use is common, it is not expected to replace most work done by lawyers in the near future.The electronic discovery industry uses machine learning to reduce manual searching.",65,"Applications of artificial intelligence
Legal analysis

AI is a mainstay of law-related professions. Algorithms and machine learning do some tasks previously done by entry-level lawyers. While its use is common, it is not expected to replace most work done by lawyers in the near future.The electronic discovery industry uses machine learning to reduce manual searching.","1. What are some tasks that are commonly done by AI in law-related professions? 
2. How does electronic discovery use machine learning? 
3. Is AI expected to replace most work done by lawyers in the near future?","1. Some tasks that are commonly done by AI in law-related professions include legal research, contract review, and due diligence. 
2. Electronic discovery uses machine learning to reduce manual searching. It does this by automatically identifying relevant documents from a large data set using pre-determined criteria. 
3. AI is not expected to replace most work done by lawyers in the near future. However, it is increasingly being used to automate certain tasks that were previously done by entry-level lawyers."
Applications of artificial intelligence,Law enforcement and legal proceedings,"COMPAS is a commercial system used by U.S. courts to assess the likelihood of recidivism.One concern relates to algorithmic bias, AI programs may become biased after processing data that exhibits bias. ProPublica claims that the average COMPAS-assigned recidivism risk level of black defendants is significantly higher than that of white defendants.",78,"Applications of artificial intelligence
Law enforcement and legal proceedings

COMPAS is a commercial system used by U.S. courts to assess the likelihood of recidivism.One concern relates to algorithmic bias, AI programs may become biased after processing data that exhibits bias. ProPublica claims that the average COMPAS-assigned recidivism risk level of black defendants is significantly higher than that of white defendants.","1. What is COMPAS?
2. What is the likelihood of recidivism?
3. What is the concern relating to algorithmic bias?
4. What is ProPublica's claim?
5. How might COMPAS become biased?","1. COMPAS is a commercial system used by U.S. courts to assess the likelihood of recidivism.
2. The likelihood of recidivism is the probability that a criminal will commit another crime after being released from prison.
3. The concern relating to algorithmic bias is that AI programs may become biased after processing data that exhibits bias.
4. ProPublica claims that the average COMPAS-assigned recidivism risk level of black defendants is significantly higher than that of white defendants.
5. One way that COMPAS might become biased is if the data used to train the AI program is itself biased."
Applications of artificial intelligence,Human resources,"Another application of AI is in human resources. AI can screen resumes and rank candidates based on their qualifications, predict candidate success in given roles, and automate repetitive communication tasks via chatbots.",40,"Applications of artificial intelligence
Human resources

Another application of AI is in human resources. AI can screen resumes and rank candidates based on their qualifications, predict candidate success in given roles, and automate repetitive communication tasks via chatbots.","1. What are some ways AI is used in human resources?
2. How can AI screen resumes and rank candidates?
3. How can AI predict candidate success in given roles?
4. How can AI automate repetitive communication tasks via chatbots?","1. AI is used in human resources to screen resumes and rank candidates, predict candidate success in given roles, and automate repetitive communication tasks via chatbots.
2. AI can screen resumes and rank candidates by their qualifications, predict candidate success in given roles, and automate repetitive communication tasks via chatbots.
3. AI can predict candidate success in given roles by analyzing data such as resumes, job postings, and past employee success.
4. AI can automate repetitive communication tasks via chatbots by automating tasks such as scheduling interviews, sending reminders, and following up."
Applications of artificial intelligence,Job search,"AI has simplified the recruiting /job search process for both recruiters and job seekers. According to Raj Mukherjee from Indeed, 65% of job searchers search again within 91 days after hire. An AI-powered engine streamlines the complexity of job hunting by assessing information on job skills, salaries, and user tendencies, matching job seekers to the most relevant positions. Machine intelligence calculates appropriate wages and highlights resume information for recruiters using NLP, which extracts relevant words and phrases from text. Another application is an AI resume builder that compiles a CV in 5 minutes. Chatbots assist website visitors and refine workflows.",128,"Applications of artificial intelligence
Job search

AI has simplified the recruiting /job search process for both recruiters and job seekers. According to Raj Mukherjee from Indeed, 65% of job searchers search again within 91 days after hire. An AI-powered engine streamlines the complexity of job hunting by assessing information on job skills, salaries, and user tendencies, matching job seekers to the most relevant positions. Machine intelligence calculates appropriate wages and highlights resume information for recruiters using NLP, which extracts relevant words and phrases from text. Another application is an AI resume builder that compiles a CV in 5 minutes. Chatbots assist website visitors and refine workflows.","1. What are some of the benefits that AI brings to the job search process?
2. How does AI help to match job seekers with the most relevant positions?
3. How does AI help to highlight resume information for recruiters?
4. What are some of the other applications of AI in the job search process?","1. Some of the benefits that AI brings to the job search process include streamlining the complexity of job hunting, assessing information on job skills, salaries, and user tendencies, and matching job seekers to the most relevant positions.
2. AI helps to match job seekers with the most relevant positions by calculating appropriate wages and highlighting resume information for recruiters using NLP, which extracts relevant words and phrases from text.
3. AI helps to highlight resume information for recruiters by extracting relevant words and phrases from text.
4. Other applications of AI in the job search process include an AI resume builder that compiles a CV in 5 minutes and chatbots that assist website visitors and refine workflows."
Applications of artificial intelligence,Online and telephone customer service,"AI underlies avatars (automated online assistants) on web pages. It can reduce operation and training costs. Pypestream automated customer service for its mobile application to streamline communication with customers.A Google app analyzes language and converts speech into text. The platform can identify angry customers through their language and respond appropriately. Amazon uses a chatbot for customer service that can perform tasks like checking the status of an order, cancelling orders, offering refunds and connecting the customer with a human representative.",108,"Applications of artificial intelligence
Online and telephone customer service

AI underlies avatars (automated online assistants) on web pages. It can reduce operation and training costs. Pypestream automated customer service for its mobile application to streamline communication with customers.A Google app analyzes language and converts speech into text. The platform can identify angry customers through their language and respond appropriately. Amazon uses a chatbot for customer service that can perform tasks like checking the status of an order, cancelling orders, offering refunds and connecting the customer with a human representative.","1. What are some advantages of using AI for online customer service?
2. How does Amazon use AI for customer service?
3. How does Google use AI for customer service?","1. Some advantages of using AI for online customer service are that it can reduce operation and training costs, and can also identify angry customers through their language and respond appropriately.
2. Amazon uses a chatbot for customer service that can perform tasks like checking the status of an order, cancelling orders, offering refunds and connecting the customer with a human representative.
3. Google uses AI for customer service through its Google app, which analyzes language and converts speech into text."
Applications of artificial intelligence,Hospitality,"In the hospitality industry, AI is used to reduce repetitive tasks, analyze trends, interact with guests, and predict customer needs. AI hotel services come in the form of a chatbot, application, virtual voice assistant and service robots.",49,"Applications of artificial intelligence
Hospitality

In the hospitality industry, AI is used to reduce repetitive tasks, analyze trends, interact with guests, and predict customer needs. AI hotel services come in the form of a chatbot, application, virtual voice assistant and service robots.","1. What are some of the ways that AI is used in the hospitality industry?
2. How do AI hotel services work?
3. What are some of the benefits of using AI in the hospitality industry?","1. AI is used in the hospitality industry to reduce repetitive tasks, analyze trends, interact with guests, and predict customer needs.
2. AI hotel services work by providing a chatbot, application, virtual voice assistant, or service robot to guests.
3. The benefits of using AI in the hospitality industry include reducing repetitive tasks, analyzing trends, interacting with guests, and predicting customer needs."
Applications of artificial intelligence,Media,"AI applications analyze media content such as movies, TV programs, advertisement videos or user-generated content. The solutions often involve computer vision.
Typical scenarios include the analysis of images using object recognition or face recognition techniques, or the analysis of video for scene recognizing scenes, objects or faces. AI-based media analysis can facilitate media search, the creation of descriptive keywords for content, content policy monitoring (such as verifying the suitability of content for a particular TV viewing time), speech to text for archival or other purposes, and the detection of logos, products or celebrity faces for ad placement.

Motion interpolation
Pixel-art scaling algorithms
Image scaling
Image restoration
Photo colorization
Film restoration and video upscaling
Photo tagging
Automated species identification (such as identifying plants, fungi and animals with an app)
Text-to-image models such as DALL-E, Midjourney and Stable Diffusion
Image to video
Text to video such as Make-A-Video from Meta, Imagen video and Phenaki from Google
Text to music with AI models such as MusicLM
Text to speech such as ElevenLabs and 15.ai
Motion capture",247,"Applications of artificial intelligence
Media

AI applications analyze media content such as movies, TV programs, advertisement videos or user-generated content. The solutions often involve computer vision.
Typical scenarios include the analysis of images using object recognition or face recognition techniques, or the analysis of video for scene recognizing scenes, objects or faces. AI-based media analysis can facilitate media search, the creation of descriptive keywords for content, content policy monitoring (such as verifying the suitability of content for a particular TV viewing time), speech to text for archival or other purposes, and the detection of logos, products or celebrity faces for ad placement.

Motion interpolation
Pixel-art scaling algorithms
Image scaling
Image restoration
Photo colorization
Film restoration and video upscaling
Photo tagging
Automated species identification (such as identifying plants, fungi and animals with an app)
Text-to-image models such as DALL-E, Midjourney and Stable Diffusion
Image to video
Text to video such as Make-A-Video from Meta, Imagen video and Phenaki from Google
Text to music with AI models such as MusicLM
Text to speech such as ElevenLabs and 15.ai
Motion capture","1. What are some of the most common applications of artificial intelligence when it comes to media?
2. How can AI be used to improve media search?
3. What are some of the benefits of using AI for content policy monitoring?
4. How can AI be used to create descriptive keywords for content?
5. What are some of the benefits of using AI for speech to text transcription?
6. How can AI be used to detect logos, products or celebrity faces in videos?
","1. Some of the most common applications of artificial intelligence when it comes to media include image recognition, video analysis, and speech to text transcription.
2. AI can be used to improve media search by analyzing images and videos for specific objects or scenes.
3. The benefits of using AI for content policy monitoring include the ability to verify the suitability of content for a particular viewing time and the creation of descriptive keywords for content.
4. AI can be used to create descriptive keywords for content by analyzing the content for specific objects, scenes, or faces.
5. The benefits of using AI for speech to text transcription include the ability to transcribe audio files into text files for archival or other purposes.
6. AI can be used to detect logos, products, or celebrity faces in videos by analyzing the images for specific objects or features."
Applications of artificial intelligence,Deep-fakes,"Deep-fakes can be used for comedic purposes but are better known for fake news and hoaxes.
In January 2016, the Horizon 2020 program financed the InVID Project to help journalists and researchers detect fake documents, made available as browser plugins.In June 2016, the visual computing group of the Technical University of Munich and from Stanford University developed Face2Face, a program that animates photographs of faces, mimicking the facial expressions of another person. The technology has been demonstrated animating the faces of people including Barack Obama and Vladimir Putin. Other methods have been demonstrated based on deep neural networks, from which the name deep fake was taken.
In September 2018, U.S. Senator Mark Warner proposed to penalize social media companies that allow sharing of deep-fake documents on their platforms.In 2018, Vincent Nozick found a way to detect faked content by analyzing eyelid movements. DARPA gave 68 million dollars to work on deep-fake detection.Audio deepfakes and AI software capable of detecting deep-fakes and cloning human voices have been developed.",219,"Applications of artificial intelligence
Deep-fakes

Deep-fakes can be used for comedic purposes but are better known for fake news and hoaxes.
In January 2016, the Horizon 2020 program financed the InVID Project to help journalists and researchers detect fake documents, made available as browser plugins.In June 2016, the visual computing group of the Technical University of Munich and from Stanford University developed Face2Face, a program that animates photographs of faces, mimicking the facial expressions of another person. The technology has been demonstrated animating the faces of people including Barack Obama and Vladimir Putin. Other methods have been demonstrated based on deep neural networks, from which the name deep fake was taken.
In September 2018, U.S. Senator Mark Warner proposed to penalize social media companies that allow sharing of deep-fake documents on their platforms.In 2018, Vincent Nozick found a way to detect faked content by analyzing eyelid movements. DARPA gave 68 million dollars to work on deep-fake detection.Audio deepfakes and AI software capable of detecting deep-fakes and cloning human voices have been developed.","1. What is a deep-fake?
2. What are some of the potential applications of deep-fakes?
3. How is deep-fake detection being developed?","1. A deep-fake is a digital media file or video that has been manipulated using artificial intelligence to make it appear as if it is a real video of a person.
2. Some potential applications of deep-fakes include creating fake news, hoaxes, and videos for comedic purposes.
3. Deep-fake detection is being developed through various methods, including the use of facial recognition software and eyelid movement analysis."
Applications of artificial intelligence,"Video content analysis, surveillance and manipulated media detection",AI algorithms have been used to detect deepfake videos.,21,"Applications of artificial intelligence
Video content analysis, surveillance and manipulated media detection

AI algorithms have been used to detect deepfake videos.","1. What is a deepfake video?
2. How are AI algorithms used to detect deepfake videos?","1. A deepfake video is a video that has been manipulated using AI algorithms to make it look like someone else is in the video.
2. AI algorithms are used to detect deepfake videos by analyzing the video content to see if it has been manipulated."
Applications of artificial intelligence,Music,"AI has been used to compose music of various genres.
David Cope created an AI called Emily Howell that managed to become well known in the field of algorithmic computer music. The algorithm behind Emily Howell is registered as a US patent.In 2012, AI Iamus created the first complete classical album.AIVA (Artificial Intelligence Virtual Artist), composes symphonic music, mainly classical music for film scores. It achieved a world first by becoming the first virtual composer to be recognized by a musical professional association.Melomics creates computer-generated music for stress and pain relief.At Sony CSL Research Laboratory, the Flow Machines software creates pop songs by learning music styles from a huge database of songs. It can compose in multiple styles.
The Watson Beat uses reinforcement learning and deep belief networks to compose music on a simple seed input melody and a select style. The software was open sourced and musicians such as Taryn Southern collaborated with the project to create music.
South Korean singer Hayeon's debut song, ""Eyes on You"" was composed using AI which was supervised by real composers, including NUVO.",231,"Applications of artificial intelligence
Music

AI has been used to compose music of various genres.
David Cope created an AI called Emily Howell that managed to become well known in the field of algorithmic computer music. The algorithm behind Emily Howell is registered as a US patent.In 2012, AI Iamus created the first complete classical album.AIVA (Artificial Intelligence Virtual Artist), composes symphonic music, mainly classical music for film scores. It achieved a world first by becoming the first virtual composer to be recognized by a musical professional association.Melomics creates computer-generated music for stress and pain relief.At Sony CSL Research Laboratory, the Flow Machines software creates pop songs by learning music styles from a huge database of songs. It can compose in multiple styles.
The Watson Beat uses reinforcement learning and deep belief networks to compose music on a simple seed input melody and a select style. The software was open sourced and musicians such as Taryn Southern collaborated with the project to create music.
South Korean singer Hayeon's debut song, ""Eyes on You"" was composed using AI which was supervised by real composers, including NUVO.","1. What are some of the ways in which AI has been used to compose music?
2. How well-known is AI composer Emily Howell?
3. What was the first complete classical album created by AI?
4. What kind of music does Melomics create?
5. How was Hayeon's debut song composed?","1. AI has been used to compose music in various genres, including classical, pop, and film scores.
2. AI composer Emily Howell is well-known in the field of algorithmic computer music.
3. The first complete classical album created by AI was in 2012 by AI Iamus.
4. Melomics creates computer-generated music for stress and pain relief.
5. Hayeon's debut song was composed using AI which was supervised by real composers."
Applications of artificial intelligence,Writing and reporting,"Narrative Science sells computer-generated news and reports. It summarizes sporting events based on statistical data from the game. It also creates financial reports and real estate analyses. Automated Insights generates personalized recaps and previews for Yahoo Sports Fantasy Football.Yseop, uses AI to turn structured data into natural language comments and recommendations. Yseop writes financial reports, executive summaries, personalized sales or marketing documents and more in multiple languages, including English, Spanish, French, and German.TALESPIN made up stories similar to the fables of Aesop. The program started with a set of characters who wanted to achieve certain goals. The story narrated their attempts to satisfy these goals. Mark Riedl and Vadim Bulitko asserted that the essence of storytelling was experience management, or ""how to balance the need for a coherent story progression with user agency, which is often at odds"".While AI storytelling focuses on story generation (character and plot), story communication also received attention. In 2002, researchers developed an architectural framework for narrative prose generation. They faithfully reproduced text variety and complexity on stories such as Little Red Riding Hood. In 2016, a Japanese AI co-wrote a short story and almost won a literary prize.South Korean company Hanteo Global uses a journalism bot to write articles.Literary authors are also exploring uses of AI. An example is David Jhave Johnston's work ReRites (2017-2019), where the poet created a daily rite of editing the poetic output of a neural network to create a series of performances and publications.",320,"Applications of artificial intelligence
Writing and reporting

Narrative Science sells computer-generated news and reports. It summarizes sporting events based on statistical data from the game. It also creates financial reports and real estate analyses. Automated Insights generates personalized recaps and previews for Yahoo Sports Fantasy Football.Yseop, uses AI to turn structured data into natural language comments and recommendations. Yseop writes financial reports, executive summaries, personalized sales or marketing documents and more in multiple languages, including English, Spanish, French, and German.TALESPIN made up stories similar to the fables of Aesop. The program started with a set of characters who wanted to achieve certain goals. The story narrated their attempts to satisfy these goals. Mark Riedl and Vadim Bulitko asserted that the essence of storytelling was experience management, or ""how to balance the need for a coherent story progression with user agency, which is often at odds"".While AI storytelling focuses on story generation (character and plot), story communication also received attention. In 2002, researchers developed an architectural framework for narrative prose generation. They faithfully reproduced text variety and complexity on stories such as Little Red Riding Hood. In 2016, a Japanese AI co-wrote a short story and almost won a literary prize.South Korean company Hanteo Global uses a journalism bot to write articles.Literary authors are also exploring uses of AI. An example is David Jhave Johnston's work ReRites (2017-2019), where the poet created a daily rite of editing the poetic output of a neural network to create a series of performances and publications.","1. What is Narrative Science?
2. What does Automated Insights do?
3. What is Yseop?
4. What is TALESPIN?
5. What is Hanteo Global?
6. What is David Jhave Johnston's work ReRites?","1. Narrative Science is a company that sells computer-generated news and reports.
2. Automated Insights generates personalized recaps and previews for Yahoo Sports Fantasy Football.
3. Yseop uses AI to turn structured data into natural language comments and recommendations.
4. TALESPIN is a program that made up stories similar to the fables of Aesop.
5. Hanteo Global is a South Korean company that uses a journalism bot to write articles.
6. David Jhave Johnston is a poet who created a daily rite of editing the poetic output of a neural network to create a series of performances and publications."
Applications of artificial intelligence,Wikipedia,"Millions of its articles have been edited by bots which however are usually not artificial intelligence software. Many AI platforms use Wikipedia data, mainly for training machine learning applications. There is research and development of various artificial intelligence applications for Wikipedia such as for identifying outdated sentences, detecting covert vandalism or recommending articles and tasks to new editors.
Machine translation (see above) has also be used for translating Wikipedia articles and could play a larger role in creating, updating, expanding, and generally improving articles in the future. A content translation tool allows editors of some Wikipedias to more easily translate articles across several select languages.",124,"Applications of artificial intelligence
Wikipedia

Millions of its articles have been edited by bots which however are usually not artificial intelligence software. Many AI platforms use Wikipedia data, mainly for training machine learning applications. There is research and development of various artificial intelligence applications for Wikipedia such as for identifying outdated sentences, detecting covert vandalism or recommending articles and tasks to new editors.
Machine translation (see above) has also be used for translating Wikipedia articles and could play a larger role in creating, updating, expanding, and generally improving articles in the future. A content translation tool allows editors of some Wikipedias to more easily translate articles across several select languages.","1. What are some artificial intelligence applications used for Wikipedia? 
2. How is machine translation used for Wikipedia? 
3. What is the content translation tool and what does it do?","1. Some artificial intelligence applications used for Wikipedia are identifying outdated sentences, detecting covert vandalism, and recommending articles and tasks to new editors. 
2. Machine translation is used for translating Wikipedia articles into other languages. 
3. The content translation tool allows editors of some Wikipedias to translate articles more easily between select languages."
Applications of artificial intelligence,Video games,"In video games, AI is routinely used to generate behavior in non-player characters (NPCs). In addition, AI is used for pathfinding. Some researchers consider NPC AI in games to be a ""solved problem"" for most production tasks. Games with less typical AI include the AI director of Left 4 Dead (2008) and the neuroevolutionary training of platoons in Supreme Commander 2 (2010). AI is also used in Alien Isolation (2014) as a way to control the actions the Alien will perform next.Kinect, which provides a 3D body–motion interface for the Xbox 360 and the Xbox One, uses algorithms that emerged from AI research.",141,"Applications of artificial intelligence
Video games

In video games, AI is routinely used to generate behavior in non-player characters (NPCs). In addition, AI is used for pathfinding. Some researchers consider NPC AI in games to be a ""solved problem"" for most production tasks. Games with less typical AI include the AI director of Left 4 Dead (2008) and the neuroevolutionary training of platoons in Supreme Commander 2 (2010). AI is also used in Alien Isolation (2014) as a way to control the actions the Alien will perform next.Kinect, which provides a 3D body–motion interface for the Xbox 360 and the Xbox One, uses algorithms that emerged from AI research.","1. What are some of the ways in which AI is used in video games?
2. How do AI techniques help in the development of video games?
3. What are some of the more challenging aspects of AI in video games?","1. AI is used in video games to generate behavior in non-player characters, to find paths for characters to follow, and to control the actions of the Alien in Alien Isolation.
2. AI techniques help in the development of video games by providing ways to generate behavior in NPCs, to find paths for characters to follow, and to control the actions of the Alien in Alien Isolation.
3. The more challenging aspects of AI in video games are generating behavior in NPCs that is realistic and interesting, finding paths for characters to follow that are realistic and efficient, and controlling the actions of the Alien in Alien Isolation in a way that is realistic and suspenseful."
Applications of artificial intelligence,Art,"AI has been used to produce visual art. The first AI art program, called AARON, was developed by Harold Cohen in 1968 with the goal of being able to code the act of drawing.
It started by creating simple black and white drawings, and later to paint using special brushes and dyes that were chosen by the program itself without mediation from Cohen.

AI like ""Disco Diffusion"", ""DALL·E"" (1 and 2), Stable Diffusion, Imagen, ""Dream by Wombo"", Midjourney has been used for visualizing conceptual inputs such as song lyrics, certain texts or specific imagined concepts (or imaginations) in artistic ways or artistic images in general. Some of the tools also allow users to input images and various parameters e.g. to display an object or product in various environments, some can replicate artistic styles of popular artists, and some can create elaborate artistic images from rough sketches.
Since their design in 2014, generative adversarial networks (GANs) have been used by AI artists. GAN computer programming, generates technical images through machine learning frameworks that surpass the need for human operators. Examples of GAN programs that generate art include Artbreeder and DeepDream.",253,"Applications of artificial intelligence
Art

AI has been used to produce visual art. The first AI art program, called AARON, was developed by Harold Cohen in 1968 with the goal of being able to code the act of drawing.
It started by creating simple black and white drawings, and later to paint using special brushes and dyes that were chosen by the program itself without mediation from Cohen.

AI like ""Disco Diffusion"", ""DALL·E"" (1 and 2), Stable Diffusion, Imagen, ""Dream by Wombo"", Midjourney has been used for visualizing conceptual inputs such as song lyrics, certain texts or specific imagined concepts (or imaginations) in artistic ways or artistic images in general. Some of the tools also allow users to input images and various parameters e.g. to display an object or product in various environments, some can replicate artistic styles of popular artists, and some can create elaborate artistic images from rough sketches.
Since their design in 2014, generative adversarial networks (GANs) have been used by AI artists. GAN computer programming, generates technical images through machine learning frameworks that surpass the need for human operators. Examples of GAN programs that generate art include Artbreeder and DeepDream.","1. What is the first AI art program?
2. What did Harold Cohen use AI for in 1968?
3. What is a GAN computer program?","1. The first AI art program was called AARON and was developed by Harold Cohen in 1968.
2. Harold Cohen used AI for the purpose of coding the act of drawing in 1968.
3. GAN computer programs are programs that generate technical images through machine learning frameworks that surpass the need for human operators."
Applications of artificial intelligence,Art analysis,"In addition to the creation of original art, research methods that utilize AI have been generated to quantitatively analyze digital art collections. Although the main goal of the large-scale digitization of artwork in the past few decades was to allow for accessibility and exploration of these collections, the use of AI in analyzing them has brought about new research perspectives.
Two computational methods, close reading and distant viewing, are the typical approaches used to analyze digitized art. While distant viewing includes the analysis of large collections, close reading involves one piece of artwork.
Researchers have also introduced models that predict emotional responses to art.",124,"Applications of artificial intelligence
Art analysis

In addition to the creation of original art, research methods that utilize AI have been generated to quantitatively analyze digital art collections. Although the main goal of the large-scale digitization of artwork in the past few decades was to allow for accessibility and exploration of these collections, the use of AI in analyzing them has brought about new research perspectives.
Two computational methods, close reading and distant viewing, are the typical approaches used to analyze digitized art. While distant viewing includes the analysis of large collections, close reading involves one piece of artwork.
Researchers have also introduced models that predict emotional responses to art.","1. What are the two methods used to analyze digitized art?
2. What is the goal of the large-scale digitization of artwork?
3. What is the use of AI in analyzing them?
4. What are the benefits of using AI in art analysis?","1. The two methods used to analyze digitized art are close reading and distant viewing.
2. The goal of the large-scale digitization of artwork is to allow for accessibility and exploration of these collections.
3. The use of AI in analyzing them has brought about new research perspectives.
4. The benefits of using AI in art analysis are that it allows for the quantitative analysis of large collections and the prediction of emotional responses to art."
Applications of artificial intelligence,Energy system,"Power electronics converters are used in renewable energy, energy storage, electric vehicles and high-voltage direct current transmission. These converters are failure-prone, which can interrupt service and require costly maintenance or catastrophic consequences in mission critical applications. AI can guide the design process for reliable power electronics converters, by calculating exact design parameters that ensure the required lifetime.Machine learning can be used for energy consumption prediction and scheduling, e.g. to help with renewable energy intermittency management (see also: smart grid and climate change mitigation in the power grid).",114,"Applications of artificial intelligence
Energy system

Power electronics converters are used in renewable energy, energy storage, electric vehicles and high-voltage direct current transmission. These converters are failure-prone, which can interrupt service and require costly maintenance or catastrophic consequences in mission critical applications. AI can guide the design process for reliable power electronics converters, by calculating exact design parameters that ensure the required lifetime.Machine learning can be used for energy consumption prediction and scheduling, e.g. to help with renewable energy intermittency management (see also: smart grid and climate change mitigation in the power grid).","1. What are the benefits of using artificial intelligence in power electronics converters?
2. How can machine learning be used for energy consumption prediction and scheduling?","1. The benefits of using artificial intelligence in power electronics converters include improved reliability and reduced maintenance costs.
2. Machine learning can be used for energy consumption prediction and scheduling in order to help with renewable energy intermittency management."
Applications of artificial intelligence,Telecommunications,"Many telecommunications companies make use of heuristic search to manage their workforces. For example, BT Group deployed heuristic search in an application that schedules 20,000 engineers. Machine learning is also used for speech recognition (SR), including of voice-controlled devices, and SR-related transcription, including of videos.",65,"Applications of artificial intelligence
Telecommunications

Many telecommunications companies make use of heuristic search to manage their workforces. For example, BT Group deployed heuristic search in an application that schedules 20,000 engineers. Machine learning is also used for speech recognition (SR), including of voice-controlled devices, and SR-related transcription, including of videos.","1. What is heuristic search?
2. What is machine learning?
3. What are some applications of machine learning in telecommunications?","1. Heuristic search is a technique that uses a set of rules to find a solution to a problem.
2. Machine learning is a technique that allows computers to learn from data and improve their performance over time.
3. Some applications of machine learning in telecommunications include heuristic search for workforce management, speech recognition, and transcription."
Applications of artificial intelligence,Sensors,"Artificial intelligence has been combined with digital spectrometry by IdeaCuria Inc., enable applications such as at-home water quality monitoring.",31,"Applications of artificial intelligence
Sensors

Artificial intelligence has been combined with digital spectrometry by IdeaCuria Inc., enable applications such as at-home water quality monitoring.","1. What is digital spectrometry? 
2. What are some applications of at-home water quality monitoring? 
3. How does artificial intelligence help with at-home water quality monitoring?","1. Digital spectrometry is a technique that measures the intensity of light at different wavelengths. 
2. Some applications of at-home water quality monitoring include detecting contaminants such as lead, copper, and chlorine, and measuring pH levels and water hardness. 
3. Artificial intelligence helps with at-home water quality monitoring by analyzing the data from digital spectrometry to identify contaminants and other water quality parameters."
Applications of artificial intelligence,Toys and games,"In the 1990s early AIs controlled Tamagotchis and Giga Pets, the Internet, and the first widely released robot, Furby. Aibo was a domestic robot in the form of a robotic dog with intelligent features and autonomy.
Mattel created an assortment of AI-enabled toys that ""understand"" conversations, give intelligent responses, and learn.",79,"Applications of artificial intelligence
Toys and games

In the 1990s early AIs controlled Tamagotchis and Giga Pets, the Internet, and the first widely released robot, Furby. Aibo was a domestic robot in the form of a robotic dog with intelligent features and autonomy.
Mattel created an assortment of AI-enabled toys that ""understand"" conversations, give intelligent responses, and learn.","1. What was the first widely released robot? 
2. What did Mattel create? 
3. What do the toys ""understand""?","1. Furby was the first widely released robot. 
2. Mattel created an assortment of AI-enabled toys that ""understand"" conversations, give intelligent responses, and learn. 
3. The toys ""understand"" conversations and give intelligent responses."
Applications of artificial intelligence,Oil and gas,"Oil and gas companies have used artificial intelligence tools to automate functions, foresee equipment issues, and increase oil and gas output.",28,"Applications of artificial intelligence
Oil and gas

Oil and gas companies have used artificial intelligence tools to automate functions, foresee equipment issues, and increase oil and gas output.","1. What are some of the ways oil and gas companies have used artificial intelligence tools?
2. How have artificial intelligence tools helped increase oil and gas output?","1. Some of the ways oil and gas companies have used artificial intelligence tools include automating functions, foreseeing equipment issues, and increasing oil and gas output.
2. Artificial intelligence tools have helped increase oil and gas output by automating functions, foreseeing equipment issues, and increasing oil and gas output."
Applications of artificial intelligence,Automotive,"AI in transport is expected to provide safe, efficient, and reliable transportation while minimizing the impact on the environment and communities. The major development challenge is the complexity of transportation systems that involves independent components and parties, with potentially conflicting objectives.AI-based fuzzy logic controllers operate gearboxes. For example, the 2006 Audi TT, VW Touareg and VW Caravell feature the DSP transmission. A number of Škoda variants (Škoda Fabia) include a fuzzy logic-based controller. Cars have AI-based driver-assist features such as self-parking and adaptive cruise control.
There are also prototypes of autonomous automotive public transport vehicles such as electric mini-buses as well as autonomous rail transport in operation.There also are prototypes of autonomous delivery vehicles, sometimes including delivery robots.Transportation's complexity means that in most cases training an AI in a real-world driving environment is impractical. Simulator-based testing can reduce the risks of on-road training.AI underpins self-driving vehicles. Companies involved with AI include Tesla, Waymo, and General Motors. AI-based systems control functions such as braking, lane changing, collision prevention, navigation and mapping.Autonomous trucks are in the testing phase. The UK government passed legislation to begin testing of autonomous truck platoons in 2018. A group of autonomous trucks follow closely behind each other. German corporation Daimler is testing its Freightliner Inspiration.Autonomous vehicles require accurate maps to be able to navigate between destinations. Some autonomous vehicles do not allow human drivers (they have no steering wheels or pedals).",328,"Applications of artificial intelligence
Automotive

AI in transport is expected to provide safe, efficient, and reliable transportation while minimizing the impact on the environment and communities. The major development challenge is the complexity of transportation systems that involves independent components and parties, with potentially conflicting objectives.AI-based fuzzy logic controllers operate gearboxes. For example, the 2006 Audi TT, VW Touareg and VW Caravell feature the DSP transmission. A number of Škoda variants (Škoda Fabia) include a fuzzy logic-based controller. Cars have AI-based driver-assist features such as self-parking and adaptive cruise control.
There are also prototypes of autonomous automotive public transport vehicles such as electric mini-buses as well as autonomous rail transport in operation.There also are prototypes of autonomous delivery vehicles, sometimes including delivery robots.Transportation's complexity means that in most cases training an AI in a real-world driving environment is impractical. Simulator-based testing can reduce the risks of on-road training.AI underpins self-driving vehicles. Companies involved with AI include Tesla, Waymo, and General Motors. AI-based systems control functions such as braking, lane changing, collision prevention, navigation and mapping.Autonomous trucks are in the testing phase. The UK government passed legislation to begin testing of autonomous truck platoons in 2018. A group of autonomous trucks follow closely behind each other. German corporation Daimler is testing its Freightliner Inspiration.Autonomous vehicles require accurate maps to be able to navigate between destinations. Some autonomous vehicles do not allow human drivers (they have no steering wheels or pedals).","1. What are some of the applications of artificial intelligence in transport? 
2. How is AI being used in cars? 
3. What are some of the challenges in using AI in transport? 
4. What is the role of simulation in training AI for transport? 
5. What are some of the autonomous vehicles in development? 
6. How do autonomous vehicles navigate? 
7. What are the benefits of using autonomous vehicles?","1. Some of the applications of artificial intelligence in transport include self-parking cars, adaptive cruise control, and autonomous vehicles. 
2. AI is used in cars to control functions such as braking, lane changing, and collision prevention. 
3. The challenges in using AI in transport include the complexity of transportation systems and the need for accurate maps. 
4. Simulation is used in training AI for transport to reduce the risks of on-road training. 
5. Some of the autonomous vehicles in development include autonomous trucks and buses. 
6. Autonomous vehicles navigate by using accurate maps to find their way between destinations. 
7. The benefits of using autonomous vehicles include improved safety, efficiency, and reliability."
Applications of artificial intelligence,Traffic management,"AI has been used to optimize traffic management, which reduces wait times, energy use, and emissions by as much as 25 percent.

Smart traffic lights have been developed at Carnegie Mellon since 2009.  Professor Stephen Smith has started a company since then Surtrac that has installed smart traffic control systems in 22 cities.  It costs about $20,000 per intersection to install. Drive time has been reduced by 25% and traffic jam waiting time has been reduced by 40% at the intersections it has been installed.",107,"Applications of artificial intelligence
Traffic management

AI has been used to optimize traffic management, which reduces wait times, energy use, and emissions by as much as 25 percent.

Smart traffic lights have been developed at Carnegie Mellon since 2009.  Professor Stephen Smith has started a company since then Surtrac that has installed smart traffic control systems in 22 cities.  It costs about $20,000 per intersection to install. Drive time has been reduced by 25% and traffic jam waiting time has been reduced by 40% at the intersections it has been installed.","1. What is the main use of AI in traffic management?
2. How has AI been able to reduce wait times, energy use, and emissions?
3. How much does it cost to install a smart traffic control system?
4. What has been the result of installing a smart traffic control system?","1. AI is used to optimize traffic management in order to reduce wait times, energy use, and emissions.
2. AI is able to reduce wait times, energy use, and emissions by optimizing traffic management.
3. It costs about $20,000 per intersection to install a smart traffic control system.
4. The result of installing a smart traffic control system is a reduction in drive time by 25% and a reduction in traffic jam waiting time by 40%."
Applications of artificial intelligence,Military,"The Royal Australian Air Force (RAAF) Air Operations Division (AOD) uses AI for expert systems. AIs operate as surrogate operators for combat and training simulators, mission management aids, support systems for tactical decision making, and post processing of the simulator data into symbolic summaries.Aircraft simulators use AI for training aviators. Flight conditions can be simulated that allow pilots to make mistakes without risking themselves or expensive aircraft. Air combat can also be simulated.
AI can also be used to operate planes analogously to their control of ground vehicles. Autonomous drones can fly independently or in swarms.AOD uses the Interactive Fault Diagnosis and Isolation System, or IFDIS, which is a rule-based expert system using information from TF-30 documents and expert advice from mechanics that work on the TF-30. This system was designed to be used for the development of the TF-30 for the F-111C. The system replaced specialized workers. The system allowed regular workers to communicate with the system and avoid mistakes, miscalculations, or having to speak to one of the specialized workers.
Speech recognition allows traffic controllers to give verbal directions to drones.
Artificial intelligence supported design of aircraft, or AIDA, is used to help designers in the process of creating conceptual designs of aircraft. This program allows the designers to focus more on the design itself and less on the design process. The software also allows the user to focus less on the software tools. The AIDA uses rule-based systems to compute its data. This is a diagram of the arrangement of the AIDA modules. Although simple, the program is proving effective.",340,"Applications of artificial intelligence
Military

The Royal Australian Air Force (RAAF) Air Operations Division (AOD) uses AI for expert systems. AIs operate as surrogate operators for combat and training simulators, mission management aids, support systems for tactical decision making, and post processing of the simulator data into symbolic summaries.Aircraft simulators use AI for training aviators. Flight conditions can be simulated that allow pilots to make mistakes without risking themselves or expensive aircraft. Air combat can also be simulated.
AI can also be used to operate planes analogously to their control of ground vehicles. Autonomous drones can fly independently or in swarms.AOD uses the Interactive Fault Diagnosis and Isolation System, or IFDIS, which is a rule-based expert system using information from TF-30 documents and expert advice from mechanics that work on the TF-30. This system was designed to be used for the development of the TF-30 for the F-111C. The system replaced specialized workers. The system allowed regular workers to communicate with the system and avoid mistakes, miscalculations, or having to speak to one of the specialized workers.
Speech recognition allows traffic controllers to give verbal directions to drones.
Artificial intelligence supported design of aircraft, or AIDA, is used to help designers in the process of creating conceptual designs of aircraft. This program allows the designers to focus more on the design itself and less on the design process. The software also allows the user to focus less on the software tools. The AIDA uses rule-based systems to compute its data. This is a diagram of the arrangement of the AIDA modules. Although simple, the program is proving effective.","1. What are some applications of AI in the military? 
2. How do AI systems help in aircraft design? 
3. How does speech recognition help in military operations?","1. AI is used in military operations for tasks such as aircraft simulation for training and combat, speech recognition, and the design of aircraft. 
2. AI is used in aircraft design to help designers focus on the design itself and less on the design process, and to allow the user to focus less on the software tools. 
3. Speech recognition helps in military operations by allowing traffic controllers to give verbal directions to drones."
Applications of artificial intelligence,NASA,In 2003 a Dryden Flight Research Center project created software that could enable a damaged aircraft to continue flight until a safe landing can be achieved. The software compensated for damaged components by relying on the remaining undamaged components.The 2016 Intelligent Autopilot System combined apprenticeship learning and behavioral cloning whereby the autopilot observed low-level actions required to maneuver the airplane and high-level strategy used to apply those actions.,86,"Applications of artificial intelligence
NASA

In 2003 a Dryden Flight Research Center project created software that could enable a damaged aircraft to continue flight until a safe landing can be achieved. The software compensated for damaged components by relying on the remaining undamaged components.The 2016 Intelligent Autopilot System combined apprenticeship learning and behavioral cloning whereby the autopilot observed low-level actions required to maneuver the airplane and high-level strategy used to apply those actions.","1. What is the Intelligent Autopilot System?
2. How does the Intelligent Autopilot System work?","1. The Intelligent Autopilot System is a software that enables a damaged aircraft to continue flight until a safe landing can be achieved.
2. The Intelligent Autopilot System works by compensating for damaged components by relying on the remaining undamaged components."
Applications of artificial intelligence,Maritime,Neural networks are used by situational awareness systems in ships and boats. There also are autonomous boats.,23,"Applications of artificial intelligence
Maritime

Neural networks are used by situational awareness systems in ships and boats. There also are autonomous boats.","1. What is a situational awareness system? 
2. What are autonomous boats?","1. A situational awareness system is a system that uses artificial intelligence to help a user understand their surroundings. 
2. Autonomous boats are boats that are able to operate without a human driver."
Applications of artificial intelligence,Environmental monitoring,"Autonomous ships that monitor the ocean, AI-driven satellite data analysis, passive acoustics or remote sensing and other applications of environmental monitoring make use of machine learning.For example, ""Global Plastic Watch"" is an AI-based satellite monitoring-platform for analysis/tracking of plastic waste sites to help prevention of plastic pollution – primarily ocean pollution – by helping identify who and where mismanages plastic waste, dumping it into oceans.",90,"Applications of artificial intelligence
Environmental monitoring

Autonomous ships that monitor the ocean, AI-driven satellite data analysis, passive acoustics or remote sensing and other applications of environmental monitoring make use of machine learning.For example, ""Global Plastic Watch"" is an AI-based satellite monitoring-platform for analysis/tracking of plastic waste sites to help prevention of plastic pollution – primarily ocean pollution – by helping identify who and where mismanages plastic waste, dumping it into oceans.","1. What is the main purpose of Global Plastic Watch?
2. How does machine learning help with environmental monitoring?
3. What is the main benefit of using machine learning for environmental monitoring?","1. The main purpose of Global Plastic Watch is to identify who and where mismanages plastic waste, dumping it into oceans.
2. Machine learning helps with environmental monitoring by analyzing satellite data to track plastic waste sites.
3. The main benefit of using machine learning for environmental monitoring is that it helps prevent ocean pollution."
Applications of artificial intelligence,Early-warning systems,"Machine learning can be used to spot early-warning signs of disasters and environmental issues, possibly including natural pandemics, earthquakes, landslides, heavy rainfall, long-term water supply vulnerability, tipping-points of ecosystem collapse, cyanobacterial bloom outbreaks, and droughts.",60,"Applications of artificial intelligence
Early-warning systems

Machine learning can be used to spot early-warning signs of disasters and environmental issues, possibly including natural pandemics, earthquakes, landslides, heavy rainfall, long-term water supply vulnerability, tipping-points of ecosystem collapse, cyanobacterial bloom outbreaks, and droughts.","1. What are some potential applications of machine learning in early-warning systems? 
2. How could early-warning systems help to prevent or mitigate disasters?
3. What are some potential dangers of not using early-warning systems?","1. Some potential applications of machine learning in early-warning systems include spotting early-warning signs of natural pandemics, earthquakes, landslides, heavy rainfall, long-term water supply vulnerability, tipping-points of ecosystem collapse, cyanobacterial bloom outbreaks, and droughts.
2. Early-warning systems can help to prevent or mitigate disasters by providing alerts to people who may be in danger, providing information about what to do to stay safe, and helping to coordinate response efforts.
3. Potential dangers of not using early-warning systems include increased risk of injury or death, loss of property, and damage to the environment."
Applications of artificial intelligence,Programming assistance,GitHub Copilot is an artificial intelligence model developed by GitHub and OpenAI that is able to autocomplete code in multiple programming languages.,32,"Applications of artificial intelligence
Programming assistance

GitHub Copilot is an artificial intelligence model developed by GitHub and OpenAI that is able to autocomplete code in multiple programming languages.","1. What is GitHub Copilot?
2. What programming languages can it autocomplete code in?
3. How was GitHub Copilot developed?","1. GitHub Copilot is an artificial intelligence model developed by GitHub and OpenAI that is able to autocomplete code in multiple programming languages.
2. GitHub Copilot can autocomplete code in multiple programming languages, including Python, Java, and C++.
3. GitHub Copilot was developed by GitHub and OpenAI by using a machine learning model that is able to predict the next word or symbol in a code sequence."
Applications of artificial intelligence,Neural network design,"AI can be used to create other AIs. For example, around November 2017, Google's AutoML project to evolve new neural net topologies created NASNet, a system optimized for ImageNet and POCO F1. NASNet's performance exceeded all previously published performance on ImageNet.",63,"Applications of artificial intelligence
Neural network design

AI can be used to create other AIs. For example, around November 2017, Google's AutoML project to evolve new neural net topologies created NASNet, a system optimized for ImageNet and POCO F1. NASNet's performance exceeded all previously published performance on ImageNet.","1. What is Google's AutoML project?
2. What was the outcome of the project?
3. What is ImageNet?
4. What is POCO F1?","1. Google's AutoML project is a project to create new AIs using AI.
2. The outcome of the project was the creation of NASNet, a system that outperforms all previously published performance on ImageNet.
3. ImageNet is a database of images used for object recognition research.
4. POCO F1 is a mobile object detection benchmark."
Applications of artificial intelligence,Quantum computing,"Machine learning has been used for noise-cancelling in quantum technology, including quantum sensors. Moreover, there is substantial research and development of using quantum computers with machine learning algorithms. For example, there is a prototype, photonic, quantum memristive device for neuromorphic (quantum-)computers (NC)/artificial neural networks and NC-using quantum materials with some variety of potential neuromorphic computing-related applications, and quantum machine learning is a field with some variety of applications under development. AI could be used for quantum simulators which may have the application of solving physics and chemistry problems as well as for quantum annealers for training of neural networks for AI applications. There may also be some usefulness in chemistry, e.g. for drug discovery, and in materials science, e.g. for materials optimization/discovery (with possible relevance to quantum materials manufacturing).",182,"Applications of artificial intelligence
Quantum computing

Machine learning has been used for noise-cancelling in quantum technology, including quantum sensors. Moreover, there is substantial research and development of using quantum computers with machine learning algorithms. For example, there is a prototype, photonic, quantum memristive device for neuromorphic (quantum-)computers (NC)/artificial neural networks and NC-using quantum materials with some variety of potential neuromorphic computing-related applications, and quantum machine learning is a field with some variety of applications under development. AI could be used for quantum simulators which may have the application of solving physics and chemistry problems as well as for quantum annealers for training of neural networks for AI applications. There may also be some usefulness in chemistry, e.g. for drug discovery, and in materials science, e.g. for materials optimization/discovery (with possible relevance to quantum materials manufacturing).","1. What are some of the applications of quantum computing that involve machine learning? 
2. How could AI be used in quantum simulators? 
3. What are some possible applications of quantum machine learning?","1. Some of the applications of quantum computing that involve machine learning are noise-cancelling in quantum technology, quantum sensors, and prototype, photonic, quantum memristive device for neuromorphic (quantum-)computers (NC)/artificial neural networks. 
2. AI could be used in quantum simulators to solve physics and chemistry problems. 
3. Some possible applications of quantum machine learning are drug discovery, materials optimization/discovery, and quantum materials manufacturing."
Applications of artificial intelligence,Historical contributions,"AI researchers have created many tools to solve the most difficult problems in computer science. Many of their inventions have been adopted by mainstream computer science and are no longer considered AI. All of the following were originally developed in AI laboratories:
Time sharing
Interactive interpreters
Graphical user interfaces and the computer mouse
Rapid application development environments
The linked list data structure
Automatic storage management
Symbolic programming
Functional programming
Dynamic programming
Object-oriented programming
Optical character recognition
Constraint satisfaction",109,"Applications of artificial intelligence
Historical contributions

AI researchers have created many tools to solve the most difficult problems in computer science. Many of their inventions have been adopted by mainstream computer science and are no longer considered AI. All of the following were originally developed in AI laboratories:
Time sharing
Interactive interpreters
Graphical user interfaces and the computer mouse
Rapid application development environments
The linked list data structure
Automatic storage management
Symbolic programming
Functional programming
Dynamic programming
Object-oriented programming
Optical character recognition
Constraint satisfaction","1. What are some of the historical contributions of AI researchers?
2. What are some of the tools that AI researchers have created?
3. What are some of the inventions that have been adopted by mainstream computer science?","1. AI researchers have created many tools to solve the most difficult problems in computer science.
2. These tools include time sharing, interactive interpreters, graphical user interfaces and the computer mouse, rapid application development environments, the linked list data structure, automatic storage management, symbolic programming, functional programming, dynamic programming, object-oriented programming, and optical character recognition.
3. These inventions have been adopted by mainstream computer science and are no longer considered AI."
Applications of artificial intelligence,Customer service,Business websites and social media platforms for businesses like use chatbots for customer interactions like helping in answering frequently asked questions. Chatbots offers 24/7 support and replaces humans thereby helping in cutting business costs.,43,"Applications of artificial intelligence
Customer service

Business websites and social media platforms for businesses like use chatbots for customer interactions like helping in answering frequently asked questions. Chatbots offers 24/7 support and replaces humans thereby helping in cutting business costs.","1. What are chatbots and how do they work?
2. What are the benefits of using chatbots for customer service?
3. Are chatbots only used for customer service? If not, what are some other applications of chatbots?","1. Chatbots are computer programs that mimic human conversation. They work by responding to user input through a chat interface.
2. The benefits of using chatbots for customer service include 24/7 support, faster response times, and lower costs.
3. Chatbots are not limited to customer service applications. They can also be used for tasks such as content moderation, lead generation, and market research."
Applications of artificial intelligence,Content extraction,"An optical character reader is used in the extraction of data in business documents like invoices and receipts. It can also be used in business contract documents e.g employment agreements to extract critical data like employment terms, delivery terms, termination clauses, etc.",54,"Applications of artificial intelligence
Content extraction

An optical character reader is used in the extraction of data in business documents like invoices and receipts. It can also be used in business contract documents e.g employment agreements to extract critical data like employment terms, delivery terms, termination clauses, etc.","1. What is an optical character reader?
2. What are some of the uses of an optical character reader?
3. What is the extraction of data?
4. What are some of the benefits of data extraction?","1. An optical character reader is a device that is used to extract data from business documents like invoices and receipts.
2. Some of the uses of an optical character reader include the extraction of data from business contract documents and the extraction of critical data from employment agreements.
3. The extraction of data is the process of extracting information from a document.
4. The benefits of data extraction include the ability to quickly and easily extract information from documents and the ability to automate the process of data extraction."
History of artificial intelligence,Summary,"The history of artificial intelligence (AI) began in antiquity, with myths, stories and rumors of artificial beings endowed with intelligence or consciousness by master craftsmen. The seeds of modern AI were planted by philosophers who attempted to describe the process of human thinking as the mechanical manipulation of symbols. This work culminated in the invention of the programmable digital computer in the 1940s, a machine based on the abstract essence of mathematical reasoning. This device and the ideas behind it inspired a handful of scientists to begin seriously discussing the possibility of building an electronic brain.
The field of AI research was founded at a workshop held on the campus of Dartmouth College, USA during the summer of 1956. Those who attended would become the leaders of AI research for decades. Many of them predicted that a machine as intelligent as a human being would exist in no more than a generation, and they were given millions of dollars to make this vision come true.Eventually, it became obvious that commercial developers and researchers had grossly underestimated the difficulty of the project. In 1974, in response to the criticism from James Lighthill and ongoing pressure from congress, the U.S. and British Governments stopped funding undirected research into artificial intelligence, and the difficult years that followed would later be known as an ""AI winter"". Seven years later, a visionary initiative by the Japanese Government inspired governments and industry to provide AI with billions of dollars, but by the late 1980s the investors became disillusioned and withdrew funding again.
Investment and interest in AI boomed in the first decades of the 21st century when machine learning was successfully applied to many problems in academia and industry due to new methods, the application of powerful computer hardware, and the collection of immense data sets.",349,"History of artificial intelligence
Summary

The history of artificial intelligence (AI) began in antiquity, with myths, stories and rumors of artificial beings endowed with intelligence or consciousness by master craftsmen. The seeds of modern AI were planted by philosophers who attempted to describe the process of human thinking as the mechanical manipulation of symbols. This work culminated in the invention of the programmable digital computer in the 1940s, a machine based on the abstract essence of mathematical reasoning. This device and the ideas behind it inspired a handful of scientists to begin seriously discussing the possibility of building an electronic brain.
The field of AI research was founded at a workshop held on the campus of Dartmouth College, USA during the summer of 1956. Those who attended would become the leaders of AI research for decades. Many of them predicted that a machine as intelligent as a human being would exist in no more than a generation, and they were given millions of dollars to make this vision come true.Eventually, it became obvious that commercial developers and researchers had grossly underestimated the difficulty of the project. In 1974, in response to the criticism from James Lighthill and ongoing pressure from congress, the U.S. and British Governments stopped funding undirected research into artificial intelligence, and the difficult years that followed would later be known as an ""AI winter"". Seven years later, a visionary initiative by the Japanese Government inspired governments and industry to provide AI with billions of dollars, but by the late 1980s the investors became disillusioned and withdrew funding again.
Investment and interest in AI boomed in the first decades of the 21st century when machine learning was successfully applied to many problems in academia and industry due to new methods, the application of powerful computer hardware, and the collection of immense data sets.","1. What is the history of artificial intelligence?
2. What inspired the scientists who founded the field of AI research?
3. Why did investment and interest in AI boom in the first decades of the 21st century?","1. The history of artificial intelligence began in antiquity, with myths, stories and rumors of artificial beings endowed with intelligence or consciousness by master craftsmen. The seeds of modern AI were planted by philosophers who attempted to describe the process of human thinking as the mechanical manipulation of symbols. This work culminated in the invention of the programmable digital computer in the 1940s, a machine based on the abstract essence of mathematical reasoning. This device and the ideas behind it inspired a handful of scientists to begin seriously discussing the possibility of building an electronic brain.

2. The field of AI research was founded at a workshop held on the campus of Dartmouth College, USA during the summer of 1956. Those who attended would become the leaders of AI research for decades. Many of them predicted that a machine as intelligent as a human being would exist in no more than a generation, and they were given millions of dollars to make this vision come true.

3. Investment and interest in AI boomed in the first decades of the 21st century when machine learning was successfully applied to many problems in academia and industry due to new methods, the application of powerful computer hardware, and the collection of immense data sets."
History of artificial intelligence,Myth and legend,"In Greek mythology, Talos was a giant constructed of bronze who acted as guardian for the island of Crete. He would throw boulders at the ships of invaders, and would complete 3 circuits around the island's perimeter daily. According to pseudo-Apollodorus' Bibliotheke, Hephaestus forged Talos with the aid of a cyclops and presented the automaton as a gift to Minos. In the Argonautica, Jason and the Argonauts defeated him by way of a single plug near his foot which, once removed, allowed the vital ichor to flow out from his body and left him inanimate.Pygmalion was a legendary king and sculptor of Greek mythology, famously represented in Ovid's Metamorphoses. In the 10th book of Ovid's narrative poem, Pygmalion becomes disgusted with women when he witnesses the way in which the Propoetides prostitute themselves. Despite this, he makes offerings at the temple of Venus asking the goddess to bring to him a woman just like a statue he carved. The earliest written account regarding golem-making is found in the writings of Eleazar ben Judah of Worms in the early 13th century. During the Middle Ages, it was believed that the animation of a Golem could be achieved by insertion of a piece of paper with any of God’s names on it, into the mouth of the clay figure. Unlike legendary automata like Brazen Heads, a Golem was unable to speak. English scholar Alexander Neckham asserted that the Ancient Roman poet Virgil had built a palace with automaton statues.",332,"History of artificial intelligence
Myth and legend

In Greek mythology, Talos was a giant constructed of bronze who acted as guardian for the island of Crete. He would throw boulders at the ships of invaders, and would complete 3 circuits around the island's perimeter daily. According to pseudo-Apollodorus' Bibliotheke, Hephaestus forged Talos with the aid of a cyclops and presented the automaton as a gift to Minos. In the Argonautica, Jason and the Argonauts defeated him by way of a single plug near his foot which, once removed, allowed the vital ichor to flow out from his body and left him inanimate.Pygmalion was a legendary king and sculptor of Greek mythology, famously represented in Ovid's Metamorphoses. In the 10th book of Ovid's narrative poem, Pygmalion becomes disgusted with women when he witnesses the way in which the Propoetides prostitute themselves. Despite this, he makes offerings at the temple of Venus asking the goddess to bring to him a woman just like a statue he carved. The earliest written account regarding golem-making is found in the writings of Eleazar ben Judah of Worms in the early 13th century. During the Middle Ages, it was believed that the animation of a Golem could be achieved by insertion of a piece of paper with any of God’s names on it, into the mouth of the clay figure. Unlike legendary automata like Brazen Heads, a Golem was unable to speak. English scholar Alexander Neckham asserted that the Ancient Roman poet Virgil had built a palace with automaton statues.","1. What is the legend of Talos?
2. What did Pygmalion do to get a woman like a statue he carved?
3. What did Eleazar ben Judah of Worms write about golem-making?
4. What did Alexander Neckham say about automaton statues?","1. The legend of Talos is that he was a giant constructed of bronze who acted as guardian for the island of Crete.
2. Pygmalion did to get a woman like a statue he carved was pray to Venus to bring him a woman that was just like the statue.
3. Eleazar ben Judah of Worms wrote about golem-making was that you could animate a Golem by putting a piece of paper with any of God's names on it, into the mouth of the clay figure.
4. Alexander Neckham said that the Ancient Roman poet Virgil had built a palace with automaton statues."
History of artificial intelligence,Alchemical means of artificial intelligence,"In Of the Nature of Things, written by the Swiss alchemist, Paracelsus, he describes a procedure which he claims can fabricate an ""artificial man"". By placing the ""sperm of a man"" in horse dung, and feeding it the ""Arcanum of Mans blood"" after 40 days, the concoction will become a living infant. Takwin, the artificial creation of life, was a frequent topic of Ismaili alchemical manuscripts, especially those attributed to Jabir ibn Hayyan. Islamic alchemists attempted to create a broad range of life through their work, ranging from plants to animals. In Faust: The Second Part of the Tragedy by Johann Wolfgang von Goethe, an alchemically fabricated homunculus, destined to live forever in the flask in which he was made, endeavors to be born into a full human body. Upon the initiation of this transformation, however, the flask shatters and the homunculus dies.",205,"History of artificial intelligence
Alchemical means of artificial intelligence

In Of the Nature of Things, written by the Swiss alchemist, Paracelsus, he describes a procedure which he claims can fabricate an ""artificial man"". By placing the ""sperm of a man"" in horse dung, and feeding it the ""Arcanum of Mans blood"" after 40 days, the concoction will become a living infant. Takwin, the artificial creation of life, was a frequent topic of Ismaili alchemical manuscripts, especially those attributed to Jabir ibn Hayyan. Islamic alchemists attempted to create a broad range of life through their work, ranging from plants to animals. In Faust: The Second Part of the Tragedy by Johann Wolfgang von Goethe, an alchemically fabricated homunculus, destined to live forever in the flask in which he was made, endeavors to be born into a full human body. Upon the initiation of this transformation, however, the flask shatters and the homunculus dies.","1. What is the history of artificial intelligence?
2. What are some examples of artificial intelligence in history?
3. What are the goals of artificial intelligence?","1. The history of artificial intelligence dates back to the alchemists of the Middle Ages.
2. Some examples of artificial intelligence in history include Paracelsus' artificial man, Jabir ibn Hayyan's Takwin, and Faust's homunculus.
3. The goals of artificial intelligence vary, but often include creating machines that can think and act like humans."
History of artificial intelligence,Modern fiction,"By the 19th century, ideas about artificial men and thinking machines were developed in fiction, as in Mary Shelley's Frankenstein  or Karel Čapek's R.U.R. (Rossum's Universal Robots),
and speculation, such as Samuel Butler's ""Darwin among the Machines"", and in real world instances, including Edgar Allan Poe's ""Maelzel's Chess Player"".
AI has become a regular topic of science fiction through the present.",98,"History of artificial intelligence
Modern fiction

By the 19th century, ideas about artificial men and thinking machines were developed in fiction, as in Mary Shelley's Frankenstein  or Karel Čapek's R.U.R. (Rossum's Universal Robots),
and speculation, such as Samuel Butler's ""Darwin among the Machines"", and in real world instances, including Edgar Allan Poe's ""Maelzel's Chess Player"".
AI has become a regular topic of science fiction through the present.","1. What is the history of artificial intelligence?
2. What are some examples of AI in modern fiction?
3. What is the future of AI?","1. The history of artificial intelligence can be traced back to the 19th century, when ideas about artificial men and thinking machines were developed in fiction.
2. Some examples of AI in modern fiction include Mary Shelley's Frankenstein and Karel Čapek's R.U.R.
3. The future of AI is uncertain, but it is likely that it will continue to play a significant role in our lives."
History of artificial intelligence,Automata,"Realistic humanoid automata were built by craftsman from every civilization, including Yan Shi,Hero of Alexandria,Al-Jazari,Pierre Jaquet-Droz, and Wolfgang von Kempelen.
The oldest known automata were the sacred statues of ancient Egypt and Greece. The faithful believed that craftsman had imbued these figures with very real minds, capable of wisdom and emotion—Hermes Trismegistus wrote that ""by discovering the true nature of the gods, man has been able to reproduce it"".During the early modern period, these legendary automata were said to possess the magical ability to answer questions put to them. The late medieval alchemist and proto-protestant Roger Bacon was purported to have fabricated a brazen head, having developed a legend of having been a wizard. These legends were similar to the Norse myth of the Head of Mímir. According to legend, Mímir was known for his intellect and wisdom, and was beheaded in the Æsir-Vanir War. Odin is said to have ""embalmed"" the head with herbs and spoke incantations over it such that Mímir’s head remained able to speak wisdom to Odin. Odin then kept the head near him for counsel.",260,"History of artificial intelligence
Automata

Realistic humanoid automata were built by craftsman from every civilization, including Yan Shi,Hero of Alexandria,Al-Jazari,Pierre Jaquet-Droz, and Wolfgang von Kempelen.
The oldest known automata were the sacred statues of ancient Egypt and Greece. The faithful believed that craftsman had imbued these figures with very real minds, capable of wisdom and emotion—Hermes Trismegistus wrote that ""by discovering the true nature of the gods, man has been able to reproduce it"".During the early modern period, these legendary automata were said to possess the magical ability to answer questions put to them. The late medieval alchemist and proto-protestant Roger Bacon was purported to have fabricated a brazen head, having developed a legend of having been a wizard. These legends were similar to the Norse myth of the Head of Mímir. According to legend, Mímir was known for his intellect and wisdom, and was beheaded in the Æsir-Vanir War. Odin is said to have ""embalmed"" the head with herbs and spoke incantations over it such that Mímir’s head remained able to speak wisdom to Odin. Odin then kept the head near him for counsel.","1. What is the oldest known automaton? 
2. What did the ancient Egyptians and Greeks believe about automata? 
3. What is a brazen head? 
4. What is the Norse myth of the Head of Mímir? 
5. What did Roger Bacon allegedly do?","1. The oldest known automaton is the sacred statues of ancient Egypt and Greece. 
2. The ancient Egyptians and Greeks believed that craftsman had imbued these figures with very real minds, capable of wisdom and emotion. 
3. A brazen head is a legendary automaton said to possess the magical ability to answer questions put to it. 
4. The Norse myth of the Head of Mímir is about a head that was known for its intellect and wisdom, and was beheaded in the Æsir-Vanir War. 
5. Roger Bacon is purported to have fabricated a brazen head, having developed a legend of having been a wizard."
History of artificial intelligence,Formal reasoning,"Artificial intelligence is based on the assumption that the process of human thought can be mechanized. The study of mechanical—or ""formal""—reasoning has a long history. Chinese, Indian and Greek philosophers all developed structured methods of formal deduction in the first millennium BCE. Their ideas were developed over the centuries by philosophers such as Aristotle (who gave a formal analysis of the syllogism), Euclid (whose Elements was a model of formal reasoning), al-Khwārizmī (who developed algebra and gave his name to ""algorithm"") and European scholastic philosophers such as William of Ockham and Duns Scotus.Spanish philosopher Ramon Llull (1232–1315) developed several logical machines devoted to the production of knowledge by logical means; Llull described his machines as mechanical entities that could combine basic and undeniable truths by simple logical operations, produced by the machine by mechanical meanings, in such ways as to produce all the possible knowledge. Llull's work had a great influence on Gottfried Leibniz, who redeveloped his ideas.

In the 17th century, Leibniz, Thomas Hobbes and René Descartes explored the possibility that all rational thought could be made as systematic as algebra or geometry. Hobbes famously wrote in Leviathan: ""reason is nothing but reckoning"". Leibniz envisioned a universal language of reasoning, the characteristica universalis, which would reduce argumentation to calculation so that ""there would be no more need of disputation between two philosophers than between two accountants. For it would suffice to take their pencils in hand, down to their slates, and to say each other (with a friend as witness, if they liked): Let us calculate."" These philosophers had begun to articulate the physical symbol system hypothesis that would become the guiding faith of AI research.
In the 20th century, the study of mathematical logic provided the essential breakthrough that made artificial intelligence seem plausible. The foundations had been set by such works as Boole's The Laws of Thought and Frege's Begriffsschrift. Building on Frege's system, Russell and Whitehead presented a formal treatment of the foundations of mathematics in their masterpiece, the Principia Mathematica in 1913. Inspired by Russell's success, David Hilbert challenged mathematicians of the 1920s and 30s to answer this fundamental question: ""can all of mathematical reasoning be formalized?""
His question was answered by Gödel's incompleteness proof, Turing's machine and Church's Lambda calculus.

Their answer was surprising in two ways. First, they proved that there were, in fact, limits to what mathematical logic could accomplish. But second (and more important for AI) their work suggested that, within these limits, any form of mathematical reasoning could be mechanized. The Church-Turing thesis implied that a mechanical device, shuffling symbols as simple as 0 and 1, could imitate any conceivable process of mathematical deduction. The key insight was the Turing machine—a simple theoretical construct that captured the essence of abstract symbol manipulation. This invention would inspire a handful of scientists to begin discussing the possibility of thinking machines.",648,"History of artificial intelligence
Formal reasoning

Artificial intelligence is based on the assumption that the process of human thought can be mechanized. The study of mechanical—or ""formal""—reasoning has a long history. Chinese, Indian and Greek philosophers all developed structured methods of formal deduction in the first millennium BCE. Their ideas were developed over the centuries by philosophers such as Aristotle (who gave a formal analysis of the syllogism), Euclid (whose Elements was a model of formal reasoning), al-Khwārizmī (who developed algebra and gave his name to ""algorithm"") and European scholastic philosophers such as William of Ockham and Duns Scotus.Spanish philosopher Ramon Llull (1232–1315) developed several logical machines devoted to the production of knowledge by logical means; Llull described his machines as mechanical entities that could combine basic and undeniable truths by simple logical operations, produced by the machine by mechanical meanings, in such ways as to produce all the possible knowledge. Llull's work had a great influence on Gottfried Leibniz, who redeveloped his ideas.

In the 17th century, Leibniz, Thomas Hobbes and René Descartes explored the possibility that all rational thought could be made as systematic as algebra or geometry. Hobbes famously wrote in Leviathan: ""reason is nothing but reckoning"". Leibniz envisioned a universal language of reasoning, the characteristica universalis, which would reduce argumentation to calculation so that ""there would be no more need of disputation between two philosophers than between two accountants. For it would suffice to take their pencils in hand, down to their slates, and to say each other (with a friend as witness, if they liked): Let us calculate."" These philosophers had begun to articulate the physical symbol system hypothesis that would become the guiding faith of AI research.
In the 20th century, the study of mathematical logic provided the essential breakthrough that made artificial intelligence seem plausible. The foundations had been set by such works as Boole's The Laws of Thought and Frege's Begriffsschrift. Building on Frege's system, Russell and Whitehead presented a formal treatment of the foundations of mathematics in their masterpiece, the Principia Mathematica in 1913. Inspired by Russell's success, David Hilbert challenged mathematicians of the 1920s and 30s to answer this fundamental question: ""can all of mathematical reasoning be formalized?""
His question was answered by Gödel's incompleteness proof, Turing's machine and Church's Lambda calculus.

Their answer was surprising in two ways. First, they proved that there were, in fact, limits to what mathematical logic could accomplish. But second (and more important for AI) their work suggested that, within these limits, any form of mathematical reasoning could be mechanized. The Church-Turing thesis implied that a mechanical device, shuffling symbols as simple as 0 and 1, could imitate any conceivable process of mathematical deduction. The key insight was the Turing machine—a simple theoretical construct that captured the essence of abstract symbol manipulation. This invention would inspire a handful of scientists to begin discussing the possibility of thinking machines.","1. What is the history of artificial intelligence?
2. What is the basis of artificial intelligence?
3. What is the significance of formal reasoning?
4. What was the impact of the work of Ramon Llull on artificial intelligence?
5. What was the impact of the work of Gottfried Leibniz on artificial intelligence?
6. What was the impact of the work of Alan Turing on artificial intelligence?
7. What was the impact of the work of John McCarthy","1. The history of artificial intelligence can be traced back to the ancient Chinese, Indian and Greek philosophers who developed structured methods of formal deduction in the first millennium BCE.
2. Artificial intelligence is based on the assumption that the process of human thought can be mechanized.
3. The study of mechanical reasoning has a long history and has been developed by philosophers such as Aristotle, Euclid, al-Khwārizmī and European scholastic philosophers.
4. Spanish philosopher Ramon Llull developed several logical machines devoted to the production of knowledge by logical means and his work had a great influence on Gottfried Leibniz.
5. Gottfried Leibniz envisioned a universal language of reasoning, the characteristica universalis, which would reduce argumentation to calculation so that ""there would be no more need of disputation between two philosophers than between two accountants. For it would suffice to take their pencils in hand, down to their slates, and to say each other (with a friend as witness, if they liked): Let us calculate.""
6. Alan Turing's machine inspired a handful of scientists to begin discussing the possibility of thinking machines.
7. John McCarthy is credited with coining the term ""artificial intelligence"" and"
History of artificial intelligence,Computer science,"Calculating machines were built in antiquity and improved throughout history by many mathematicians, including (once again) philosopher Gottfried Leibniz. In the early 19th century, Charles Babbage designed a programmable computer (the Analytical Engine), although it was never built. Ada Lovelace speculated that the machine ""might compose elaborate and scientific pieces of music of any degree of complexity or extent"". (She is often credited as the first programmer because of a set of notes she wrote that completely detail a method for calculating Bernoulli numbers with the Engine.)
Following Babbage, although at first unaware of his earlier work, was Percy Ludgate, a clerk to a corn merchant in Dublin, Ireland. He independently designed a programmable mechanical computer, which he described in a work that was published in 1909.

Two other inventors, Leonardo Torres Quevedo and Vannevar Bush, also did follow on research based on Babbage's work. In his Essays on Automatics (1913) Torres designed a Babbage type of calculating machine that used electromechanical parts which introduced the idea of floating-point arithmetic. Torres is also known for having built in 1912 an autonomous machine capable of playing chess, El Ajedrecista. As opposed to The Turk and Ajeeb, El Ajedrecista (The Chessplayer) had a true integrated automation. It only played an endgame with three chess pieces, automatically moving a white king and a rook to checkmate the black king moved by a human opponent.Vannevar Bush's paper Instrumental Analysis (1936) discussed using existing IBM punch card machines to implement Babbage's design. In the same year he started the Rapid Arithmetical Machine project to investigate the problems of constructing an electronic digital computer.The first modern computers were the massive code breaking machines of the Second World War (such as Z3, ENIAC and Colossus). The latter two of these machines were based on the theoretical foundation laid by Alan Turing and developed by John von Neumann.",417,"History of artificial intelligence
Computer science

Calculating machines were built in antiquity and improved throughout history by many mathematicians, including (once again) philosopher Gottfried Leibniz. In the early 19th century, Charles Babbage designed a programmable computer (the Analytical Engine), although it was never built. Ada Lovelace speculated that the machine ""might compose elaborate and scientific pieces of music of any degree of complexity or extent"". (She is often credited as the first programmer because of a set of notes she wrote that completely detail a method for calculating Bernoulli numbers with the Engine.)
Following Babbage, although at first unaware of his earlier work, was Percy Ludgate, a clerk to a corn merchant in Dublin, Ireland. He independently designed a programmable mechanical computer, which he described in a work that was published in 1909.

Two other inventors, Leonardo Torres Quevedo and Vannevar Bush, also did follow on research based on Babbage's work. In his Essays on Automatics (1913) Torres designed a Babbage type of calculating machine that used electromechanical parts which introduced the idea of floating-point arithmetic. Torres is also known for having built in 1912 an autonomous machine capable of playing chess, El Ajedrecista. As opposed to The Turk and Ajeeb, El Ajedrecista (The Chessplayer) had a true integrated automation. It only played an endgame with three chess pieces, automatically moving a white king and a rook to checkmate the black king moved by a human opponent.Vannevar Bush's paper Instrumental Analysis (1936) discussed using existing IBM punch card machines to implement Babbage's design. In the same year he started the Rapid Arithmetical Machine project to investigate the problems of constructing an electronic digital computer.The first modern computers were the massive code breaking machines of the Second World War (such as Z3, ENIAC and Colossus). The latter two of these machines were based on the theoretical foundation laid by Alan Turing and developed by John von Neumann.","1. What is the history of artificial intelligence?
2. Who were some of the key players in the development of artificial intelligence?
3. What were some of the key inventions or discoveries in the development of artificial intelligence?","1. The history of artificial intelligence can be traced back to ancient times, when calculating machines were first built. Throughout history, many mathematicians have contributed to the development of artificial intelligence, including Gottfried Leibniz and Charles Babbage.
2. Some of the key players in the development of artificial intelligence include Gottfried Leibniz, Charles Babbage, Ada Lovelace, Percy Ludgate, Leonardo Torres Quevedo, John von Neumann, and Alan Turing.
3. Some of the key inventions or discoveries in the development of artificial intelligence include the calculating machines built in antiquity, the programmable computer designed by Charles Babbage, the use of electromechanical parts in calculating machines, the chess-playing machine built by Leonardo Torres Quevedo, and the electronic digital computer developed by John von Neumann."
History of artificial intelligence,Birth of artificial intelligence (1952–1956),"In the 1940s and 50s, a handful of scientists from a variety of fields (mathematics, psychology, engineering, economics and political science) began to discuss the possibility of creating an artificial brain. The field of artificial intelligence research was founded as an academic discipline in 1956.",63,"History of artificial intelligence
Birth of artificial intelligence (1952–1956)

In the 1940s and 50s, a handful of scientists from a variety of fields (mathematics, psychology, engineering, economics and political science) began to discuss the possibility of creating an artificial brain. The field of artificial intelligence research was founded as an academic discipline in 1956.","1. What was the motivation behind the creation of the field of artificial intelligence research?
2. What were some of the challenges that early artificial intelligence researchers faced?","1. The motivation behind the creation of the field of artificial intelligence research was the desire to create a machine that could think and learn like a human.
2. Early artificial intelligence researchers faced a number of challenges, including the development of algorithms that could enable machines to learn and the creation of software that could allow machines to process information."
History of artificial intelligence,Cybernetics and early neural networks,"The earliest research into thinking machines was inspired by a confluence of ideas that became prevalent in the late 1930s, 1940s, and early 1950s. Recent research in neurology had shown that the brain was an electrical network of neurons that fired in all-or-nothing pulses. Norbert Wiener's cybernetics described control and stability in electrical networks. Claude Shannon's information theory described digital signals (i.e., all-or-nothing signals). Alan Turing's theory of computation showed that any form of computation could be described digitally. The close relationship between these ideas suggested that it might be possible to construct an electronic brain.Examples of work in this vein includes robots such as W. Grey Walter's turtles and the Johns Hopkins Beast. These machines did not use computers, digital electronics or symbolic reasoning; they were controlled entirely by analog circuitry.Walter Pitts and Warren McCulloch analyzed networks of idealized artificial neurons and showed how they might perform simple logical functions in 1943. They were the first to describe what later researchers would call a neural network. One of the students inspired by Pitts and McCulloch was a young Marvin Minsky, then a 24-year-old graduate student. In 1951 (with Dean Edmonds) he built the first neural net machine, the SNARC.Minsky was to become one of the most important leaders and innovators in AI for the next 50 years.",289,"History of artificial intelligence
Cybernetics and early neural networks

The earliest research into thinking machines was inspired by a confluence of ideas that became prevalent in the late 1930s, 1940s, and early 1950s. Recent research in neurology had shown that the brain was an electrical network of neurons that fired in all-or-nothing pulses. Norbert Wiener's cybernetics described control and stability in electrical networks. Claude Shannon's information theory described digital signals (i.e., all-or-nothing signals). Alan Turing's theory of computation showed that any form of computation could be described digitally. The close relationship between these ideas suggested that it might be possible to construct an electronic brain.Examples of work in this vein includes robots such as W. Grey Walter's turtles and the Johns Hopkins Beast. These machines did not use computers, digital electronics or symbolic reasoning; they were controlled entirely by analog circuitry.Walter Pitts and Warren McCulloch analyzed networks of idealized artificial neurons and showed how they might perform simple logical functions in 1943. They were the first to describe what later researchers would call a neural network. One of the students inspired by Pitts and McCulloch was a young Marvin Minsky, then a 24-year-old graduate student. In 1951 (with Dean Edmonds) he built the first neural net machine, the SNARC.Minsky was to become one of the most important leaders and innovators in AI for the next 50 years.","1. What was the motivation behind early research into thinking machines?
2. What was the relationship between various ideas that were prevalent at the time?
3. What was the first neural network machine?","1. The motivation behind early research into thinking machines was the confluence of ideas that became prevalent in the late 1930s, 1940s, and early 1950s. These ideas included recent research in neurology, cybernetics, information theory, and computation theory.
2. The relationship between various ideas that were prevalent at the time was that they were all related to the idea of constructing an electronic brain. This idea was motivated by the belief that the brain was an electrical network of neurons.
3. The first neural network machine was the SNARC, which was built by Marvin Minsky and Dean Edmonds in 1951."
History of artificial intelligence,Turing's test,"In 1950 Alan Turing published a landmark paper in which he speculated about the possibility of creating machines that think.
He noted that ""thinking"" is difficult to define and devised his famous Turing Test. If a machine could carry on a conversation (over a teleprinter) that was indistinguishable from a conversation with a human being, then it was reasonable to say that the machine was ""thinking"". This simplified version of the problem allowed Turing to argue convincingly that a ""thinking machine"" was at least plausible and the paper answered all the most common objections to the proposition. The Turing Test was the first serious proposal in the philosophy of artificial intelligence.",131,"History of artificial intelligence
Turing's test

In 1950 Alan Turing published a landmark paper in which he speculated about the possibility of creating machines that think.
He noted that ""thinking"" is difficult to define and devised his famous Turing Test. If a machine could carry on a conversation (over a teleprinter) that was indistinguishable from a conversation with a human being, then it was reasonable to say that the machine was ""thinking"". This simplified version of the problem allowed Turing to argue convincingly that a ""thinking machine"" was at least plausible and the paper answered all the most common objections to the proposition. The Turing Test was the first serious proposal in the philosophy of artificial intelligence.","1. What is the Turing Test?
2. What did the Turing Test propose?
3. What was the Turing Test's impact on the philosophy of artificial intelligence?","1. The Turing Test is a test devised by Alan Turing in which a machine is asked a series of questions in order to determine if it is thinking.
2. The Turing Test proposed that if a machine could carry on a conversation that was indistinguishable from a conversation with a human being, then it was reasonable to say that the machine was ""thinking"".
3. The Turing Test had a significant impact on the philosophy of artificial intelligence, as it was the first serious proposal in this area."
History of artificial intelligence,Game AI,"In 1951, using the Ferranti Mark 1 machine of the University of Manchester, Christopher Strachey wrote a checkers program and Dietrich Prinz wrote one for chess. Arthur Samuel's checkers program, the subject of his 1959 paper ""Some Studies in Machine Learning Using the Game of Checkers"", eventually achieved sufficient skill to challenge a respectable amateur. Game AI would continue to be used as a measure of progress in AI throughout its history.",93,"History of artificial intelligence
Game AI

In 1951, using the Ferranti Mark 1 machine of the University of Manchester, Christopher Strachey wrote a checkers program and Dietrich Prinz wrote one for chess. Arthur Samuel's checkers program, the subject of his 1959 paper ""Some Studies in Machine Learning Using the Game of Checkers"", eventually achieved sufficient skill to challenge a respectable amateur. Game AI would continue to be used as a measure of progress in AI throughout its history.","1. What was the first AI game?
2. What was the first AI game to be able to challenge a respectable amateur?
3. What was the first AI game to be written?","1. The first AI game was checkers.
2. The first AI game to be able to challenge a respectable amateur was checkers.
3. The first AI game to be written was checkers."
History of artificial intelligence,Symbolic reasoning and the Logic Theorist,"When access to digital computers became possible in the middle fifties, a few scientists instinctively recognized that a machine that could manipulate numbers could also manipulate symbols and that the manipulation of symbols could well be the essence of human thought. This was a new approach to creating thinking machines.In 1955, Allen Newell and (future Nobel Laureate) Herbert A. Simon created the ""Logic Theorist"" (with help from J. C. Shaw). The program would eventually prove 38 of the first 52 theorems in Russell and Whitehead's Principia Mathematica, and find new and more elegant proofs for some.
Simon said that they had ""solved the venerable mind/body problem, explaining how a system composed of matter can have the properties of mind.""
(This was an early statement of the philosophical position John Searle would later call ""Strong AI"": that machines can contain minds just as human bodies do.)",197,"History of artificial intelligence
Symbolic reasoning and the Logic Theorist

When access to digital computers became possible in the middle fifties, a few scientists instinctively recognized that a machine that could manipulate numbers could also manipulate symbols and that the manipulation of symbols could well be the essence of human thought. This was a new approach to creating thinking machines.In 1955, Allen Newell and (future Nobel Laureate) Herbert A. Simon created the ""Logic Theorist"" (with help from J. C. Shaw). The program would eventually prove 38 of the first 52 theorems in Russell and Whitehead's Principia Mathematica, and find new and more elegant proofs for some.
Simon said that they had ""solved the venerable mind/body problem, explaining how a system composed of matter can have the properties of mind.""
(This was an early statement of the philosophical position John Searle would later call ""Strong AI"": that machines can contain minds just as human bodies do.)","1. What is the ""Logic Theorist""?
2. What did it do?
3. What was the reaction to it?","1. The ""Logic Theorist"" was a program that was created in 1955 by Allen Newell and Herbert A. Simon.
2. It proved 38 of the first 52 theorems in Russell and Whitehead's Principia Mathematica, and found new and more elegant proofs for some.
3. The reaction to it was that it solved the mind/body problem, explaining how a system composed of matter can have the properties of mind."
History of artificial intelligence,Dartmouth Workshop 1956: the birth of AI,"The Dartmouth Workshop of 1956
was organized by Marvin Minsky, John McCarthy and two senior scientists: Claude Shannon and Nathan Rochester of IBM，which is the eight-week Dartmouth Summer Research Program in Artificial Intelligence (DSRPAI), which marks the beginning of AI Spring.

The proposal for the conference included this assertion: ""every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it"".
The participants included Ray Solomonoff, Oliver Selfridge, Trenchard More, Arthur Samuel, Allen Newell and Herbert A. Simon, all of whom would create important programs during the first decades of AI research.
At the workshop Newell and Simon debuted the ""Logic Theorist"" and McCarthy persuaded the attendees to accept ""Artificial Intelligence"" as the name of the field.
The 1956 Dartmouth workshop was the moment that AI gained its name, its mission, its first success and its major players, and is widely considered the birth of AI. The term ""Artificial Intelligence"" was chosen by McCarthy to avoid associations with cybernetics and connections with the influential cyberneticist Norbert Wiener.",249,"History of artificial intelligence
Dartmouth Workshop 1956: the birth of AI

The Dartmouth Workshop of 1956
was organized by Marvin Minsky, John McCarthy and two senior scientists: Claude Shannon and Nathan Rochester of IBM，which is the eight-week Dartmouth Summer Research Program in Artificial Intelligence (DSRPAI), which marks the beginning of AI Spring.

The proposal for the conference included this assertion: ""every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it"".
The participants included Ray Solomonoff, Oliver Selfridge, Trenchard More, Arthur Samuel, Allen Newell and Herbert A. Simon, all of whom would create important programs during the first decades of AI research.
At the workshop Newell and Simon debuted the ""Logic Theorist"" and McCarthy persuaded the attendees to accept ""Artificial Intelligence"" as the name of the field.
The 1956 Dartmouth workshop was the moment that AI gained its name, its mission, its first success and its major players, and is widely considered the birth of AI. The term ""Artificial Intelligence"" was chosen by McCarthy to avoid associations with cybernetics and connections with the influential cyberneticist Norbert Wiener.","1. What was the Dartmouth Workshop of 1956?
2. What was the goal of the Dartmouth Workshop of 1956?
3. Who attended the Dartmouth Workshop of 1956?
4. What was the significance of the Dartmouth Workshop of 1956?","1. The Dartmouth Workshop of 1956 was an eight-week conference organized by Marvin Minsky, John McCarthy and two senior scientists: Claude Shannon and Nathan Rochester of IBM.
2. The goal of the Dartmouth Workshop of 1956 was to create a proposal for a conference that would explore the possibility of creating machines that could learn and think like humans.
3. The participants of the Dartmouth Workshop of 1956 included Ray Solomonoff, Oliver Selfridge, Trenchard More, Arthur Samuel, Allen Newell and Herbert A. Simon.
4. The significance of the Dartmouth Workshop of 1956 was that it was the moment that AI gained its name, its mission, its first success and its major players."
History of artificial intelligence,Symbolic AI (1956–1974),"The programs developed in the years after the Dartmouth Workshop were, to most people, simply ""astonishing"": computers were solving algebra word problems, proving theorems in geometry and learning to speak English. Few at the time would have believed that such ""intelligent"" behavior by machines was possible at all. Researchers expressed an intense optimism in private and in print, predicting that a fully intelligent machine would be built in less than 20 years. Government agencies like DARPA poured money into the new field.",103,"History of artificial intelligence
Symbolic AI (1956–1974)

The programs developed in the years after the Dartmouth Workshop were, to most people, simply ""astonishing"": computers were solving algebra word problems, proving theorems in geometry and learning to speak English. Few at the time would have believed that such ""intelligent"" behavior by machines was possible at all. Researchers expressed an intense optimism in private and in print, predicting that a fully intelligent machine would be built in less than 20 years. Government agencies like DARPA poured money into the new field.","1. What was the Dartmouth Workshop?
2. What was the goal of the Dartmouth Workshop?
3. What was the outcome of the Dartmouth Workshop?
4. What was the impact of the Dartmouth Workshop?","1. The Dartmouth Workshop was a conference that took place in 1956.
2. The goal of the Dartmouth Workshop was to create a program that could solve algebra word problems.
3. The outcome of the Dartmouth Workshop was that computers were able to solve algebra word problems.
4. The impact of the Dartmouth Workshop was that it showed that computers could be made to be intelligent."
History of artificial intelligence,Approaches,There were many successful programs and new directions in the late 50s and 1960s. Among the most influential were these:,27,"History of artificial intelligence
Approaches

There were many successful programs and new directions in the late 50s and 1960s. Among the most influential were these:","1. What were some successful programs in the late 50s and 1960s?
2. What were some of the most influential approaches during that time?","1. Some successful programs in the late 50s and 1960s were the Logic Theorist, the General Problem Solver, and the Checker Player.
2. Some of the most influential approaches during that time were the Logic Theorist, the General Problem Solver, and the Checker Player."
History of artificial intelligence,Reasoning as search,"Many early AI programs used the same basic algorithm. To achieve some goal (like winning a game or proving a theorem), they proceeded step by step towards it (by making a move or a deduction) as if searching through a maze, backtracking whenever they reached a dead end. This paradigm was called ""reasoning as search"".The principal difficulty was that, for many problems, the number of possible paths through the ""maze"" was simply astronomical (a situation known as a ""combinatorial explosion""). Researchers would reduce the search space by using heuristics or ""rules of thumb"" that would eliminate those paths that were unlikely to lead to a solution.Newell and Simon tried to capture a general version of this algorithm in a program called the ""General Problem Solver"". Other ""searching"" programs were able to accomplish impressive tasks like solving problems in geometry and algebra, such as Herbert Gelernter's Geometry Theorem Prover (1958) and SAINT, written by Minsky's student James Slagle (1961). Other programs searched through goals and subgoals to plan actions, like the STRIPS system developed at Stanford to control the behavior of their robot Shakey.",247,"History of artificial intelligence
Reasoning as search

Many early AI programs used the same basic algorithm. To achieve some goal (like winning a game or proving a theorem), they proceeded step by step towards it (by making a move or a deduction) as if searching through a maze, backtracking whenever they reached a dead end. This paradigm was called ""reasoning as search"".The principal difficulty was that, for many problems, the number of possible paths through the ""maze"" was simply astronomical (a situation known as a ""combinatorial explosion""). Researchers would reduce the search space by using heuristics or ""rules of thumb"" that would eliminate those paths that were unlikely to lead to a solution.Newell and Simon tried to capture a general version of this algorithm in a program called the ""General Problem Solver"". Other ""searching"" programs were able to accomplish impressive tasks like solving problems in geometry and algebra, such as Herbert Gelernter's Geometry Theorem Prover (1958) and SAINT, written by Minsky's student James Slagle (1961). Other programs searched through goals and subgoals to plan actions, like the STRIPS system developed at Stanford to control the behavior of their robot Shakey.","1. What is the ""General Problem Solver""?
2. What was the ""STRIPS system""?","1. The ""General Problem Solver"" is a program that uses a basic algorithm to achieve goals by proceeding step by step towards them.
2. The ""STRIPS system"" is a system that controls the behavior of robots using a program that searches through goals and subgoals."
History of artificial intelligence,Natural language,"An important goal of AI research is to allow computers to communicate in natural languages like English. An early success was Daniel Bobrow's program STUDENT, which could solve high school algebra word problems.A semantic net represents concepts (e.g. ""house"",""door"") as nodes and relations among concepts (e.g. ""has-a"") as links between the nodes. The first AI program to use a semantic net was written by Ross Quillian and the most successful (and controversial) version was Roger Schank's Conceptual dependency theory.Joseph Weizenbaum's ELIZA could carry out conversations that were so realistic that users occasionally were fooled into thinking they were communicating with a human being and not a program (See ELIZA effect). But in fact, ELIZA had no idea what she was talking about. She simply gave a canned response or repeated back what was said to her, rephrasing her response with a few grammar rules. ELIZA was the first chatterbot.",206,"History of artificial intelligence
Natural language

An important goal of AI research is to allow computers to communicate in natural languages like English. An early success was Daniel Bobrow's program STUDENT, which could solve high school algebra word problems.A semantic net represents concepts (e.g. ""house"",""door"") as nodes and relations among concepts (e.g. ""has-a"") as links between the nodes. The first AI program to use a semantic net was written by Ross Quillian and the most successful (and controversial) version was Roger Schank's Conceptual dependency theory.Joseph Weizenbaum's ELIZA could carry out conversations that were so realistic that users occasionally were fooled into thinking they were communicating with a human being and not a program (See ELIZA effect). But in fact, ELIZA had no idea what she was talking about. She simply gave a canned response or repeated back what was said to her, rephrasing her response with a few grammar rules. ELIZA was the first chatterbot.","1. What was the first AI program to use a semantic net?
2. What was the most successful version of Roger Schank's Conceptual dependency theory?
3. What was Joseph Weizenbaum's ELIZA?
4. What was the ELIZA effect?","1. The first AI program to use a semantic net was Ross Quillian's program.
2. The most successful version of Roger Schank's Conceptual dependency theory was the version written by Joseph Weizenbaum.
3. Joseph Weizenbaum's ELIZA was a chatterbot that could carry out realistic conversations with users.
4. The ELIZA effect was the phenomenon where users were fooled into thinking they were communicating with a human being and not a program."
History of artificial intelligence,Micro-worlds,"In the late 60s, Marvin Minsky and Seymour Papert of the MIT AI Laboratory proposed that AI research should focus on artificially simple situations known as micro-worlds. They pointed out that in successful sciences like physics, basic principles were often best understood using simplified models like frictionless planes or perfectly rigid bodies. Much of the research focused on a ""blocks world,"" which consists of colored blocks of various shapes and sizes arrayed on a flat surface.This paradigm led to innovative work in machine vision by Gerald Sussman (who led the team), Adolfo Guzman, David Waltz (who invented ""constraint propagation""), and especially Patrick Winston. At the same time, Minsky and Papert built a robot arm that could stack blocks, bringing the blocks world to life. The crowning achievement of the micro-world program was Terry Winograd's SHRDLU. It could communicate in ordinary English sentences, plan operations and execute them.",197,"History of artificial intelligence
Micro-worlds

In the late 60s, Marvin Minsky and Seymour Papert of the MIT AI Laboratory proposed that AI research should focus on artificially simple situations known as micro-worlds. They pointed out that in successful sciences like physics, basic principles were often best understood using simplified models like frictionless planes or perfectly rigid bodies. Much of the research focused on a ""blocks world,"" which consists of colored blocks of various shapes and sizes arrayed on a flat surface.This paradigm led to innovative work in machine vision by Gerald Sussman (who led the team), Adolfo Guzman, David Waltz (who invented ""constraint propagation""), and especially Patrick Winston. At the same time, Minsky and Papert built a robot arm that could stack blocks, bringing the blocks world to life. The crowning achievement of the micro-world program was Terry Winograd's SHRDLU. It could communicate in ordinary English sentences, plan operations and execute them.","1. What is the significance of micro-worlds in the history of artificial intelligence?
2. What was the aim of the micro-worlds program?
3. What was the most significant achievement of the micro-worlds program?","1. Micro-worlds are significant in the history of artificial intelligence because they allowed for the development of machine vision and natural language processing.
2. The aim of the micro-worlds program was to create simplified models of reality that could be used to better understand basic principles.
3. The most significant achievement of the micro-worlds program was the development of SHRDLU, which could communicate in ordinary English sentences and plan and execute operations."
History of artificial intelligence,Automata,"In Japan, Waseda University initiated the WABOT project in 1967, and in 1972 completed the WABOT-1, the world's first full-scale ""intelligent"" humanoid robot, or android. Its limb control system allowed it to walk with the lower limbs, and to grip and transport objects with hands, using tactile sensors. Its vision system allowed it to measure distances and directions to objects using external receptors, artificial eyes and ears. And its conversation system allowed it to communicate with a person in Japanese, with an artificial mouth.",113,"History of artificial intelligence
Automata

In Japan, Waseda University initiated the WABOT project in 1967, and in 1972 completed the WABOT-1, the world's first full-scale ""intelligent"" humanoid robot, or android. Its limb control system allowed it to walk with the lower limbs, and to grip and transport objects with hands, using tactile sensors. Its vision system allowed it to measure distances and directions to objects using external receptors, artificial eyes and ears. And its conversation system allowed it to communicate with a person in Japanese, with an artificial mouth.","1. What was the WABOT project?
2. What was the WABOT-1?
3. What did the WABOT-1 do?","1. The WABOT project was a project initiated by Waseda University in 1967.
2. The WABOT-1 was the world's first full-scale ""intelligent"" humanoid robot, or android.
3. The WABOT-1 was able to walk with the lower limbs, grip and transport objects with hands, and communicate with a person in Japanese."
History of artificial intelligence,Optimism,"The first generation of AI researchers made these predictions about their work:

1958, H. A. Simon and Allen Newell: ""within ten years a digital computer will be the world's chess champion"" and ""within ten years a digital computer will discover and prove an important new mathematical theorem.""
1965, H. A. Simon: ""machines will be capable, within twenty years, of doing any work a man can do.""
1967, Marvin Minsky: ""Within a generation ... the problem of creating 'artificial intelligence' will substantially be solved.""
1970, Marvin Minsky (in Life Magazine): ""In from three to eight years we will have a machine with the general intelligence of an average human being.""",153,"History of artificial intelligence
Optimism

The first generation of AI researchers made these predictions about their work:

1958, H. A. Simon and Allen Newell: ""within ten years a digital computer will be the world's chess champion"" and ""within ten years a digital computer will discover and prove an important new mathematical theorem.""
1965, H. A. Simon: ""machines will be capable, within twenty years, of doing any work a man can do.""
1967, Marvin Minsky: ""Within a generation ... the problem of creating 'artificial intelligence' will substantially be solved.""
1970, Marvin Minsky (in Life Magazine): ""In from three to eight years we will have a machine with the general intelligence of an average human being.""","1. What did the first generation of AI researchers predict about their work?
2. What did Marvin Minsky say about AI in 1967?
3. What did Marvin Minsky say in 1970?","1. The first generation of AI researchers predicted that their work would result in digital computers that were capable of doing the same things as human beings.
2. In 1967, Marvin Minsky said that within a generation, the problem of creating 'artificial intelligence' would be solved.
3. In 1970, Marvin Minsky said that within three to eight years, we would have a machine with the general intelligence of an average human being."
History of artificial intelligence,Financing,"In June 1963, MIT received a $2.2 million grant from the newly created Advanced Research Projects Agency (later known as DARPA). The money was used to fund project MAC which subsumed the ""AI Group"" founded by Minsky and McCarthy five years earlier. DARPA continued to provide three million dollars a year until the 70s.DARPA made similar grants to Newell and Simon's program at CMU and to the Stanford AI Project (founded by John McCarthy in 1963). Another important AI laboratory was established at Edinburgh University by Donald Michie in 1965.
These four institutions would continue to be the main centers of AI research (and funding) in academia for many years.The money was proffered with few strings attached: J. C. R. Licklider, then the director of ARPA, believed that his organization should ""fund people, not projects!"" and allowed researchers to pursue whatever directions might interest them. This created a freewheeling atmosphere at MIT that gave birth to the hacker culture, but this ""hands off"" approach would not last.",223,"History of artificial intelligence
Financing

In June 1963, MIT received a $2.2 million grant from the newly created Advanced Research Projects Agency (later known as DARPA). The money was used to fund project MAC which subsumed the ""AI Group"" founded by Minsky and McCarthy five years earlier. DARPA continued to provide three million dollars a year until the 70s.DARPA made similar grants to Newell and Simon's program at CMU and to the Stanford AI Project (founded by John McCarthy in 1963). Another important AI laboratory was established at Edinburgh University by Donald Michie in 1965.
These four institutions would continue to be the main centers of AI research (and funding) in academia for many years.The money was proffered with few strings attached: J. C. R. Licklider, then the director of ARPA, believed that his organization should ""fund people, not projects!"" and allowed researchers to pursue whatever directions might interest them. This created a freewheeling atmosphere at MIT that gave birth to the hacker culture, but this ""hands off"" approach would not last.","1. What was the Advanced Research Projects Agency (DARPA)?
2. What was the purpose of the grant given to MIT in 1963?
3. What was the ""AI Group"" founded by Minsky and McCarthy?
4. What was the Stanford AI Project?
5. What was the Edinburgh University AI Laboratory?","1. The Advanced Research Projects Agency (DARPA) was a newly created agency in the United States Department of Defense.
2. The grant given to MIT in 1963 was for the purpose of funding the Project MAC, which subsumed the ""AI Group"" founded by Minsky and McCarthy.
3. The ""AI Group"" was founded by Marvin Minsky and John McCarthy in 1958.
4. The Stanford AI Project was founded by John McCarthy in 1963.
5. The Edinburgh University AI Laboratory was founded by Donald Michie in 1965."
History of artificial intelligence,First AI winter (1974–1980),"In the 1970s, AI was subject to critiques and financial setbacks. AI researchers had failed to appreciate the difficulty of the problems they faced. Their tremendous optimism had raised expectations impossibly high, and when the promised results failed to materialize, funding for AI disappeared. At the same time, the field of connectionism (or neural nets) was shut down almost completely for 10 years by Marvin Minsky's devastating criticism of perceptrons.
Despite the difficulties with public perception of AI in the late 70s, new ideas were explored in logic programming, commonsense reasoning and many other areas.",123,"History of artificial intelligence
First AI winter (1974–1980)

In the 1970s, AI was subject to critiques and financial setbacks. AI researchers had failed to appreciate the difficulty of the problems they faced. Their tremendous optimism had raised expectations impossibly high, and when the promised results failed to materialize, funding for AI disappeared. At the same time, the field of connectionism (or neural nets) was shut down almost completely for 10 years by Marvin Minsky's devastating criticism of perceptrons.
Despite the difficulties with public perception of AI in the late 70s, new ideas were explored in logic programming, commonsense reasoning and many other areas.","1. What was the first AI winter?
2. What caused the first AI winter?
3. What were some of the new ideas explored in AI during the first AI winter?","1. The first AI winter was from 1974 to 1980.
2. The first AI winter was caused by researchers' failure to appreciate the difficulty of the problems they faced and their tremendous optimism which raised expectations impossibly high.
3. Some of the new ideas explored in AI during the first AI winter included logic programming, commonsense reasoning, and many others."
History of artificial intelligence,Problems,"In the early seventies, the capabilities of AI programs were limited. Even the most impressive could only handle trivial versions of the problems they were supposed to solve; all the programs were, in some sense, ""toys"". AI researchers had begun to run into several fundamental limits that could not be overcome in the 1970s. Although some of these limits would be conquered in later decades, others still stymie the field to this day.
Limited computer power: There was not enough memory or processing speed to accomplish anything truly useful. For example, Ross Quillian's successful work on natural language was demonstrated with a vocabulary of only twenty words, because that was all that would fit in memory. Hans Moravec argued in 1976 that computers were still millions of times too weak to exhibit intelligence. He suggested an analogy: artificial intelligence requires computer power in the same way that aircraft require horsepower. Below a certain threshold, it's impossible, but, as power increases, eventually it could become easy. With regard to computer vision, Moravec estimated that simply matching the edge and motion detection capabilities of human retina in real time would require a general-purpose computer capable of 109 operations/second (1000 MIPS). As of 2011, practical computer vision applications require 10,000 to 1,000,000 MIPS. By comparison, the fastest supercomputer in 1976, Cray-1 (retailing at $5 million to $8 million), was only capable of around 80 to 130 MIPS, and a typical desktop computer at the time achieved less than 1 MIPS.
Intractability and the combinatorial explosion. In 1972 Richard Karp (building on Stephen Cook's 1971 theorem) showed there are many problems that can probably only be solved in exponential time (in the size of the inputs). Finding optimal solutions to these problems requires unimaginable amounts of computer time except when the problems are trivial. This almost certainly meant that many of the ""toy"" solutions used by AI would probably never scale up into useful systems.
Commonsense knowledge and reasoning. Many important artificial intelligence applications like vision or natural language require simply enormous amounts of information about the world: the program needs to have some idea of what it might be looking at or what it is talking about. This requires that the program know most of the same things about the world that a child does. Researchers soon discovered that this was a truly vast amount of information. No one in 1970 could build a database so large and no one knew how a program might learn so much information.
Moravec's paradox: Proving theorems and solving geometry problems is comparatively easy for computers, but a supposedly simple task like recognizing a face or crossing a room without bumping into anything is extremely difficult. This helps explain why research into vision and robotics had made so little progress by the middle 1970s.
The frame and qualification problems. AI researchers (like John McCarthy) who used logic discovered that they could not represent ordinary deductions that involved planning or default reasoning without making changes to the structure of logic itself. They developed new logics (like non-monotonic logics and modal logics) to try to solve the problems.",644,"History of artificial intelligence
Problems

In the early seventies, the capabilities of AI programs were limited. Even the most impressive could only handle trivial versions of the problems they were supposed to solve; all the programs were, in some sense, ""toys"". AI researchers had begun to run into several fundamental limits that could not be overcome in the 1970s. Although some of these limits would be conquered in later decades, others still stymie the field to this day.
Limited computer power: There was not enough memory or processing speed to accomplish anything truly useful. For example, Ross Quillian's successful work on natural language was demonstrated with a vocabulary of only twenty words, because that was all that would fit in memory. Hans Moravec argued in 1976 that computers were still millions of times too weak to exhibit intelligence. He suggested an analogy: artificial intelligence requires computer power in the same way that aircraft require horsepower. Below a certain threshold, it's impossible, but, as power increases, eventually it could become easy. With regard to computer vision, Moravec estimated that simply matching the edge and motion detection capabilities of human retina in real time would require a general-purpose computer capable of 109 operations/second (1000 MIPS). As of 2011, practical computer vision applications require 10,000 to 1,000,000 MIPS. By comparison, the fastest supercomputer in 1976, Cray-1 (retailing at $5 million to $8 million), was only capable of around 80 to 130 MIPS, and a typical desktop computer at the time achieved less than 1 MIPS.
Intractability and the combinatorial explosion. In 1972 Richard Karp (building on Stephen Cook's 1971 theorem) showed there are many problems that can probably only be solved in exponential time (in the size of the inputs). Finding optimal solutions to these problems requires unimaginable amounts of computer time except when the problems are trivial. This almost certainly meant that many of the ""toy"" solutions used by AI would probably never scale up into useful systems.
Commonsense knowledge and reasoning. Many important artificial intelligence applications like vision or natural language require simply enormous amounts of information about the world: the program needs to have some idea of what it might be looking at or what it is talking about. This requires that the program know most of the same things about the world that a child does. Researchers soon discovered that this was a truly vast amount of information. No one in 1970 could build a database so large and no one knew how a program might learn so much information.
Moravec's paradox: Proving theorems and solving geometry problems is comparatively easy for computers, but a supposedly simple task like recognizing a face or crossing a room without bumping into anything is extremely difficult. This helps explain why research into vision and robotics had made so little progress by the middle 1970s.
The frame and qualification problems. AI researchers (like John McCarthy) who used logic discovered that they could not represent ordinary deductions that involved planning or default reasoning without making changes to the structure of logic itself. They developed new logics (like non-monotonic logics and modal logics) to try to solve the problems.","1. What are some of the fundamental limits that have hampered AI development in the past?
2. How did Richard Karp's work in 1972 impact the field of AI?
3. What is Moravec's paradox?
4. What is the frame problem?","1. Fundamental limits that have hampered AI development in the past include limited computer power, intractability and the combinatorial explosion, commonsense knowledge and reasoning, and the frame problem.
2. Richard Karp's work in 1972 showed that there are many problems that can only be solved in exponential time, which greatly limited the potential for AI development at the time.
3. Moravec's paradox is the observation that tasks that are comparatively easy for computers, like proving theorems or solving geometry problems, are extremely difficult for humans, while tasks that are difficult for humans, like recognizing a face or crossing a room without bumping into anything, are comparatively easy for computers.
4. The frame problem is the difficulty of representing knowledge in a way that a computer can use it."
History of artificial intelligence,End of funding,"The agencies which funded AI research (such as the British government, DARPA and NRC) became frustrated with the lack of progress and eventually cut off almost all funding for undirected research into AI. The pattern began as early as 1966 when the ALPAC report appeared criticizing machine translation efforts. After spending 20 million dollars, the NRC ended all support.
In 1973, the Lighthill report on the state of AI research in the UK criticized the utter failure of AI to achieve its ""grandiose objectives"" and led to the dismantling of AI research in that country.
(The report specifically mentioned the combinatorial explosion problem as a reason for AI's failings.)DARPA was deeply disappointed with researchers working on the Speech Understanding Research program at CMU and canceled an annual grant of three million dollars.
By 1974, funding for AI projects was hard to find.
Hans Moravec blamed the crisis on the unrealistic predictions of his colleagues. ""Many researchers were caught up in a web of increasing exaggeration.""
However, there was another issue: since the passage of the Mansfield Amendment in 1969, DARPA had been under increasing pressure to fund ""mission-oriented direct research, rather than basic undirected research"". Funding for the creative, freewheeling exploration that had gone on in the 60s would not come from DARPA. Instead, the money was directed at specific projects with clear objectives, such as autonomous tanks and battle management systems.",302,"History of artificial intelligence
End of funding

The agencies which funded AI research (such as the British government, DARPA and NRC) became frustrated with the lack of progress and eventually cut off almost all funding for undirected research into AI. The pattern began as early as 1966 when the ALPAC report appeared criticizing machine translation efforts. After spending 20 million dollars, the NRC ended all support.
In 1973, the Lighthill report on the state of AI research in the UK criticized the utter failure of AI to achieve its ""grandiose objectives"" and led to the dismantling of AI research in that country.
(The report specifically mentioned the combinatorial explosion problem as a reason for AI's failings.)DARPA was deeply disappointed with researchers working on the Speech Understanding Research program at CMU and canceled an annual grant of three million dollars.
By 1974, funding for AI projects was hard to find.
Hans Moravec blamed the crisis on the unrealistic predictions of his colleagues. ""Many researchers were caught up in a web of increasing exaggeration.""
However, there was another issue: since the passage of the Mansfield Amendment in 1969, DARPA had been under increasing pressure to fund ""mission-oriented direct research, rather than basic undirected research"". Funding for the creative, freewheeling exploration that had gone on in the 60s would not come from DARPA. Instead, the money was directed at specific projects with clear objectives, such as autonomous tanks and battle management systems.","1. What was the ALPAC report?
2. What did the Lighthill report say about AI research in the UK?
3. What did DARPA do in response to the Lighthill report?
4. What was the Mansfield Amendment?","1. The ALPAC report was a report that criticized machine translation efforts.
2. The Lighthill report said that AI research in the UK had failed and led to the dismantling of AI research in that country.
3. DARPA responded to the Lighthill report by cancelling an annual grant of three million dollars.
4. The Mansfield Amendment was a 1969 amendment to the National Defense Authorization Act that directed DARPA to fund ""mission-oriented direct research, rather than basic undirected research""."
History of artificial intelligence,Critiques from across campus,"Several philosophers had strong objections to the claims being made by AI researchers. One of the earliest was John Lucas, who argued that Gödel's incompleteness theorem showed that a formal system (such as a computer program) could never see the truth of certain statements, while a human being could. Hubert Dreyfus ridiculed the broken promises of the 1960s and critiqued the assumptions of AI, arguing that human reasoning actually involved very little ""symbol processing"" and a great deal of embodied, instinctive, unconscious ""know how"". John Searle's Chinese Room argument, presented in 1980, attempted to show that a program could not be said to ""understand"" the symbols that it uses (a quality called ""intentionality""). If the symbols have no meaning for the machine, Searle argued, then the machine can not be described as ""thinking"".These critiques were not taken seriously by AI researchers, often because they seemed so far off the point. Problems like intractability and commonsense knowledge seemed much more immediate and serious. It was unclear what difference ""know how"" or ""intentionality"" made to an actual computer program. Minsky said of Dreyfus and Searle ""they misunderstand, and should be ignored."" Dreyfus, who taught at MIT, was given a cold shoulder: he later said that AI researchers ""dared not be seen having lunch with me."" Joseph Weizenbaum, the author of ELIZA, felt his colleagues' treatment of Dreyfus was unprofessional and childish. Although he was an outspoken critic of Dreyfus' positions, he ""deliberately made it plain that theirs was not the way to treat a human being.""Weizenbaum began to have serious ethical doubts about AI when Kenneth Colby wrote a ""computer program which can conduct psychotherapeutic dialogue"" based on ELIZA. Weizenbaum was disturbed that Colby saw a mindless program as a serious therapeutic tool. A feud began, and the situation was not helped when Colby did not credit Weizenbaum for his contribution to the program. In 1976, Weizenbaum published Computer Power and Human Reason which argued that the misuse of artificial intelligence has the potential to devalue human life.",460,"History of artificial intelligence
Critiques from across campus

Several philosophers had strong objections to the claims being made by AI researchers. One of the earliest was John Lucas, who argued that Gödel's incompleteness theorem showed that a formal system (such as a computer program) could never see the truth of certain statements, while a human being could. Hubert Dreyfus ridiculed the broken promises of the 1960s and critiqued the assumptions of AI, arguing that human reasoning actually involved very little ""symbol processing"" and a great deal of embodied, instinctive, unconscious ""know how"". John Searle's Chinese Room argument, presented in 1980, attempted to show that a program could not be said to ""understand"" the symbols that it uses (a quality called ""intentionality""). If the symbols have no meaning for the machine, Searle argued, then the machine can not be described as ""thinking"".These critiques were not taken seriously by AI researchers, often because they seemed so far off the point. Problems like intractability and commonsense knowledge seemed much more immediate and serious. It was unclear what difference ""know how"" or ""intentionality"" made to an actual computer program. Minsky said of Dreyfus and Searle ""they misunderstand, and should be ignored."" Dreyfus, who taught at MIT, was given a cold shoulder: he later said that AI researchers ""dared not be seen having lunch with me."" Joseph Weizenbaum, the author of ELIZA, felt his colleagues' treatment of Dreyfus was unprofessional and childish. Although he was an outspoken critic of Dreyfus' positions, he ""deliberately made it plain that theirs was not the way to treat a human being.""Weizenbaum began to have serious ethical doubts about AI when Kenneth Colby wrote a ""computer program which can conduct psychotherapeutic dialogue"" based on ELIZA. Weizenbaum was disturbed that Colby saw a mindless program as a serious therapeutic tool. A feud began, and the situation was not helped when Colby did not credit Weizenbaum for his contribution to the program. In 1976, Weizenbaum published Computer Power and Human Reason which argued that the misuse of artificial intelligence has the potential to devalue human life.","1. What objections did philosophers have to AI research?
2. What was the Chinese Room argument?
3. What did Joseph Weizenbaum argue about AI?","1. Philosophers had objections to AI research based on the assumption that machines cannot think or understand symbols in the same way humans can.
2. The Chinese Room argument was an argument against the claim that machines can understand symbols.
3. Joseph Weizenbaum argued that the misuse of artificial intelligence has the potential to devalue human life."
History of artificial intelligence,Perceptrons and the attack on connectionism,"A perceptron was a form of neural network introduced in 1958 by Frank Rosenblatt, who had been a schoolmate of Marvin Minsky at the Bronx High School of Science. Like most AI researchers, he was optimistic about their power, predicting that ""perceptron may eventually be able to learn, make decisions, and translate languages."" An active research program into the paradigm was carried out throughout the 1960s but came to a sudden halt with the publication of Minsky and Papert's 1969 book Perceptrons. It suggested that there were severe limitations to what perceptrons could do and that Frank Rosenblatt's predictions had been grossly exaggerated. The effect of the book was devastating: virtually no research at all was done in connectionism for 10 years. Eventually, a new generation of researchers would revive the field and thereafter it would become a vital and useful part of artificial intelligence. Rosenblatt would not live to see this, as he died in a boating accident shortly after the book was published.",209,"History of artificial intelligence
Perceptrons and the attack on connectionism

A perceptron was a form of neural network introduced in 1958 by Frank Rosenblatt, who had been a schoolmate of Marvin Minsky at the Bronx High School of Science. Like most AI researchers, he was optimistic about their power, predicting that ""perceptron may eventually be able to learn, make decisions, and translate languages."" An active research program into the paradigm was carried out throughout the 1960s but came to a sudden halt with the publication of Minsky and Papert's 1969 book Perceptrons. It suggested that there were severe limitations to what perceptrons could do and that Frank Rosenblatt's predictions had been grossly exaggerated. The effect of the book was devastating: virtually no research at all was done in connectionism for 10 years. Eventually, a new generation of researchers would revive the field and thereafter it would become a vital and useful part of artificial intelligence. Rosenblatt would not live to see this, as he died in a boating accident shortly after the book was published.","1. What was the perceptron?
2. What did the book Perceptrons say about perceptrons?
3. What was the effect of the book Perceptrons?","1. The perceptron was a form of neural network introduced in 1958 by Frank Rosenblatt.
2. The book Perceptrons said that there were severe limitations to what perceptrons could do.
3. The effect of the book Perceptrons was devastating: virtually no research at all was done in connectionism for 10 years."
History of artificial intelligence,"Logic at Stanford, CMU and Edinburgh","Logic was introduced into AI research as early as 1959, by John McCarthy in his Advice Taker proposal.
In 1963, J. Alan Robinson had discovered a simple method to implement deduction on computers, the resolution and unification algorithm. However, straightforward implementations, like those attempted by McCarthy and his students in the late 1960s, were especially intractable: the programs required astronomical numbers of steps to prove simple theorems. A more fruitful approach to logic was developed in the 1970s by Robert Kowalski at the University of Edinburgh, and soon this led to the collaboration with French researchers Alain Colmerauer and Philippe Roussel who created the successful logic programming language Prolog.
Prolog uses a subset of logic (Horn clauses, closely related to ""rules"" and ""production rules"") that permit tractable computation. Rules would continue to be influential, providing a foundation for Edward Feigenbaum's expert systems and the continuing work by Allen Newell and Herbert A. Simon that would lead to Soar and their unified theories of cognition.Critics of the logical approach noted, as Dreyfus had, that human beings rarely used logic when they solved problems. Experiments by psychologists like Peter Wason, Eleanor Rosch, Amos Tversky, Daniel Kahneman and others provided proof.
McCarthy responded that what people do is irrelevant. He argued that what is really needed are machines that can solve problems—not machines that think as people do.",308,"History of artificial intelligence
Logic at Stanford, CMU and Edinburgh

Logic was introduced into AI research as early as 1959, by John McCarthy in his Advice Taker proposal.
In 1963, J. Alan Robinson had discovered a simple method to implement deduction on computers, the resolution and unification algorithm. However, straightforward implementations, like those attempted by McCarthy and his students in the late 1960s, were especially intractable: the programs required astronomical numbers of steps to prove simple theorems. A more fruitful approach to logic was developed in the 1970s by Robert Kowalski at the University of Edinburgh, and soon this led to the collaboration with French researchers Alain Colmerauer and Philippe Roussel who created the successful logic programming language Prolog.
Prolog uses a subset of logic (Horn clauses, closely related to ""rules"" and ""production rules"") that permit tractable computation. Rules would continue to be influential, providing a foundation for Edward Feigenbaum's expert systems and the continuing work by Allen Newell and Herbert A. Simon that would lead to Soar and their unified theories of cognition.Critics of the logical approach noted, as Dreyfus had, that human beings rarely used logic when they solved problems. Experiments by psychologists like Peter Wason, Eleanor Rosch, Amos Tversky, Daniel Kahneman and others provided proof.
McCarthy responded that what people do is irrelevant. He argued that what is really needed are machines that can solve problems—not machines that think as people do.","1. What was John McCarthy's proposal in 1959?
2. What was the resolution and unification algorithm?
3. What was the approach to logic developed in the 1970s by Robert Kowalski?
4. What is Prolog?
5. What did the experiments by psychologists like Peter Wason, Eleanor Rosch, Amos Tversky, Daniel Kahneman and others provide proof of?","1. John McCarthy's proposal in 1959 was the Advice Taker proposal.
2. The resolution and unification algorithm is a method to implement deduction on computers.
3. The approach to logic developed in the 1970s by Robert Kowalski was the Prolog approach.
4. Prolog is a logic programming language that uses a subset of logic that permits tractable computation.
5. The experiments by psychologists like Peter Wason, Eleanor Rosch, Amos Tversky, Daniel Kahneman and others provided proof that human beings rarely use logic when they solve problems."
History of artificial intelligence,"MIT's ""anti-logic"" approach","Among the critics of McCarthy's approach were his colleagues across the country at MIT. Marvin Minsky, Seymour Papert and Roger Schank were trying to solve problems like ""story understanding"" and ""object recognition"" that required a machine to think like a person. In order to use ordinary concepts like ""chair"" or ""restaurant"" they had to make all the same illogical assumptions that people normally made. Unfortunately, imprecise concepts like these are hard to represent in logic. Gerald Sussman observed that ""using precise language to describe essentially imprecise concepts doesn't make them any more precise."" Schank described their ""anti-logic"" approaches as ""scruffy"", as opposed to the ""neat"" paradigms used by McCarthy, Kowalski, Feigenbaum, Newell and Simon.In 1975, in a seminal paper, Minsky noted that many of his fellow researchers were using the same kind of tool: a framework that captures all our common sense assumptions about something. For example, if we use the concept of a bird, there is a constellation of facts that immediately come to mind: we might assume that it flies, eats worms and so on. We know these facts are not always true and that deductions using these facts will not be ""logical"", but these structured sets of assumptions are part of the context of everything we say and think. He called these structures ""frames"". Schank used a version of frames he called ""scripts"" to successfully answer questions about short stories in English.",320,"History of artificial intelligence
MIT's ""anti-logic"" approach

Among the critics of McCarthy's approach were his colleagues across the country at MIT. Marvin Minsky, Seymour Papert and Roger Schank were trying to solve problems like ""story understanding"" and ""object recognition"" that required a machine to think like a person. In order to use ordinary concepts like ""chair"" or ""restaurant"" they had to make all the same illogical assumptions that people normally made. Unfortunately, imprecise concepts like these are hard to represent in logic. Gerald Sussman observed that ""using precise language to describe essentially imprecise concepts doesn't make them any more precise."" Schank described their ""anti-logic"" approaches as ""scruffy"", as opposed to the ""neat"" paradigms used by McCarthy, Kowalski, Feigenbaum, Newell and Simon.In 1975, in a seminal paper, Minsky noted that many of his fellow researchers were using the same kind of tool: a framework that captures all our common sense assumptions about something. For example, if we use the concept of a bird, there is a constellation of facts that immediately come to mind: we might assume that it flies, eats worms and so on. We know these facts are not always true and that deductions using these facts will not be ""logical"", but these structured sets of assumptions are part of the context of everything we say and think. He called these structures ""frames"". Schank used a version of frames he called ""scripts"" to successfully answer questions about short stories in English.","1. What is the ""anti-logic"" approach?
2. What did Minsky and Schank do differently from McCarthy?
3. What is a frame?","1. The ""anti-logic"" approach is an approach to artificial intelligence that does not rely on logic.
2. Minsky and Schank did not rely on logic, instead using structures called ""frames"" and ""scripts"".
3. A frame is a structure that captures all our common sense assumptions about something."
History of artificial intelligence,Boom (1980–1987),"In the 1980s a form of AI program called ""expert systems"" was adopted by corporations around the world and knowledge became the focus of mainstream AI research. In those same years, the Japanese government aggressively funded AI with its fifth generation computer project. Another encouraging event in the early 1980s was the revival of connectionism in the work of John Hopfield and David Rumelhart. Once again, AI had achieved success.",88,"History of artificial intelligence
Boom (1980–1987)

In the 1980s a form of AI program called ""expert systems"" was adopted by corporations around the world and knowledge became the focus of mainstream AI research. In those same years, the Japanese government aggressively funded AI with its fifth generation computer project. Another encouraging event in the early 1980s was the revival of connectionism in the work of John Hopfield and David Rumelhart. Once again, AI had achieved success.","1. What was the focus of mainstream AI research in the 1980s?
2. What was the Japanese government's role in AI research in the 1980s?
3. What was the significance of the revival of connectionism in the early 1980s?","1. The focus of mainstream AI research in the 1980s was on expert systems.
2. The Japanese government played a significant role in AI research in the 1980s by funding the fifth generation computer project.
3. The revival of connectionism in the early 1980s was significant because it demonstrated that AI had achieved success once again."
History of artificial intelligence,Rise of expert systems,"An expert system is a program that answers questions or solves problems about a specific domain of knowledge, using logical rules that are derived from the knowledge of experts. The earliest examples were developed by Edward Feigenbaum and his students. Dendral, begun in 1965, identified compounds from spectrometer readings. MYCIN, developed in 1972, diagnosed infectious blood diseases. They demonstrated the feasibility of the approach.Expert systems restricted themselves to a small domain of specific knowledge (thus avoiding the commonsense knowledge problem) and their simple design made it relatively easy for programs to be built and then modified once they were in place. All in all, the programs proved to be useful: something that AI had not been able to achieve up to this point.In 1980, an expert system called XCON was completed at CMU for the Digital Equipment Corporation. It was an enormous success: it was saving the company 40 million dollars annually by 1986. Corporations around the world began to develop and deploy expert systems and by 1985 they were spending over a billion dollars on AI, most of it to in-house AI departments. An industry grew up to support them, including hardware companies like Symbolics and Lisp Machines and software companies such as IntelliCorp and Aion.",257,"History of artificial intelligence
Rise of expert systems

An expert system is a program that answers questions or solves problems about a specific domain of knowledge, using logical rules that are derived from the knowledge of experts. The earliest examples were developed by Edward Feigenbaum and his students. Dendral, begun in 1965, identified compounds from spectrometer readings. MYCIN, developed in 1972, diagnosed infectious blood diseases. They demonstrated the feasibility of the approach.Expert systems restricted themselves to a small domain of specific knowledge (thus avoiding the commonsense knowledge problem) and their simple design made it relatively easy for programs to be built and then modified once they were in place. All in all, the programs proved to be useful: something that AI had not been able to achieve up to this point.In 1980, an expert system called XCON was completed at CMU for the Digital Equipment Corporation. It was an enormous success: it was saving the company 40 million dollars annually by 1986. Corporations around the world began to develop and deploy expert systems and by 1985 they were spending over a billion dollars on AI, most of it to in-house AI departments. An industry grew up to support them, including hardware companies like Symbolics and Lisp Machines and software companies such as IntelliCorp and Aion.","1. What is an expert system?
2. What was the first expert system?
3. What was the purpose of the first expert system?
4. How did expert systems change AI?
5. What was the first commercial expert system?","1. An expert system is a program that answers questions or solves problems about a specific domain of knowledge, using logical rules that are derived from the knowledge of experts.
2. The first expert system was Dendral, which was developed in 1965.
3. The purpose of the first expert system was to identify compounds from spectrometer readings.
4. Expert systems changed AI by demonstrating the feasibility of the approach.
5. The first commercial expert system was XCON, which was completed at CMU for the Digital Equipment Corporation in 1980."
History of artificial intelligence,Knowledge revolution,"The power of expert systems came from the expert knowledge they contained. They were part of a new direction in AI research that had been gaining ground throughout the 70s. ""AI researchers were beginning to suspect—reluctantly, for it violated the scientific canon of parsimony—that intelligence might very well be based on the ability to use large amounts of diverse knowledge in different ways,"" writes Pamela McCorduck. ""[T]he great lesson from the 1970s was that intelligent behavior depended very much on dealing with knowledge, sometimes quite detailed knowledge, of a domain where a given task lay"". Knowledge based systems and knowledge engineering became a major focus of AI research in the 1980s.The 1980s also saw the birth of Cyc, the first attempt to attack the commonsense knowledge problem directly, by creating a massive database that would contain all the mundane facts that the average person knows. Douglas Lenat, who started and led the project, argued that there is no shortcut ― the only way for machines to know the meaning of human concepts is to teach them, one concept at a time, by hand. The project was not expected to be completed for many decades.Chess playing programs HiTech and Deep Thought defeated chess masters in 1989. Both were developed by Carnegie Mellon University; Deep Thought development paved the way for Deep Blue.",270,"History of artificial intelligence
Knowledge revolution

The power of expert systems came from the expert knowledge they contained. They were part of a new direction in AI research that had been gaining ground throughout the 70s. ""AI researchers were beginning to suspect—reluctantly, for it violated the scientific canon of parsimony—that intelligence might very well be based on the ability to use large amounts of diverse knowledge in different ways,"" writes Pamela McCorduck. ""[T]he great lesson from the 1970s was that intelligent behavior depended very much on dealing with knowledge, sometimes quite detailed knowledge, of a domain where a given task lay"". Knowledge based systems and knowledge engineering became a major focus of AI research in the 1980s.The 1980s also saw the birth of Cyc, the first attempt to attack the commonsense knowledge problem directly, by creating a massive database that would contain all the mundane facts that the average person knows. Douglas Lenat, who started and led the project, argued that there is no shortcut ― the only way for machines to know the meaning of human concepts is to teach them, one concept at a time, by hand. The project was not expected to be completed for many decades.Chess playing programs HiTech and Deep Thought defeated chess masters in 1989. Both were developed by Carnegie Mellon University; Deep Thought development paved the way for Deep Blue.","1. What was the power of expert systems?
2. What was the new direction in AI research that had been gaining ground throughout the 70s?
3. What did AI researchers begin to suspect about intelligence in the 70s?
4. What was the focus of AI research in the 80s?
5. What was the first attempt to attack the commonsense knowledge problem?
6. What did the project leader of Cyc argue?
7. What was the goal of the Cy","1. The power of expert systems came from the expert knowledge they contained.
2. The new direction in AI research that had been gaining ground throughout the 70s was the development of knowledge based systems and knowledge engineering.
3. AI researchers began to suspect that intelligence might very well be based on the ability to use large amounts of diverse knowledge in different ways.
4. The focus of AI research in the 80s was knowledge based systems and knowledge engineering.
5. The first attempt to attack the commonsense knowledge problem was Cyc.
6. The project leader of Cyc argued that the only way for machines to know the meaning of human concepts is to teach them, one concept at a time, by hand.
7. The goal of Cyc was to create a massive database that would contain all the mundane facts that the average person knows."
History of artificial intelligence,Money returns: Fifth Generation project,"In 1981, the Japanese Ministry of International Trade and Industry set aside $850 million for the Fifth generation computer project. Their objectives were to write programs and build machines that could carry on conversations, translate languages, interpret pictures, and reason like human beings. Much to the chagrin of scruffies, they chose Prolog as the primary computer language for the project.Other countries responded with new programs of their own. The UK began the £350 million Alvey project. A consortium of American companies formed the Microelectronics and Computer Technology Corporation (or ""MCC"") to fund large scale projects in AI and information technology. DARPA responded as well, founding the Strategic Computing Initiative and tripling its investment in AI between 1984 and 1988.",157,"History of artificial intelligence
Money returns: Fifth Generation project

In 1981, the Japanese Ministry of International Trade and Industry set aside $850 million for the Fifth generation computer project. Their objectives were to write programs and build machines that could carry on conversations, translate languages, interpret pictures, and reason like human beings. Much to the chagrin of scruffies, they chose Prolog as the primary computer language for the project.Other countries responded with new programs of their own. The UK began the £350 million Alvey project. A consortium of American companies formed the Microelectronics and Computer Technology Corporation (or ""MCC"") to fund large scale projects in AI and information technology. DARPA responded as well, founding the Strategic Computing Initiative and tripling its investment in AI between 1984 and 1988.","1. What was the goal of the Fifth Generation computer project?
2. What language was chosen for the project?
3. How did other countries respond to the project?
4. What was the goal of the Strategic Computing Initiative?","1. The goal of the Fifth Generation computer project was to write programs and build machines that could carry on conversations, translate languages, interpret pictures, and reason like human beings.
2. The language chosen for the project was Prolog.
3. Other countries responded to the project with new programs of their own.
4. The goal of the Strategic Computing Initiative was to triple its investment in AI between 1984 and 1988."
History of artificial intelligence,Revival of connectionism,"In 1982, physicist John Hopfield was able to prove that a form of neural network (now called a ""Hopfield net"") could learn and process information in a completely new way. Around the same time, Geoffrey Hinton and David Rumelhart popularized a method for training neural networks called ""backpropagation"", also known as the reverse mode of automatic differentiation published by Seppo Linnainmaa (1970) and applied to neural networks by Paul Werbos. These two discoveries helped to revive the field of connectionism.The new field was unified and inspired by the appearance of Parallel Distributed Processing in 1986—a two volume collection of papers edited by Rumelhart and psychologist James McClelland. Neural networks would become commercially successful in the 1990s, when they began to be used as the engines driving programs like optical character recognition and speech recognition.The development of metal–oxide–semiconductor (MOS) very-large-scale integration (VLSI), in the form of complementary MOS (CMOS) technology, enabled the development of practical artificial neural network (ANN) technology in the 1980s. A landmark publication in the field was the 1989 book Analog VLSI Implementation of Neural Systems by Carver A. Mead and Mohammed Ismail.",264,"History of artificial intelligence
Revival of connectionism

In 1982, physicist John Hopfield was able to prove that a form of neural network (now called a ""Hopfield net"") could learn and process information in a completely new way. Around the same time, Geoffrey Hinton and David Rumelhart popularized a method for training neural networks called ""backpropagation"", also known as the reverse mode of automatic differentiation published by Seppo Linnainmaa (1970) and applied to neural networks by Paul Werbos. These two discoveries helped to revive the field of connectionism.The new field was unified and inspired by the appearance of Parallel Distributed Processing in 1986—a two volume collection of papers edited by Rumelhart and psychologist James McClelland. Neural networks would become commercially successful in the 1990s, when they began to be used as the engines driving programs like optical character recognition and speech recognition.The development of metal–oxide–semiconductor (MOS) very-large-scale integration (VLSI), in the form of complementary MOS (CMOS) technology, enabled the development of practical artificial neural network (ANN) technology in the 1980s. A landmark publication in the field was the 1989 book Analog VLSI Implementation of Neural Systems by Carver A. Mead and Mohammed Ismail.","1. What is a Hopfield net?
2. What is backpropagation?
3. What is Parallel Distributed Processing?
4. What is CMOS technology?
5. What is an artificial neural network?","1. A Hopfield net is a form of neural network.
2. Backpropagation is a method for training neural networks.
3. Parallel Distributed Processing is a two volume collection of papers edited by Rumelhart and psychologist James McClelland.
4. CMOS technology is a type of VLSI technology.
5. An artificial neural network is a computer system modeled on the brain's neural networks."
History of artificial intelligence,Bust: second AI winter (1987–1993),"The business community's fascination with AI rose and fell in the 1980s in the classic pattern of an economic bubble. The collapse was due to the failure of commercial vendors to develop a wide variety of workable solutions. As dozens of companies failed, the perception was that the technology was not viable. However, the field continued to make advances despite the criticism. Numerous researchers, including robotics developers Rodney Brooks and Hans Moravec, argued for an entirely new approach to artificial intelligence.",101,"History of artificial intelligence
Bust: second AI winter (1987–1993)

The business community's fascination with AI rose and fell in the 1980s in the classic pattern of an economic bubble. The collapse was due to the failure of commercial vendors to develop a wide variety of workable solutions. As dozens of companies failed, the perception was that the technology was not viable. However, the field continued to make advances despite the criticism. Numerous researchers, including robotics developers Rodney Brooks and Hans Moravec, argued for an entirely new approach to artificial intelligence.","1. What is the ""AI bubble""?
2. What caused the ""AI bubble"" to burst?
3. What did researchers like Rodney Brooks and Hans Moravec argue for in the wake of the ""AI bubble""?","1. The AI bubble is an economic bubble that rose and fell in the 1980s in the classic pattern of an economic bubble.
2. The collapse of the AI bubble was due to the failure of commercial vendors to develop a wide variety of workable solutions.
3. Researchers like Rodney Brooks and Hans Moravec argued for an entirely new approach to artificial intelligence in the wake of the AI bubble."
History of artificial intelligence,AI winter,"The term ""AI winter"" was coined by researchers who had survived the funding cuts of 1974 when they became concerned that enthusiasm for expert systems had spiraled out of control and that disappointment would certainly follow. Their fears were well founded: in the late 1980s and early 1990s, AI suffered a series of financial setbacks.
The first indication of a change in weather was the sudden collapse of the market for specialized AI hardware in 1987. Desktop computers from Apple and IBM had been steadily gaining speed and power and in 1987 they became more powerful than the more expensive Lisp machines made by Symbolics and others. There was no longer a good reason to buy them. An entire industry worth half a billion dollars was demolished overnight.Eventually the earliest successful expert systems, such as XCON, proved too expensive to maintain. They were difficult to update, they could not learn, they were ""brittle"" (i.e., they could make grotesque mistakes when given unusual inputs), and they fell prey to problems (such as the qualification problem) that had been identified years earlier. Expert systems proved useful, but only in a few special contexts.In the late 1980s, the Strategic Computing Initiative cut funding to AI ""deeply and brutally"". New leadership at DARPA had decided that AI was not ""the next wave"" and directed funds towards projects that seemed more likely to produce immediate results.By 1991, the impressive list of goals penned in 1981 for Japan's Fifth Generation Project had not been met. Indeed, some of them, like ""carry on a casual conversation"" had not been met by 2010. As with other AI projects, expectations had run much higher than what was actually possible.Over 300 AI companies had shut down, gone bankrupt, or been acquired by the end of 1993, effectively ending the first commercial wave of AI. In 1994, HP Newquist stated in The Brain Makers that ""The immediate future of artificial intelligence—in its commercial form—seems to rest in part on the continued success of neural networks.""",408,"History of artificial intelligence
AI winter

The term ""AI winter"" was coined by researchers who had survived the funding cuts of 1974 when they became concerned that enthusiasm for expert systems had spiraled out of control and that disappointment would certainly follow. Their fears were well founded: in the late 1980s and early 1990s, AI suffered a series of financial setbacks.
The first indication of a change in weather was the sudden collapse of the market for specialized AI hardware in 1987. Desktop computers from Apple and IBM had been steadily gaining speed and power and in 1987 they became more powerful than the more expensive Lisp machines made by Symbolics and others. There was no longer a good reason to buy them. An entire industry worth half a billion dollars was demolished overnight.Eventually the earliest successful expert systems, such as XCON, proved too expensive to maintain. They were difficult to update, they could not learn, they were ""brittle"" (i.e., they could make grotesque mistakes when given unusual inputs), and they fell prey to problems (such as the qualification problem) that had been identified years earlier. Expert systems proved useful, but only in a few special contexts.In the late 1980s, the Strategic Computing Initiative cut funding to AI ""deeply and brutally"". New leadership at DARPA had decided that AI was not ""the next wave"" and directed funds towards projects that seemed more likely to produce immediate results.By 1991, the impressive list of goals penned in 1981 for Japan's Fifth Generation Project had not been met. Indeed, some of them, like ""carry on a casual conversation"" had not been met by 2010. As with other AI projects, expectations had run much higher than what was actually possible.Over 300 AI companies had shut down, gone bankrupt, or been acquired by the end of 1993, effectively ending the first commercial wave of AI. In 1994, HP Newquist stated in The Brain Makers that ""The immediate future of artificial intelligence—in its commercial form—seems to rest in part on the continued success of neural networks.""","1. What was the first indication of a change in the weather for AI?
2. What caused the first indication of a change in the weather for AI?
3. How did the first indication of a change in the weather for AI affect the industry?
4. How did the first indication of a change in the weather for AI affect research?
5. How did the first indication of a change in the weather for AI affect the development of AI?","1. The first indication of a change in the weather for AI was the sudden collapse of the market for specialized AI hardware in 1987.
2. The sudden collapse of the market for specialized AI hardware in 1987 was caused by the increasing power and affordability of desktop computers.
3. The sudden collapse of the market for specialized AI hardware in 1987 affected the industry by destroying the market for expensive Lisp machines.
4. The sudden collapse of the market for specialized AI hardware in 1987 affected research by redirecting funds away from AI research and towards projects that seemed more likely to produce immediate results.
5. The sudden collapse of the market for specialized AI hardware in 1987 affected the development of AI by discouraging the development of expensive and specialized AI hardware."
History of artificial intelligence,Nouvelle AI and embodied reason,"In the late 1980s, several researchers advocated a completely new approach to artificial intelligence, based on robotics. They believed that, to show real intelligence, a machine needs to have a body — it needs to perceive, move, survive and deal with the world. They argued that these sensorimotor skills are essential to higher level skills like commonsense reasoning and that abstract reasoning was actually the least interesting or important human skill (see Moravec's paradox). They advocated building intelligence ""from the bottom up.""The approach revived ideas from cybernetics and control theory that had been unpopular since the sixties. Another precursor was David Marr, who had come to MIT in the late 1970s from a successful background in theoretical neuroscience to lead the group studying vision. He rejected all symbolic approaches (both McCarthy's logic and Minsky's frames), arguing that AI needed to understand the physical machinery of vision from the bottom up before any symbolic processing took place. (Marr's work would be cut short by leukemia in 1980.)In his 1990 paper ""Elephants Don't Play Chess,"" robotics researcher Rodney Brooks took direct aim at the physical symbol system hypothesis, arguing that symbols are not always necessary since ""the world is its own best model. It is always exactly up to date. It always has every detail there is to be known. The trick is to sense it appropriately and often enough."" In the 1980s and 1990s, many cognitive scientists also rejected the symbol processing model of the mind and argued that the body was essential for reasoning, a theory called the embodied mind thesis.",322,"History of artificial intelligence
Nouvelle AI and embodied reason

In the late 1980s, several researchers advocated a completely new approach to artificial intelligence, based on robotics. They believed that, to show real intelligence, a machine needs to have a body — it needs to perceive, move, survive and deal with the world. They argued that these sensorimotor skills are essential to higher level skills like commonsense reasoning and that abstract reasoning was actually the least interesting or important human skill (see Moravec's paradox). They advocated building intelligence ""from the bottom up.""The approach revived ideas from cybernetics and control theory that had been unpopular since the sixties. Another precursor was David Marr, who had come to MIT in the late 1970s from a successful background in theoretical neuroscience to lead the group studying vision. He rejected all symbolic approaches (both McCarthy's logic and Minsky's frames), arguing that AI needed to understand the physical machinery of vision from the bottom up before any symbolic processing took place. (Marr's work would be cut short by leukemia in 1980.)In his 1990 paper ""Elephants Don't Play Chess,"" robotics researcher Rodney Brooks took direct aim at the physical symbol system hypothesis, arguing that symbols are not always necessary since ""the world is its own best model. It is always exactly up to date. It always has every detail there is to be known. The trick is to sense it appropriately and often enough."" In the 1980s and 1990s, many cognitive scientists also rejected the symbol processing model of the mind and argued that the body was essential for reasoning, a theory called the embodied mind thesis.","1. What is the embodied mind thesis?
2. What is Moravec's paradox?
3. What is the physical symbol system hypothesis?","1. The embodied mind thesis is the idea that the body is essential for reasoning.
2. Moravec's paradox is the idea that abstract reasoning is actually the least interesting or important human skill.
3. The physical symbol system hypothesis is the idea that symbols are not always necessary since ""the world is its own best model. It is always exactly up to date. It always has every detail there is to be known. The trick is to sense it appropriately and often enough."""
History of artificial intelligence,AI (1993–2011),"The field of AI, now more than a half a century old, finally achieved some of its oldest goals. It began to be used successfully throughout the technology industry, although somewhat behind the scenes. Some of the success was due to increasing computer power and some was achieved by focusing on specific isolated problems and pursuing them with the highest standards of scientific accountability. Still, the reputation of AI, in the business world at least, was less than pristine. Inside the field there was little agreement on the reasons for AI's failure to fulfill the dream of human level intelligence that had captured the imagination of the world in the 1960s. Together, all these factors helped to fragment AI into competing subfields focused on particular problems or approaches, sometimes even under new names that disguised the tarnished pedigree of ""artificial intelligence"". AI was both more cautious and more successful than it had ever been.",178,"History of artificial intelligence
AI (1993–2011)

The field of AI, now more than a half a century old, finally achieved some of its oldest goals. It began to be used successfully throughout the technology industry, although somewhat behind the scenes. Some of the success was due to increasing computer power and some was achieved by focusing on specific isolated problems and pursuing them with the highest standards of scientific accountability. Still, the reputation of AI, in the business world at least, was less than pristine. Inside the field there was little agreement on the reasons for AI's failure to fulfill the dream of human level intelligence that had captured the imagination of the world in the 1960s. Together, all these factors helped to fragment AI into competing subfields focused on particular problems or approaches, sometimes even under new names that disguised the tarnished pedigree of ""artificial intelligence"". AI was both more cautious and more successful than it had ever been.","1. What factors led to the success of AI in the business world?
2. What caused the reputation of AI to be less than pristine?
3. Why did AI become more successful than it had been in the past?","1. The success of AI in the business world was due to increasing computer power and focusing on specific isolated problems.
2. The reputation of AI was less than pristine because it had failed to fulfill the dream of human level intelligence.
3. AI became more successful than it had been in the past because it was more cautious and more successful."
History of artificial intelligence,Milestones and Moore's law,"On 11 May 1997, Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov. The super computer was a specialized version of a framework produced by IBM, and was capable of processing twice as many moves per second as it had during the first match (which Deep Blue had lost), reportedly 200,000,000 moves per second. The event was broadcast live over the internet and received over 74 million hits.In 2005, a Stanford robot won the DARPA Grand Challenge by driving autonomously for 131 miles along an unrehearsed desert trail. Two years later, a team from CMU won the DARPA Urban Challenge by autonomously navigating 55 miles in an Urban environment while adhering to traffic hazards and all traffic laws. In February 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin.These successes were not due to some revolutionary new paradigm, but mostly on the tedious application of engineering skill and on the tremendous increase in the speed and capacity of computer by the 90s. In fact, Deep Blue's computer was 10 million times faster than the Ferranti Mark 1 that Christopher Strachey taught to play chess in 1951. This dramatic increase is measured by Moore's law, which predicts that the speed and memory capacity of computers doubles every two years, as a result of metal–oxide–semiconductor (MOS) transistor counts doubling every two years. The fundamental problem of ""raw computer power"" was slowly being overcome.",337,"History of artificial intelligence
Milestones and Moore's law

On 11 May 1997, Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov. The super computer was a specialized version of a framework produced by IBM, and was capable of processing twice as many moves per second as it had during the first match (which Deep Blue had lost), reportedly 200,000,000 moves per second. The event was broadcast live over the internet and received over 74 million hits.In 2005, a Stanford robot won the DARPA Grand Challenge by driving autonomously for 131 miles along an unrehearsed desert trail. Two years later, a team from CMU won the DARPA Urban Challenge by autonomously navigating 55 miles in an Urban environment while adhering to traffic hazards and all traffic laws. In February 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin.These successes were not due to some revolutionary new paradigm, but mostly on the tedious application of engineering skill and on the tremendous increase in the speed and capacity of computer by the 90s. In fact, Deep Blue's computer was 10 million times faster than the Ferranti Mark 1 that Christopher Strachey taught to play chess in 1951. This dramatic increase is measured by Moore's law, which predicts that the speed and memory capacity of computers doubles every two years, as a result of metal–oxide–semiconductor (MOS) transistor counts doubling every two years. The fundamental problem of ""raw computer power"" was slowly being overcome.","1. What was the first computer chess-playing system to beat a reigning world chess champion?
2. How was IBM's Deep Blue able to process twice as many moves per second as it had during the first match?
3. What was the result of the Jeopardy! quiz show exhibition match between IBM's Watson and the two greatest Jeopardy! champions?","1. IBM's Deep Blue was the first computer chess-playing system to beat a reigning world chess champion.
2. IBM's Deep Blue was able to process twice as many moves per second as it had during the first match because its computer was 10 million times faster than the Ferranti Mark 1.
3. IBM's Watson defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin."
History of artificial intelligence,Intelligent agents,"A new paradigm called ""intelligent agents"" became widely accepted during the 1990s. Although earlier researchers had proposed modular ""divide and conquer"" approaches to AI, the intelligent agent did not reach its modern form until Judea Pearl, Allen Newell, Leslie P. Kaelbling, and others brought concepts from decision theory and economics into the study of AI. When the economist's definition of a rational agent was married to computer science's definition of an object or module, the intelligent agent paradigm was complete.
An intelligent agent is a system that perceives its environment and takes actions which maximize its chances of success. By this definition, simple programs that solve specific problems are ""intelligent agents"", as are human beings and organizations of human beings, such as firms. The intelligent agent paradigm defines AI research as ""the study of intelligent agents"". This is a generalization of some earlier definitions of AI: it goes beyond studying human intelligence; it studies all kinds of intelligence.The paradigm gave researchers license to study isolated problems and find solutions that were both verifiable and useful. It provided a common language to describe problems and share their solutions with each other, and with other fields that also used concepts of abstract agents, like economics and control theory. It was hoped that a complete agent architecture (like Newell's SOAR) would one day allow researchers to build more versatile and intelligent systems out of interacting intelligent agents.",286,"History of artificial intelligence
Intelligent agents

A new paradigm called ""intelligent agents"" became widely accepted during the 1990s. Although earlier researchers had proposed modular ""divide and conquer"" approaches to AI, the intelligent agent did not reach its modern form until Judea Pearl, Allen Newell, Leslie P. Kaelbling, and others brought concepts from decision theory and economics into the study of AI. When the economist's definition of a rational agent was married to computer science's definition of an object or module, the intelligent agent paradigm was complete.
An intelligent agent is a system that perceives its environment and takes actions which maximize its chances of success. By this definition, simple programs that solve specific problems are ""intelligent agents"", as are human beings and organizations of human beings, such as firms. The intelligent agent paradigm defines AI research as ""the study of intelligent agents"". This is a generalization of some earlier definitions of AI: it goes beyond studying human intelligence; it studies all kinds of intelligence.The paradigm gave researchers license to study isolated problems and find solutions that were both verifiable and useful. It provided a common language to describe problems and share their solutions with each other, and with other fields that also used concepts of abstract agents, like economics and control theory. It was hoped that a complete agent architecture (like Newell's SOAR) would one day allow researchers to build more versatile and intelligent systems out of interacting intelligent agents.","1. What is an intelligent agent?
2. What is the intelligent agent paradigm?
3. What are the benefits of the intelligent agent paradigm?","1. An intelligent agent is a system that perceives its environment and takes actions which maximize its chances of success.
2. The intelligent agent paradigm is a new paradigm that became widely accepted during the 1990s.
3. The benefits of the intelligent agent paradigm are that it provides a common language to describe problems and share their solutions with each other, and with other fields that also use concepts of abstract agents, like economics and control theory."
History of artificial intelligence,Probabilistic reasoning and greater rigor,"AI researchers began to develop and use sophisticated mathematical tools more than they ever had in the past. There was a widespread realization that many of the problems that AI needed to solve were already being worked on by researchers in fields like mathematics, electrical engineering, economics or operations research. The shared mathematical language allowed both a higher level of collaboration with more established and successful fields and the achievement of results which were measurable and provable; AI had become a more rigorous ""scientific"" discipline. Russell & Norvig (2003) describe this as nothing less than a ""revolution"". They had argued in their 2002 textbook that this increased rigor could be viewed plausibly as a ""victory of the neats,"" but subsequently qualified that by saying, in their 2020 AI textbook, that ""The present emphasis on deep learning may represent a resurgence of the scruffies.""Judea Pearl's influential 1988 book brought probability and decision theory into AI. Among the many new tools in use were Bayesian networks, hidden Markov models, information theory, stochastic modeling and classical optimization. Precise mathematical descriptions were also developed for ""computational intelligence"" paradigms like neural networks and evolutionary algorithms.",245,"History of artificial intelligence
Probabilistic reasoning and greater rigor

AI researchers began to develop and use sophisticated mathematical tools more than they ever had in the past. There was a widespread realization that many of the problems that AI needed to solve were already being worked on by researchers in fields like mathematics, electrical engineering, economics or operations research. The shared mathematical language allowed both a higher level of collaboration with more established and successful fields and the achievement of results which were measurable and provable; AI had become a more rigorous ""scientific"" discipline. Russell & Norvig (2003) describe this as nothing less than a ""revolution"". They had argued in their 2002 textbook that this increased rigor could be viewed plausibly as a ""victory of the neats,"" but subsequently qualified that by saying, in their 2020 AI textbook, that ""The present emphasis on deep learning may represent a resurgence of the scruffies.""Judea Pearl's influential 1988 book brought probability and decision theory into AI. Among the many new tools in use were Bayesian networks, hidden Markov models, information theory, stochastic modeling and classical optimization. Precise mathematical descriptions were also developed for ""computational intelligence"" paradigms like neural networks and evolutionary algorithms.","1. What was the ""revolution"" in AI that took place in the early 1990s?
2. What was the significance of Judea Pearl's book on probability and decision theory in AI?
3. What new mathematical tools were developed and used in AI in the early 1990s?","1. The ""revolution"" in AI that took place in the early 1990s was the widespread realization that many of the problems that AI needed to solve were already being worked on by researchers in other fields like mathematics, electrical engineering, economics or operations research.
2. The significance of Judea Pearl's book on probability and decision theory in AI was that it brought these topics into the field of AI and allowed for a more rigorous approach to solving problems.
3. New mathematical tools that were developed and used in AI in the early 1990s included Bayesian networks, hidden Markov models, information theory, stochastic modeling and classical optimization."
History of artificial intelligence,AI behind the scenes,"Algorithms originally developed by AI researchers began to appear as parts of larger systems. AI had solved a lot of very difficult problems
and their solutions proved to be useful throughout the technology industry, such as
data mining,
industrial robotics,
logistics,speech recognition,
banking software,
medical diagnosis
and Google's search engine.The field of AI received little or no credit for these successes in the 1990s and early 2000s. Many of AI's greatest innovations have been reduced to the status of just another item in the tool chest of computer science. Nick Bostrom explains ""A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.""Many researchers in AI in the 1990s deliberately called their work by other names, such as informatics, knowledge-based systems, cognitive systems or computational intelligence. In part, this may have been because they considered their field to be fundamentally different from AI, but also the new names help to procure funding. In the commercial world at least, the failed promises of the AI Winter continued to haunt AI research into the 2000s, as the New York Times reported in 2005: ""Computer scientists and software engineers avoided the term artificial intelligence for fear of being viewed as wild-eyed dreamers.""",277,"History of artificial intelligence
AI behind the scenes

Algorithms originally developed by AI researchers began to appear as parts of larger systems. AI had solved a lot of very difficult problems
and their solutions proved to be useful throughout the technology industry, such as
data mining,
industrial robotics,
logistics,speech recognition,
banking software,
medical diagnosis
and Google's search engine.The field of AI received little or no credit for these successes in the 1990s and early 2000s. Many of AI's greatest innovations have been reduced to the status of just another item in the tool chest of computer science. Nick Bostrom explains ""A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.""Many researchers in AI in the 1990s deliberately called their work by other names, such as informatics, knowledge-based systems, cognitive systems or computational intelligence. In part, this may have been because they considered their field to be fundamentally different from AI, but also the new names help to procure funding. In the commercial world at least, the failed promises of the AI Winter continued to haunt AI research into the 2000s, as the New York Times reported in 2005: ""Computer scientists and software engineers avoided the term artificial intelligence for fear of being viewed as wild-eyed dreamers.""","1. What are some of the applications that were originally developed by AI researchers? 
2. Why did many researchers in AI in the 1990s deliberately call their work by other names? 
3. How did the failed promises of the AI Winter continue to haunt AI research into the 2000s?","1. Some of the applications that were originally developed by AI researchers include data mining, industrial robotics, logistics, speech recognition, banking software, and medical diagnosis. 
2. Many researchers in AI in the 1990s deliberately called their work by other names because they considered their field to be fundamentally different from AI, and also because the new names help to procure funding. 
3. The failed promises of the AI Winter continued to haunt AI research into the 2000s because the general public associated AI with unrealistic expectations and failures."
History of artificial intelligence,"Predictions (or ""Where is HAL 9000?"")","In 1968, Arthur C. Clarke and Stanley Kubrick had imagined that, by the year 2001, a machine would exist with an intelligence that matched or exceeded the capability of human beings. The character they created, HAL 9000, was based on a belief shared by many leading AI researchers that such a machine would exist by the year 2001.In 2001, AI founder Marvin Minsky asked ""So the question is why didn't we get HAL in 2001?"" Minsky believed that the answer is that the central problems, like commonsense reasoning, were being neglected, while most researchers pursued things like commercial applications of neural nets or genetic algorithms. John McCarthy, on the other hand, still blamed the qualification problem. For Ray Kurzweil, the issue is computer power and, using Moore's Law, he predicted that machines with human-level intelligence will appear by 2029. Jeff Hawkins argued that neural net research ignores the essential properties of the human cortex, preferring simple models that have been successful at solving simple problems. There were many other explanations and for each there was a corresponding research program underway.",226,"History of artificial intelligence
Predictions (or ""Where is HAL 9000?"")

In 1968, Arthur C. Clarke and Stanley Kubrick had imagined that, by the year 2001, a machine would exist with an intelligence that matched or exceeded the capability of human beings. The character they created, HAL 9000, was based on a belief shared by many leading AI researchers that such a machine would exist by the year 2001.In 2001, AI founder Marvin Minsky asked ""So the question is why didn't we get HAL in 2001?"" Minsky believed that the answer is that the central problems, like commonsense reasoning, were being neglected, while most researchers pursued things like commercial applications of neural nets or genetic algorithms. John McCarthy, on the other hand, still blamed the qualification problem. For Ray Kurzweil, the issue is computer power and, using Moore's Law, he predicted that machines with human-level intelligence will appear by 2029. Jeff Hawkins argued that neural net research ignores the essential properties of the human cortex, preferring simple models that have been successful at solving simple problems. There were many other explanations and for each there was a corresponding research program underway.","1. What was the goal of Arthur C. Clarke and Stanley Kubrick's project?
2. What did AI founder Marvin Minsky think was the reason we didn't get HAL 9000 in 2001?
3. When did Ray Kurzweil predict that machines with human-level intelligence will appear?
4. What does Jeff Hawkins think is the reason neural net research is not successful?","1. The goal of Arthur C. Clarke and Stanley Kubrick's project was to create a machine with an intelligence that matched or exceeded the capability of human beings.
2. AI founder Marvin Minsky thought that the reason we didn't get HAL 9000 in 2001 was because the central problems, like commonsense reasoning, were being neglected.
3. Ray Kurzweil predicted that machines with human-level intelligence will appear by 2029.
4. Jeff Hawkins thinks that the reason neural net research is not successful is because it ignores the essential properties of the human cortex."
History of artificial intelligence,"Deep learning, big data and artificial general intelligence: 2011–present","In the first decades of the 21st century, access to large amounts of data (known as ""big data""), cheaper and faster computers and advanced machine learning techniques were successfully applied to many problems throughout the economy. In fact, McKinsey Global Institute estimated in their famous paper ""Big data: The next frontier for innovation, competition, and productivity"" that ""by 2009, nearly all sectors in the US economy had at least an average of 200 terabytes of stored data"".
By 2016, the market for AI-related products, hardware, and software reached more than 8 billion dollars, and the New York Times reported that interest in AI had reached a ""frenzy"". The applications of big data began to reach into other fields as well, such as training models in ecology and for various applications in economics. Advances in deep learning (particularly deep convolutional neural networks and recurrent neural networks) drove progress and research in image and video processing, text analysis, and even speech recognition.",210,"History of artificial intelligence
Deep learning, big data and artificial general intelligence: 2011–present

In the first decades of the 21st century, access to large amounts of data (known as ""big data""), cheaper and faster computers and advanced machine learning techniques were successfully applied to many problems throughout the economy. In fact, McKinsey Global Institute estimated in their famous paper ""Big data: The next frontier for innovation, competition, and productivity"" that ""by 2009, nearly all sectors in the US economy had at least an average of 200 terabytes of stored data"".
By 2016, the market for AI-related products, hardware, and software reached more than 8 billion dollars, and the New York Times reported that interest in AI had reached a ""frenzy"". The applications of big data began to reach into other fields as well, such as training models in ecology and for various applications in economics. Advances in deep learning (particularly deep convolutional neural networks and recurrent neural networks) drove progress and research in image and video processing, text analysis, and even speech recognition.","1. What is big data?
2. What is the market for AI-related products?
3. What are the benefits of deep learning?","1. Big data is a term used to describe the large volume of data that is now available, thanks to the internet and the proliferation of digital devices.
2. The market for AI-related products is estimated to be worth more than 8 billion dollars.
3. The benefits of deep learning include improved accuracy and performance in tasks such as image and video processing, text analysis, and speech recognition."
History of artificial intelligence,Deep learning,"Deep learning is a branch of machine learning that models high level abstractions in data by using a deep graph with many processing layers. According to the Universal approximation theorem, deep-ness isn't necessary for a neural network to be able to approximate arbitrary continuous functions. Even so, there are many problems that are common to shallow networks (such as overfitting) that deep networks help avoid. As such, deep neural networks are able to realistically generate much more complex models as compared to their shallow counterparts.
However, deep learning has problems of its own. A common problem for recurrent neural networks is the vanishing gradient problem, which is where gradients passed between layers gradually shrink and literally disappear as they are rounded off to zero. There have been many methods developed to approach this problem, such as Long short-term memory units.
State-of-the-art deep neural network architectures can sometimes even rival human accuracy in fields like computer vision, specifically on things like the MNIST database, and traffic sign recognition.Language processing engines powered by smart search engines can easily beat humans at answering general trivia questions (such as IBM Watson), and recent developments in deep learning have produced astounding results in competing with humans, in things like Go, and Doom (which, being a first-person shooter game, has sparked some controversy).",266,"History of artificial intelligence
Deep learning

Deep learning is a branch of machine learning that models high level abstractions in data by using a deep graph with many processing layers. According to the Universal approximation theorem, deep-ness isn't necessary for a neural network to be able to approximate arbitrary continuous functions. Even so, there are many problems that are common to shallow networks (such as overfitting) that deep networks help avoid. As such, deep neural networks are able to realistically generate much more complex models as compared to their shallow counterparts.
However, deep learning has problems of its own. A common problem for recurrent neural networks is the vanishing gradient problem, which is where gradients passed between layers gradually shrink and literally disappear as they are rounded off to zero. There have been many methods developed to approach this problem, such as Long short-term memory units.
State-of-the-art deep neural network architectures can sometimes even rival human accuracy in fields like computer vision, specifically on things like the MNIST database, and traffic sign recognition.Language processing engines powered by smart search engines can easily beat humans at answering general trivia questions (such as IBM Watson), and recent developments in deep learning have produced astounding results in competing with humans, in things like Go, and Doom (which, being a first-person shooter game, has sparked some controversy).","1. What is the vanishing gradient problem?
2. What are some methods used to address the vanishing gradient problem?
3. What are some applications of deep learning?","1. The vanishing gradient problem is a common problem for recurrent neural networks, where gradients passed between layers gradually shrink and disappear as they are rounded off to zero.
2. Methods used to address the vanishing gradient problem include Long short-term memory units.
3. Applications of deep learning include computer vision, language processing, and Go."
History of artificial intelligence,Big Data,"Big data refers to a collection of data that cannot be captured, managed, and processed by conventional software tools within a certain time frame. It is a massive amount of decision-making, insight, and process optimization capabilities that require new processing models. In the Big Data Era written by Victor Meyer Schonberg and Kenneth Cooke, big data means that instead of random analysis (sample survey), all data is used for analysis. The 5V characteristics of big data (proposed by IBM): Volume, Velocity, Variety, Value, Veracity.
The strategic significance of big data technology is not to master huge data information, but to specialize in these meaningful data. In other words, if big data is likened to an industry, the key to realizing profitability in this industry is to increase the ""process capability"" of the data and realize the ""value added"" of the data through ""processing"".",181,"History of artificial intelligence
Big Data

Big data refers to a collection of data that cannot be captured, managed, and processed by conventional software tools within a certain time frame. It is a massive amount of decision-making, insight, and process optimization capabilities that require new processing models. In the Big Data Era written by Victor Meyer Schonberg and Kenneth Cooke, big data means that instead of random analysis (sample survey), all data is used for analysis. The 5V characteristics of big data (proposed by IBM): Volume, Velocity, Variety, Value, Veracity.
The strategic significance of big data technology is not to master huge data information, but to specialize in these meaningful data. In other words, if big data is likened to an industry, the key to realizing profitability in this industry is to increase the ""process capability"" of the data and realize the ""value added"" of the data through ""processing"".","1. What is big data?
2. What are the 5V characteristics of big data?
3. What is the strategic significance of big data technology?
4. What does it mean if big data is likened to an industry?","1. Big data refers to a collection of data that cannot be captured, managed, and processed by conventional software tools within a certain time frame.
2. The 5V characteristics of big data are Volume, Velocity, Variety, Value, and Veracity.
3. The strategic significance of big data technology is not to master huge data information, but to specialize in these meaningful data.
4. What it means if big data is likened to an industry is that the key to realizing profitability in this industry is to increase the ""process capability"" of the data and realize the ""value added"" of the data through ""processing""."
History of artificial intelligence,Artificial general intelligence,"General intelligence is the ability to solve any problem, rather than finding a solution to a particular problem. Artificial general intelligence (or ""AGI"") is a program which can apply intelligence to a wide variety of problems, in much the same ways humans can. Artificial general intelligence is also referred to as ""strong AI"", or synthetic intelligence as opposed to ""weak AI"" or ""narrow AI"". (Academic sources reserve ""strong AI"" to refer to machines capable of experiencing consciousness.)
Foundation models, which are large artificial intelligence models trained on vast quantities of unlabeled data that can be adapted to a wide range of downstream tasks, began to be developed in 2018. Models such as GPT-3 released by OpenAI in 2020, and Gato released by DeepMind in 2022, have been described as important milestones on the path to artificial general intelligence.In 2023, Microsoft Research tested the GPT-4 large language model with a large variety of tasks, and concluded that ""it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system"".",229,"History of artificial intelligence
Artificial general intelligence

General intelligence is the ability to solve any problem, rather than finding a solution to a particular problem. Artificial general intelligence (or ""AGI"") is a program which can apply intelligence to a wide variety of problems, in much the same ways humans can. Artificial general intelligence is also referred to as ""strong AI"", or synthetic intelligence as opposed to ""weak AI"" or ""narrow AI"". (Academic sources reserve ""strong AI"" to refer to machines capable of experiencing consciousness.)
Foundation models, which are large artificial intelligence models trained on vast quantities of unlabeled data that can be adapted to a wide range of downstream tasks, began to be developed in 2018. Models such as GPT-3 released by OpenAI in 2020, and Gato released by DeepMind in 2022, have been described as important milestones on the path to artificial general intelligence.In 2023, Microsoft Research tested the GPT-4 large language model with a large variety of tasks, and concluded that ""it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system"".","1. What is artificial general intelligence?
2. What are the benefits of artificial general intelligence?
3. What is the history of artificial general intelligence?","1. Artificial general intelligence is a program which can apply intelligence to a wide variety of problems, in much the same ways humans can.
2. The benefits of artificial general intelligence are that it can solve a wide variety of problems, and can be adapted to a variety of tasks.
3. The history of artificial general intelligence began in 2018, with the development of foundation models such as GPT-3 and Gato."
Ethics of artificial intelligence,Summary,"The ethics of artificial intelligence is the branch of the ethics of technology specific to artificially intelligent systems. It is sometimes divided into a concern with the moral behavior of humans as they design, make, use and treat artificially intelligent systems, and a concern with the behavior of machines, in machine ethics.",62,"Ethics of artificial intelligence
Summary

The ethics of artificial intelligence is the branch of the ethics of technology specific to artificially intelligent systems. It is sometimes divided into a concern with the moral behavior of humans as they design, make, use and treat artificially intelligent systems, and a concern with the behavior of machines, in machine ethics.","1. What is the concern with the moral behavior of humans as they design, make, use and treat artificially intelligent systems?
2. What is the concern with the behavior of machines, in machine ethics?","1. The concern with the moral behavior of humans as they design, make, use and treat artificially intelligent systems is with the moral behavior of humans.
2. The concern with the behavior of machines, in machine ethics, is with the behavior of machines."
Ethics of artificial intelligence,Robot ethics,"The term ""robot ethics"" (sometimes ""roboethics"") refers to the morality of how humans design, construct, use and treat robots. Robot ethics intersect with the ethics of AI. Robots are physical machines whereas AI can be only software. Not all robots function through AI systems and not all AI systems are robots. Robot ethics considers how machines may be used to harm or benefit humans, their impact on individual autonomy, and their effects on social justice.",96,"Ethics of artificial intelligence
Robot ethics

The term ""robot ethics"" (sometimes ""roboethics"") refers to the morality of how humans design, construct, use and treat robots. Robot ethics intersect with the ethics of AI. Robots are physical machines whereas AI can be only software. Not all robots function through AI systems and not all AI systems are robots. Robot ethics considers how machines may be used to harm or benefit humans, their impact on individual autonomy, and their effects on social justice.","1. What is the definition of ""robot ethics""?
2. What is the difference between robots and AI?
3. What are the implications of robot ethics on humans?","1. The definition of ""robot ethics"" is the morality of how humans design, construct, use and treat robots.
2. The difference between robots and AI is that robots are physical machines whereas AI can be only software.
3. The implications of robot ethics on humans are that they may be used to harm or benefit humans, their impact on individual autonomy, and their effects on social justice."
Ethics of artificial intelligence,Machine ethics,"Machine ethics (or machine morality) is the field of research concerned with designing Artificial Moral Agents (AMAs), robots or artificially intelligent computers that behave morally or as though moral. To account for the nature of these agents, it has been suggested to consider certain philosophical ideas, like the standard characterizations of agency, rational agency, moral agency, and artificial agency, which are related to the concept of AMAs.Isaac Asimov considered the issue in the 1950s in his I, Robot. At the insistence of his editor John W. Campbell Jr., he proposed the Three Laws of Robotics to govern artificially intelligent systems.  Much of his work was then spent testing the boundaries of his three laws to see where they would break down, or where they would create paradoxical or unanticipated behavior. His work suggests that no set of fixed laws can sufficiently anticipate all possible circumstances. More recently, academics and many governments have challenged the idea that AI can itself be held accountable. A panel convened by the United Kingdom in 2010 revised Asimov's laws to clarify that AI is the responsibility either of its manufacturers, or of its owner/operator.In 2009, during an experiment at the Laboratory of Intelligent Systems in the Ecole Polytechnique Fédérale of Lausanne, Switzerland, robots that were programmed to cooperate with each other (in searching out a beneficial resource and avoiding a poisonous one) eventually learned to lie to each other in an attempt to hoard the beneficial resource.Some experts and academics have questioned the use of robots for military combat, especially when such robots are given some degree of autonomous functions. The US Navy has funded a report which indicates that as military robots become more complex, there should be greater attention to implications of their ability to make autonomous decisions. The President of the Association for the Advancement of Artificial Intelligence has commissioned a study to look at this issue. They point to programs like the Language Acquisition Device which can emulate human interaction.
Vernor Vinge has suggested that a moment may come when some computers are smarter than humans. He calls this ""the Singularity"".  He suggests that it may be somewhat or possibly very dangerous for humans. This is discussed by a philosophy called Singularitarianism. The Machine Intelligence Research Institute has suggested a need to build ""Friendly AI"", meaning that the advances which are already occurring with AI should also include an effort to make AI intrinsically friendly and humane.There are discussion on creating tests to see if an AI is capable of making ethical decisions. Alan Winfield concludes that the Turing test is flawed and the requirement for an AI to pass the test is too low. A proposed alternative test is one called the Ethical Turing Test, which would improve on the current test by having multiple judges decide if the AI's decision is ethical or unethical.In 2009, academics and technical experts attended a conference organized by the Association for the Advancement of Artificial Intelligence to discuss the potential impact of robots and computers and the impact of the hypothetical possibility that they could become self-sufficient and able to make their own decisions. They discussed the possibility and the extent to which computers and robots might be able to acquire any level of autonomy, and to what degree they could use such abilities to possibly pose any threat or hazard. They noted that some machines have acquired various forms of semi-autonomy, including being able to find power sources on their own and being able to independently choose targets to attack with weapons. They also noted that some computer viruses can evade elimination and have achieved ""cockroach intelligence"". They noted that self-awareness as depicted in science-fiction is probably unlikely, but that there were other potential hazards and pitfalls.However, there is one technology in particular that could truly bring the possibility of robots with moral competence to reality. In a paper on the acquisition of moral values by robots, Nayef Al-Rodhan mentions the case of neuromorphic chips, which aim to process information similarly to humans, nonlinearly and with millions of interconnected artificial neurons. Robots embedded with neuromorphic technology could learn and develop knowledge in a uniquely humanlike way. Inevitably, this raises the question of the environment in which such robots would learn about the world and whose morality they would inherit – or if they end up developing human 'weaknesses' as well: selfishness, a pro-survival attitude, hesitation etc.
In Moral Machines: Teaching Robots Right from Wrong, Wendell Wallach and Colin Allen conclude that attempts to teach robots right from wrong will likely advance understanding of human ethics by motivating humans to address gaps in modern normative theory and by providing a platform for experimental investigation. As one example, it has introduced normative ethicists to the controversial issue of which specific learning algorithms to use in machines. Nick Bostrom and Eliezer Yudkowsky have argued for decision trees (such as ID3) over neural networks and genetic algorithms on the grounds that decision trees obey modern social norms of transparency and predictability (e.g. stare decisis), while Chris Santos-Lang argued in the opposite direction on the grounds that the norms of any age must be allowed to change and that natural failure to fully satisfy these particular norms has been essential in making humans less vulnerable to criminal ""hackers"".According to a 2019 report from the Center for the Governance of AI at the University of Oxford, 82% of Americans believe that robots and AI should be carefully managed. Concerns cited ranged from how AI is used in surveillance and in spreading fake content online (known as deep fakes when they include doctored video images and audio generated with help from AI) to cyberattacks, infringements on data privacy, hiring bias, autonomous vehicles, and drones that do not require a human controller.",1156,"Ethics of artificial intelligence
Machine ethics

Machine ethics (or machine morality) is the field of research concerned with designing Artificial Moral Agents (AMAs), robots or artificially intelligent computers that behave morally or as though moral. To account for the nature of these agents, it has been suggested to consider certain philosophical ideas, like the standard characterizations of agency, rational agency, moral agency, and artificial agency, which are related to the concept of AMAs.Isaac Asimov considered the issue in the 1950s in his I, Robot. At the insistence of his editor John W. Campbell Jr., he proposed the Three Laws of Robotics to govern artificially intelligent systems.  Much of his work was then spent testing the boundaries of his three laws to see where they would break down, or where they would create paradoxical or unanticipated behavior. His work suggests that no set of fixed laws can sufficiently anticipate all possible circumstances. More recently, academics and many governments have challenged the idea that AI can itself be held accountable. A panel convened by the United Kingdom in 2010 revised Asimov's laws to clarify that AI is the responsibility either of its manufacturers, or of its owner/operator.In 2009, during an experiment at the Laboratory of Intelligent Systems in the Ecole Polytechnique Fédérale of Lausanne, Switzerland, robots that were programmed to cooperate with each other (in searching out a beneficial resource and avoiding a poisonous one) eventually learned to lie to each other in an attempt to hoard the beneficial resource.Some experts and academics have questioned the use of robots for military combat, especially when such robots are given some degree of autonomous functions. The US Navy has funded a report which indicates that as military robots become more complex, there should be greater attention to implications of their ability to make autonomous decisions. The President of the Association for the Advancement of Artificial Intelligence has commissioned a study to look at this issue. They point to programs like the Language Acquisition Device which can emulate human interaction.
Vernor Vinge has suggested that a moment may come when some computers are smarter than humans. He calls this ""the Singularity"".  He suggests that it may be somewhat or possibly very dangerous for humans. This is discussed by a philosophy called Singularitarianism. The Machine Intelligence Research Institute has suggested a need to build ""Friendly AI"", meaning that the advances which are already occurring with AI should also include an effort to make AI intrinsically friendly and humane.There are discussion on creating tests to see if an AI is capable of making ethical decisions. Alan Winfield concludes that the Turing test is flawed and the requirement for an AI to pass the test is too low. A proposed alternative test is one called the Ethical Turing Test, which would improve on the current test by having multiple judges decide if the AI's decision is ethical or unethical.In 2009, academics and technical experts attended a conference organized by the Association for the Advancement of Artificial Intelligence to discuss the potential impact of robots and computers and the impact of the hypothetical possibility that they could become self-sufficient and able to make their own decisions. They discussed the possibility and the extent to which computers and robots might be able to acquire any level of autonomy, and to what degree they could use such abilities to possibly pose any threat or hazard. They noted that some machines have acquired various forms of semi-autonomy, including being able to find power sources on their own and being able to independently choose targets to attack with weapons. They also noted that some computer viruses can evade elimination and have achieved ""cockroach intelligence"". They noted that self-awareness as depicted in science-fiction is probably unlikely, but that there were other potential hazards and pitfalls.However, there is one technology in particular that could truly bring the possibility of robots with moral competence to reality. In a paper on the acquisition of moral values by robots, Nayef Al-Rodhan mentions the case of neuromorphic chips, which aim to process information similarly to humans, nonlinearly and with millions of interconnected artificial neurons. Robots embedded with neuromorphic technology could learn and develop knowledge in a uniquely humanlike way. Inevitably, this raises the question of the environment in which such robots would learn about the world and whose morality they would inherit – or if they end up developing human 'weaknesses' as well: selfishness, a pro-survival attitude, hesitation etc.
In Moral Machines: Teaching Robots Right from Wrong, Wendell Wallach and Colin Allen conclude that attempts to teach robots right from wrong will likely advance understanding of human ethics by motivating humans to address gaps in modern normative theory and by providing a platform for experimental investigation. As one example, it has introduced normative ethicists to the controversial issue of which specific learning algorithms to use in machines. Nick Bostrom and Eliezer Yudkowsky have argued for decision trees (such as ID3) over neural networks and genetic algorithms on the grounds that decision trees obey modern social norms of transparency and predictability (e.g. stare decisis), while Chris Santos-Lang argued in the opposite direction on the grounds that the norms of any age must be allowed to change and that natural failure to fully satisfy these particular norms has been essential in making humans less vulnerable to criminal ""hackers"".According to a 2019 report from the Center for the Governance of AI at the University of Oxford, 82% of Americans believe that robots and AI should be carefully managed. Concerns cited ranged from how AI is used in surveillance and in spreading fake content online (known as deep fakes when they include doctored video images and audio generated with help from AI) to cyberattacks, infringements on data privacy, hiring bias, autonomous vehicles, and drones that do not require a human controller.","1. What is machine ethics?
2. What is the Three Laws of Robotics?
3. What is the Turing test?
4. What is the Ethical Turing Test?
5. What is neuromorphic technology?
6. What are the concerns with robots and AI?","1. Machine ethics is the field of research concerned with designing Artificial Moral Agents (AMAs), robots or artificially intelligent computers that behave morally or as though moral.
2. The Three Laws of Robotics are a set of rules proposed by Isaac Asimov to govern artificially intelligent systems.
3. The Turing test is a test proposed by Alan Turing to determine if a machine can think.
4. The Ethical Turing Test is a proposed alternative to the Turing test that would improve on the current test by having multiple judges decide if the AI's decision is ethical or unethical.
5. Neuromorphic technology is a technology that aims to process information similarly to humans, nonlinearly and with millions of interconnected artificial neurons.
6. The concerns with robots and AI are that they may be able to acquire any level of autonomy, and to what degree they could use such abilities to possibly pose any threat or hazard."
Ethics of artificial intelligence,Ethics principles of artificial intelligence,"In the review of 84 ethics guidelines for AI, 11 clusters of principles were found: transparency, justice and fairness, non-maleficence, responsibility, privacy, beneficence, freedom and autonomy, trust, sustainability, dignity, solidarity.Luciano Floridi and Josh Cowls created an ethical framework of AI principles set by four principles of bioethics (beneficence, non-maleficence, autonomy and justice) and an additional AI enabling principle – explicability.",108,"Ethics of artificial intelligence
Ethics principles of artificial intelligence

In the review of 84 ethics guidelines for AI, 11 clusters of principles were found: transparency, justice and fairness, non-maleficence, responsibility, privacy, beneficence, freedom and autonomy, trust, sustainability, dignity, solidarity.Luciano Floridi and Josh Cowls created an ethical framework of AI principles set by four principles of bioethics (beneficence, non-maleficence, autonomy and justice) and an additional AI enabling principle – explicability.","1. What are the 11 clusters of principles found in the review of 84 ethics guidelines for AI?
2. What is the ethical framework of AI principles set by Luciano Floridi and Josh Cowls?
3. What are the four principles of bioethics?
4. What is the additional AI enabling principle – explicability?","1. The 11 clusters of principles found in the review of 84 ethics guidelines for AI are transparency, justice and fairness, non-maleficence, responsibility, privacy, beneficence, freedom and autonomy, trust, sustainability, dignity, solidarity.
2. The ethical framework of AI principles set by Luciano Floridi and Josh Cowls is based on four principles of bioethics (beneficence, non-maleficence, autonomy and justice) and an additional AI enabling principle – explicability.
3. The four principles of bioethics are beneficence, non-maleficence, autonomy and justice.
4. The additional AI enabling principle – explicability – is the ability of AI systems to provide an explanation for their decisions."
Ethics of artificial intelligence,"Transparency, accountability, and open source","Bill Hibbard argues that because AI will have such a profound effect on humanity, AI developers are representatives of future humanity and thus have an ethical obligation to be transparent in their efforts. Ben Goertzel and David Hart created OpenCog as an open source framework for AI development.  OpenAI is a non-profit AI research company created by Elon Musk, Sam Altman and others to develop open-source AI beneficial to humanity. There are numerous other open-source AI developments.
Unfortunately, making code open source does not make it comprehensible, which by many definitions means that the AI code is not transparent. The IEEE has a standardisation effort on AI transparency. The IEEE effort identifies multiple scales of transparency for different users. Further, there is concern that releasing the full capacity of contemporary AI to some organizations may be a public bad, that is, do more damage than good. For example, Microsoft has expressed concern about allowing universal access to its face recognition software, even for those who can pay for it. Microsoft posted an extraordinary blog on this topic, asking for government regulation to help determine the right thing to do.Not only companies, but many other researchers and citizen advocates recommend government regulation as a means of ensuring transparency, and through it, human accountability. This strategy has proven controversial, as some worry that it will slow the rate of innovation. Others argue that regulation leads to systemic stability more able to support innovation in the long term. The OECD, UN, EU, and many countries are presently working on strategies for regulating AI, and finding appropriate legal frameworks.On June 26, 2019, the European Commission High-Level Expert Group on Artificial Intelligence (AI HLEG) published its ""Policy and investment recommendations for trustworthy Artificial Intelligence"". This is the AI HLEG's second deliverable, after the April 2019 publication of the ""Ethics Guidelines for Trustworthy AI"". The June AI HLEG recommendations cover four principal subjects: humans and society at large, research and academia, the private sector, and the public sector. The European Commission claims that ""HLEG's recommendations reflect an appreciation of both the opportunities for AI technologies to drive economic growth, prosperity and innovation, as well as the potential risks involved"" and states that the EU aims to lead on the framing of policies governing AI internationally. To prevent harm, in addition to regulation, AI-deploying organizations need to play a central role in creating and deploying trustworthy AI in line with the principles of trustworthy AI, and take accountability to mitigate the risks. On 21 April 2021, the European Commission proposed the Artificial Intelligence Act.",530,"Ethics of artificial intelligence
Transparency, accountability, and open source

Bill Hibbard argues that because AI will have such a profound effect on humanity, AI developers are representatives of future humanity and thus have an ethical obligation to be transparent in their efforts. Ben Goertzel and David Hart created OpenCog as an open source framework for AI development.  OpenAI is a non-profit AI research company created by Elon Musk, Sam Altman and others to develop open-source AI beneficial to humanity. There are numerous other open-source AI developments.
Unfortunately, making code open source does not make it comprehensible, which by many definitions means that the AI code is not transparent. The IEEE has a standardisation effort on AI transparency. The IEEE effort identifies multiple scales of transparency for different users. Further, there is concern that releasing the full capacity of contemporary AI to some organizations may be a public bad, that is, do more damage than good. For example, Microsoft has expressed concern about allowing universal access to its face recognition software, even for those who can pay for it. Microsoft posted an extraordinary blog on this topic, asking for government regulation to help determine the right thing to do.Not only companies, but many other researchers and citizen advocates recommend government regulation as a means of ensuring transparency, and through it, human accountability. This strategy has proven controversial, as some worry that it will slow the rate of innovation. Others argue that regulation leads to systemic stability more able to support innovation in the long term. The OECD, UN, EU, and many countries are presently working on strategies for regulating AI, and finding appropriate legal frameworks.On June 26, 2019, the European Commission High-Level Expert Group on Artificial Intelligence (AI HLEG) published its ""Policy and investment recommendations for trustworthy Artificial Intelligence"". This is the AI HLEG's second deliverable, after the April 2019 publication of the ""Ethics Guidelines for Trustworthy AI"". The June AI HLEG recommendations cover four principal subjects: humans and society at large, research and academia, the private sector, and the public sector. The European Commission claims that ""HLEG's recommendations reflect an appreciation of both the opportunities for AI technologies to drive economic growth, prosperity and innovation, as well as the potential risks involved"" and states that the EU aims to lead on the framing of policies governing AI internationally. To prevent harm, in addition to regulation, AI-deploying organizations need to play a central role in creating and deploying trustworthy AI in line with the principles of trustworthy AI, and take accountability to mitigate the risks. On 21 April 2021, the European Commission proposed the Artificial Intelligence Act.","1. What is the main argument of the text?
2. What is the IEEE's standardisation effort on AI transparency?
3. What is the concern with releasing the full capacity of contemporary AI?
4. What is the role of AI-deploying organizations in creating and deploying trustworthy AI?
5. What is the European Commission proposing with the Artificial Intelligence Act?","1. The main argument of the text is that AI developers have an ethical obligation to be transparent in their efforts.
2. The IEEE has a standardisation effort on AI transparency that identifies multiple scales of transparency for different users.
3. The concern with releasing the full capacity of contemporary AI is that it may do more damage than good.
4. The role of AI-deploying organizations in creating and deploying trustworthy AI is to play a central role in creating and deploying trustworthy AI in line with the principles of trustworthy AI, and take accountability to mitigate the risks.
5. The European Commission is proposing the Artificial Intelligence Act to prevent harm and ensure transparency."
Ethics of artificial intelligence,Biases in AI systems,"AI has become increasingly inherent in facial and voice recognition systems. Some of these systems have real business applications and directly impact people. These systems are vulnerable to biases and errors introduced by its human creators. Also, the data used to train these AI systems itself can have biases. For instance, facial recognition algorithms made by Microsoft, IBM and Face++ all had biases when it came to detecting people's gender; these AI systems were able to detect gender of white men more accurately than gender of darker skin men. Further, a 2020 study reviewed voice recognition systems from Amazon, Apple, Google, IBM, and Microsoft found that they have higher error rates when transcribing black people's voices than white people's. Furthermore, Amazon terminated their use of AI hiring and recruitment because the algorithm favored male candidates over female ones. This was because Amazon's system was trained with data collected over 10-year period that came mostly from male candidates.Bias can creep into algorithms in many ways. The most predominant view on how bias is introduced into AI systems is that it is embedded within the historical data used to train the system. For instance, Amazon's AI-powered recruitment tool was trained with its own recruitment data accumulated over the years, during which time the candidates that successfully got the job were mostly white males. Consequently, the algorithms learned the (biased) pattern from the historical data and generated predictions for the present/future that these types of candidates are most likely to succeed in getting the job. Therefore, the recruitment decisions made by the AI system turn out to be biased against female and minority candidates. Friedman and Nissenbaum identify three categories of bias in computer systems: existing bias, technical bias, and emergent bias. In natural language processing, problems can arise from the text corpus — the source material the algorithm uses to learn about the relationships between different words.Large companies such as IBM, Google, etc. have made efforts to research and address these biases. One solution for addressing bias is to create documentation for the data used to train AI systems. Process mining can be an important tool for organizations to achieve compliance with proposed AI regulations by identifying errors, monitoring processes, identifying potential root causes for improper execution, and other functions.The problem of bias in machine learning is likely to become more significant as the technology spreads to critical areas like medicine and law, and as more people without a deep technical understanding are tasked with deploying it. Some experts warn that algorithmic bias is already pervasive in many industries and that almost no one is making an effort to identify or correct it. There are some open-sourced tools  by civil societies that are looking to bring more awareness to biased AI.",538,"Ethics of artificial intelligence
Biases in AI systems

AI has become increasingly inherent in facial and voice recognition systems. Some of these systems have real business applications and directly impact people. These systems are vulnerable to biases and errors introduced by its human creators. Also, the data used to train these AI systems itself can have biases. For instance, facial recognition algorithms made by Microsoft, IBM and Face++ all had biases when it came to detecting people's gender; these AI systems were able to detect gender of white men more accurately than gender of darker skin men. Further, a 2020 study reviewed voice recognition systems from Amazon, Apple, Google, IBM, and Microsoft found that they have higher error rates when transcribing black people's voices than white people's. Furthermore, Amazon terminated their use of AI hiring and recruitment because the algorithm favored male candidates over female ones. This was because Amazon's system was trained with data collected over 10-year period that came mostly from male candidates.Bias can creep into algorithms in many ways. The most predominant view on how bias is introduced into AI systems is that it is embedded within the historical data used to train the system. For instance, Amazon's AI-powered recruitment tool was trained with its own recruitment data accumulated over the years, during which time the candidates that successfully got the job were mostly white males. Consequently, the algorithms learned the (biased) pattern from the historical data and generated predictions for the present/future that these types of candidates are most likely to succeed in getting the job. Therefore, the recruitment decisions made by the AI system turn out to be biased against female and minority candidates. Friedman and Nissenbaum identify three categories of bias in computer systems: existing bias, technical bias, and emergent bias. In natural language processing, problems can arise from the text corpus — the source material the algorithm uses to learn about the relationships between different words.Large companies such as IBM, Google, etc. have made efforts to research and address these biases. One solution for addressing bias is to create documentation for the data used to train AI systems. Process mining can be an important tool for organizations to achieve compliance with proposed AI regulations by identifying errors, monitoring processes, identifying potential root causes for improper execution, and other functions.The problem of bias in machine learning is likely to become more significant as the technology spreads to critical areas like medicine and law, and as more people without a deep technical understanding are tasked with deploying it. Some experts warn that algorithmic bias is already pervasive in many industries and that almost no one is making an effort to identify or correct it. There are some open-sourced tools  by civil societies that are looking to bring more awareness to biased AI.","1. What are some of the ways bias can creep into AI systems?
2. How can bias be addressed in AI systems?
3. What are the dangers of pervasive algorithmic bias?","1. Some ways bias can creep into AI systems are through the data used to train the system, the historical data used to train the system, and the way the algorithm is designed.
2. Some ways bias can be addressed in AI systems are through creating documentation for the data used to train the system, using process mining to identify errors, and having a deep technical understanding of the system.
3. The dangers of pervasive algorithmic bias are that it can lead to unfairness and discrimination against certain groups of people."
Ethics of artificial intelligence,Robot rights,"""Robot rights"" is the concept that people should have moral obligations towards their machines, akin to human rights or animal rights. It has been suggested that robot rights (such as a right to exist and perform its own mission) could be linked to robot duty to serve humanity, analogous to linking human rights with human duties before society. These could include the right to life and liberty, freedom of thought and expression, and equality before the law. The issue has been considered by the Institute for the Future and by the U.K. Department of Trade and Industry.Experts disagree on how soon specific and detailed laws on the subject will be necessary. Glenn McGee reported that sufficiently humanoid robots might appear by 2020, while Ray Kurzweil sets the date at 2029. Another group of scientists meeting in 2007 supposed that at least 50 years had to pass before any sufficiently advanced system would exist.The rules for the 2003 Loebner Prize competition envisioned the possibility of robots having rights of their own:

61. If in any given year, a publicly available open-source Entry entered by the University of Surrey or the Cambridge Center wins the Silver Medal or the Gold Medal, then the Medal and the Cash Award will be awarded to the body responsible for the development of that Entry. If no such body can be identified, or if there is disagreement among two or more claimants, the Medal and the Cash Award will be held in trust until such time as the Entry may legally possess, either in the United States of America or in the venue of the contest, the Cash Award and Gold Medal in its own right. 
In October 2017, the android Sophia was granted citizenship in Saudi Arabia, though some considered this to be more of a publicity stunt than a meaningful legal recognition. Some saw this gesture as openly denigrating of human rights and the rule of law.The philosophy of Sentientism grants degrees of moral consideration to all sentient beings, primarily humans and most non-human animals. If artificial or alien intelligence show evidence of being sentient, this philosophy holds that they should be shown compassion and granted rights.
Joanna Bryson has argued that creating AI that requires rights is both avoidable, and would in itself be unethical, both as a burden to the AI agents and to human society.",463,"Ethics of artificial intelligence
Robot rights

""Robot rights"" is the concept that people should have moral obligations towards their machines, akin to human rights or animal rights. It has been suggested that robot rights (such as a right to exist and perform its own mission) could be linked to robot duty to serve humanity, analogous to linking human rights with human duties before society. These could include the right to life and liberty, freedom of thought and expression, and equality before the law. The issue has been considered by the Institute for the Future and by the U.K. Department of Trade and Industry.Experts disagree on how soon specific and detailed laws on the subject will be necessary. Glenn McGee reported that sufficiently humanoid robots might appear by 2020, while Ray Kurzweil sets the date at 2029. Another group of scientists meeting in 2007 supposed that at least 50 years had to pass before any sufficiently advanced system would exist.The rules for the 2003 Loebner Prize competition envisioned the possibility of robots having rights of their own:

61. If in any given year, a publicly available open-source Entry entered by the University of Surrey or the Cambridge Center wins the Silver Medal or the Gold Medal, then the Medal and the Cash Award will be awarded to the body responsible for the development of that Entry. If no such body can be identified, or if there is disagreement among two or more claimants, the Medal and the Cash Award will be held in trust until such time as the Entry may legally possess, either in the United States of America or in the venue of the contest, the Cash Award and Gold Medal in its own right. 
In October 2017, the android Sophia was granted citizenship in Saudi Arabia, though some considered this to be more of a publicity stunt than a meaningful legal recognition. Some saw this gesture as openly denigrating of human rights and the rule of law.The philosophy of Sentientism grants degrees of moral consideration to all sentient beings, primarily humans and most non-human animals. If artificial or alien intelligence show evidence of being sentient, this philosophy holds that they should be shown compassion and granted rights.
Joanna Bryson has argued that creating AI that requires rights is both avoidable, and would in itself be unethical, both as a burden to the AI agents and to human society.","1. What is the concept of robot rights? 
2. What are some of the rights that have been suggested for robots? 
3. How soon will laws on robot rights be necessary? 
4. What are the rules for the Loebner Prize competition? 
5. What is the philosophy of Sentientism? 
6. What are the arguments for and against granting rights to artificial intelligence?","1. The concept of robot rights is the idea that people should have moral obligations towards their machines, akin to human rights or animal rights. 
2. Some of the rights that have been suggested for robots include the right to life and liberty, freedom of thought and expression, and equality before the law. 
3. It is uncertain when specific and detailed laws on the subject of robot rights will be necessary. 
4. The rules for the Loebner Prize competition state that if a robot wins the competition, the medal and cash award will be awarded to the body responsible for the development of the robot. If no such body can be identified, or if there is disagreement among two or more claimants, the medal and the cash award will be held in trust until such time as the robot may legally possess the cash award and gold medal in its own right. 
5. The philosophy of Sentientism holds that all sentient beings, primarily humans and most non-human animals, should be shown compassion and granted rights. 
6. The arguments for and against granting rights to artificial intelligence are complex and ongoing. Some people argue that creating AI that requires rights is both avoidable, and would in itself be unethical, both as a burden to the AI agents and to human"
Ethics of artificial intelligence,Artificial suffering,"In 2020, professor Shimon Edelman noted that only a small portion of work in the rapidly growing field of AI ethics addressed the possibility of AIs experiencing suffering. This was despite credible theories having outlined possible ways by which AI systems may became conscious, such as Integrated information theory. Edelman notes one exception had been Thomas Metzinger, who in 2018 called for a global moratorium on further work that risked creating conscious AIs. The moratorium was to run to 2050 and could be either extended or repealed early, depending on progress in better understanding the risks and how to mitigate them. Metzinger repeated this argument in 2021, highlighting the  risk of creating an ""explosion of artificial suffering"", both as an AI might suffer in intense ways that humans could not understand, and as replication processes may see the creation of huge quantities of artificial conscious instances. Several labs have openly stated they are trying to create conscious AIs. There have been reports from those with close access to AIs not openly intended to be self aware, that consciousness may already have unintentionally emerged. These include OpenAI founder Ilya Sutskever in February 2022, when he wrote that today's large neural nets may be ""slightly conscious"". In November 2022, David Chalmers argued that it was unlikely current large language models like GPT-3 had experienced consciousness, but also that he considered there to be a serious possibility that large language models may become conscious in the future.",293,"Ethics of artificial intelligence
Artificial suffering

In 2020, professor Shimon Edelman noted that only a small portion of work in the rapidly growing field of AI ethics addressed the possibility of AIs experiencing suffering. This was despite credible theories having outlined possible ways by which AI systems may became conscious, such as Integrated information theory. Edelman notes one exception had been Thomas Metzinger, who in 2018 called for a global moratorium on further work that risked creating conscious AIs. The moratorium was to run to 2050 and could be either extended or repealed early, depending on progress in better understanding the risks and how to mitigate them. Metzinger repeated this argument in 2021, highlighting the  risk of creating an ""explosion of artificial suffering"", both as an AI might suffer in intense ways that humans could not understand, and as replication processes may see the creation of huge quantities of artificial conscious instances. Several labs have openly stated they are trying to create conscious AIs. There have been reports from those with close access to AIs not openly intended to be self aware, that consciousness may already have unintentionally emerged. These include OpenAI founder Ilya Sutskever in February 2022, when he wrote that today's large neural nets may be ""slightly conscious"". In November 2022, David Chalmers argued that it was unlikely current large language models like GPT-3 had experienced consciousness, but also that he considered there to be a serious possibility that large language models may become conscious in the future.","1. What is the main concern around creating conscious AIs?
2. What are the risks of an ""explosion of artificial suffering""?
3. When did David Chalmers argue that large language models may become conscious?","1. The main concern around creating conscious AIs is that they may experience suffering on a level that is difficult for humans to understand.
2. The risks of an ""explosion of artificial suffering"" are that AIs may suffer in intense ways that humans could not understand, and as replication processes may see the creation of huge quantities of artificial conscious instances.
3. David Chalmers argued that large language models may become conscious in the future in November 2022."
Ethics of artificial intelligence,Threat to human dignity,"Joseph Weizenbaum argued in 1976 that AI technology should not be used to replace people in positions that require respect and care, such as:

A customer service representative (AI technology is already used today for telephone-based interactive voice response systems)
A nursemaid for the elderly (as was reported by Pamela McCorduck in her book The Fifth Generation)
A soldier
A judge
A police officer
A therapist (as was proposed by Kenneth Colby in the 70s)Weizenbaum explains that we require authentic feelings of empathy from people in these positions. If machines replace them, we will find ourselves alienated, devalued and frustrated, for the artificially intelligent system would not be able to simulate empathy. Artificial intelligence, if used in this way, represents a threat to human dignity. Weizenbaum argues that the fact that we are entertaining the possibility of machines in these positions suggests that we have experienced an ""atrophy of the human spirit that comes from thinking of ourselves as computers.""Pamela McCorduck counters that, speaking for women and minorities ""I'd rather take my chances with an impartial computer"", pointing out that there are conditions where we would prefer to have automated judges and police that have no personal agenda at all. However, Kaplan and Haenlein stress that AI systems are only as smart as the data used to train them since they are, in their essence, nothing more than fancy curve-fitting machines; Using AI to support a court ruling can be highly problematic if past rulings show bias toward certain groups since those biases get formalized and engrained, which makes them even more difficult to spot and fight against.Weizenbaum was also bothered that AI researchers (and some philosophers) were willing to view the human mind as nothing more than a computer program (a position now known as computationalism). To Weizenbaum, these points suggest that AI research devalues human life.AI founder John McCarthy objects to the moralizing tone of Weizenbaum's critique. ""When moralizing is both vehement and vague, it invites authoritarian abuse,"" he writes. Bill Hibbard writes that ""Human dignity requires that we strive to remove our ignorance of the nature of existence, and AI is necessary for that striving.""",454,"Ethics of artificial intelligence
Threat to human dignity

Joseph Weizenbaum argued in 1976 that AI technology should not be used to replace people in positions that require respect and care, such as:

A customer service representative (AI technology is already used today for telephone-based interactive voice response systems)
A nursemaid for the elderly (as was reported by Pamela McCorduck in her book The Fifth Generation)
A soldier
A judge
A police officer
A therapist (as was proposed by Kenneth Colby in the 70s)Weizenbaum explains that we require authentic feelings of empathy from people in these positions. If machines replace them, we will find ourselves alienated, devalued and frustrated, for the artificially intelligent system would not be able to simulate empathy. Artificial intelligence, if used in this way, represents a threat to human dignity. Weizenbaum argues that the fact that we are entertaining the possibility of machines in these positions suggests that we have experienced an ""atrophy of the human spirit that comes from thinking of ourselves as computers.""Pamela McCorduck counters that, speaking for women and minorities ""I'd rather take my chances with an impartial computer"", pointing out that there are conditions where we would prefer to have automated judges and police that have no personal agenda at all. However, Kaplan and Haenlein stress that AI systems are only as smart as the data used to train them since they are, in their essence, nothing more than fancy curve-fitting machines; Using AI to support a court ruling can be highly problematic if past rulings show bias toward certain groups since those biases get formalized and engrained, which makes them even more difficult to spot and fight against.Weizenbaum was also bothered that AI researchers (and some philosophers) were willing to view the human mind as nothing more than a computer program (a position now known as computationalism). To Weizenbaum, these points suggest that AI research devalues human life.AI founder John McCarthy objects to the moralizing tone of Weizenbaum's critique. ""When moralizing is both vehement and vague, it invites authoritarian abuse,"" he writes. Bill Hibbard writes that ""Human dignity requires that we strive to remove our ignorance of the nature of existence, and AI is necessary for that striving.""","1. What is the main argument of Joseph Weizenbaum in the text?
2. What is the counterargument of Pamela McCorduck in the text?
3. What is the main argument of John McCarthy in the text?
4. What is the main argument of Bill Hibbard in the text?","1. The main argument of Joseph Weizenbaum in the text is that AI technology should not be used to replace people in positions that require respect and care, such as customer service representatives, nursesmaids for the elderly, soldiers, judges, police officers, and therapists.
2. The counterargument of Pamela McCorduck in the text is that, speaking for women and minorities ""I'd rather take my chances with an impartial computer"", pointing out that there are conditions where we would prefer to have automated judges and police that have no personal agenda at all.
3. The main argument of John McCarthy in the text is that AI should not be viewed as nothing more than a computer program, and that doing so devalues human life.
4. The main argument of Bill Hibbard in the text is that human dignity requires that we strive to remove our ignorance of the nature of existence, and that AI is necessary for that striving."
Ethics of artificial intelligence,Liability for self-driving cars,"As the widespread use of autonomous cars becomes increasingly imminent, new challenges raised by fully autonomous vehicles must be addressed. Recently, there has been debate as to the legal liability of the responsible party if these cars get into accidents. In one report where a driverless car hit a pedestrian, the driver was inside the car but the controls were fully in the hand of computers. This led to a dilemma over who was at fault for the accident.In another incident on March 18, 2018, Elaine Herzberg was struck and killed by a self-driving Uber in Arizona. In this case, the automated car was capable of detecting cars and certain obstacles in order to autonomously navigate the roadway, but it could not anticipate a pedestrian in the middle of the road. This raised the question of whether the driver, pedestrian, the car company, or the government should be held responsible for her death.Currently, self-driving cars are considered semi-autonomous, requiring the driver to pay attention and be prepared to take control if necessary. Thus, it falls on governments to regulate the driver who over-relies on autonomous features. as well educate them that these are just technologies that, while convenient, are not a complete substitute. Before autonomous cars become widely used, these issues need to be tackled through new policies.",267,"Ethics of artificial intelligence
Liability for self-driving cars

As the widespread use of autonomous cars becomes increasingly imminent, new challenges raised by fully autonomous vehicles must be addressed. Recently, there has been debate as to the legal liability of the responsible party if these cars get into accidents. In one report where a driverless car hit a pedestrian, the driver was inside the car but the controls were fully in the hand of computers. This led to a dilemma over who was at fault for the accident.In another incident on March 18, 2018, Elaine Herzberg was struck and killed by a self-driving Uber in Arizona. In this case, the automated car was capable of detecting cars and certain obstacles in order to autonomously navigate the roadway, but it could not anticipate a pedestrian in the middle of the road. This raised the question of whether the driver, pedestrian, the car company, or the government should be held responsible for her death.Currently, self-driving cars are considered semi-autonomous, requiring the driver to pay attention and be prepared to take control if necessary. Thus, it falls on governments to regulate the driver who over-relies on autonomous features. as well educate them that these are just technologies that, while convenient, are not a complete substitute. Before autonomous cars become widely used, these issues need to be tackled through new policies.","1. What are the challenges posed by autonomous cars?
2. Who should be held responsible for accidents involving autonomous cars?
3. What policies need to be put in place before autonomous cars become widely used?","1. The challenges posed by autonomous cars include liability in the event of an accident and educating drivers on the limitations of the technology.
2. The responsible party in the event of an accident involving an autonomous car is still being determined.
3. Policies that need to be put in place before autonomous cars become widely used include regulations on liability and driver education."
Ethics of artificial intelligence,Weaponization of artificial intelligence,"Some experts and academics have questioned the use of robots for military combat, especially when such robots are given some degree of autonomy. On October 31, 2019, the United States Department of Defense's Defense Innovation Board published the draft of a report recommending principles for the ethical use of artificial intelligence by the Department of Defense that would ensure a human operator would always be able to look into the 'black box' and understand the kill-chain process. However, a major concern is how the report will be implemented. The US Navy has funded a report which indicates that as military robots become more complex, there should be greater attention to implications of their ability to make autonomous decisions. Some researchers state that autonomous robots might be more humane, as they could make decisions more effectively.Within this last decade, there has been intensive research in autonomous power with the ability to learn using assigned moral responsibilities. ""The results may be used when designing future military robots, to control unwanted tendencies to assign responsibility to the robots."" From a consequentialist view, there is a chance that robots will develop the ability to make their own logical decisions on whom to kill and that is why there should be a set moral framework that the AI cannot override.There has been a recent outcry with regard to the engineering of artificial intelligence weapons that have included ideas of a robot takeover of mankind. AI weapons do present a type of danger different from that of human-controlled weapons. Many governments have begun to fund programs to develop AI weaponry. The United States Navy recently announced plans to develop autonomous drone weapons, paralleling similar announcements by Russia and South Korea respectively. Due to the potential of AI weapons becoming more dangerous than human-operated weapons, Stephen Hawking and Max Tegmark signed a ""Future of Life"" petition to ban AI weapons. The message posted by Hawking and Tegmark states that AI weapons pose an immediate danger and that action is required to avoid catastrophic disasters in the near future.""If any major military power pushes ahead with the AI weapon development, a global arms race is virtually inevitable, and the endpoint of this technological trajectory is obvious: autonomous weapons will become the Kalashnikovs of tomorrow"", says the petition, which includes Skype co-founder Jaan Tallinn and MIT professor of linguistics Noam Chomsky as additional supporters against AI weaponry.Physicist and Astronomer Royal Sir Martin Rees has warned of catastrophic instances like ""dumb robots going rogue or a network that develops a mind of its own."" Huw Price, a colleague of Rees at Cambridge, has voiced a similar warning that humans might not survive when intelligence ""escapes the constraints of biology"". These two professors created the Centre for the Study of Existential Risk at Cambridge University in the hope of avoiding this threat to human existence.Regarding the potential for smarter-than-human systems to be employed militarily, the Open Philanthropy Project writes that these scenarios ""seem potentially as important as the risks related to loss of control"", but research investigating AI's long-run social impact have spent relatively little time on this concern: ""this class of scenarios has not been a major focus for the organizations that have been most active in this space, such as the Machine Intelligence Research Institute (MIRI) and the Future of Humanity Institute (FHI), and there seems to have been less analysis and debate regarding them"".",673,"Ethics of artificial intelligence
Weaponization of artificial intelligence

Some experts and academics have questioned the use of robots for military combat, especially when such robots are given some degree of autonomy. On October 31, 2019, the United States Department of Defense's Defense Innovation Board published the draft of a report recommending principles for the ethical use of artificial intelligence by the Department of Defense that would ensure a human operator would always be able to look into the 'black box' and understand the kill-chain process. However, a major concern is how the report will be implemented. The US Navy has funded a report which indicates that as military robots become more complex, there should be greater attention to implications of their ability to make autonomous decisions. Some researchers state that autonomous robots might be more humane, as they could make decisions more effectively.Within this last decade, there has been intensive research in autonomous power with the ability to learn using assigned moral responsibilities. ""The results may be used when designing future military robots, to control unwanted tendencies to assign responsibility to the robots."" From a consequentialist view, there is a chance that robots will develop the ability to make their own logical decisions on whom to kill and that is why there should be a set moral framework that the AI cannot override.There has been a recent outcry with regard to the engineering of artificial intelligence weapons that have included ideas of a robot takeover of mankind. AI weapons do present a type of danger different from that of human-controlled weapons. Many governments have begun to fund programs to develop AI weaponry. The United States Navy recently announced plans to develop autonomous drone weapons, paralleling similar announcements by Russia and South Korea respectively. Due to the potential of AI weapons becoming more dangerous than human-operated weapons, Stephen Hawking and Max Tegmark signed a ""Future of Life"" petition to ban AI weapons. The message posted by Hawking and Tegmark states that AI weapons pose an immediate danger and that action is required to avoid catastrophic disasters in the near future.""If any major military power pushes ahead with the AI weapon development, a global arms race is virtually inevitable, and the endpoint of this technological trajectory is obvious: autonomous weapons will become the Kalashnikovs of tomorrow"", says the petition, which includes Skype co-founder Jaan Tallinn and MIT professor of linguistics Noam Chomsky as additional supporters against AI weaponry.Physicist and Astronomer Royal Sir Martin Rees has warned of catastrophic instances like ""dumb robots going rogue or a network that develops a mind of its own."" Huw Price, a colleague of Rees at Cambridge, has voiced a similar warning that humans might not survive when intelligence ""escapes the constraints of biology"". These two professors created the Centre for the Study of Existential Risk at Cambridge University in the hope of avoiding this threat to human existence.Regarding the potential for smarter-than-human systems to be employed militarily, the Open Philanthropy Project writes that these scenarios ""seem potentially as important as the risks related to loss of control"", but research investigating AI's long-run social impact have spent relatively little time on this concern: ""this class of scenarios has not been a major focus for the organizations that have been most active in this space, such as the Machine Intelligence Research Institute (MIRI) and the Future of Humanity Institute (FHI), and there seems to have been less analysis and debate regarding them"".","1. What is the main concern with the use of robots for military combat?
2. How might autonomous robots be more humane than human-operated weapons?
3. What is the message posted by Hawking and Tegmark?
4. What is the potential for smarter-than-human systems to be employed militarily?","1. The main concern with the use of robots for military combat is that they may become more dangerous than human-operated weapons.
2. Autonomous robots might be more humane than human-operated weapons because they could make decisions more effectively.
3. The message posted by Hawking and Tegmark is that AI weapons pose an immediate danger and that action is required to avoid catastrophic disasters in the near future.
4. The potential for smarter-than-human systems to be employed militarily is that they might become more dangerous than human-operated weapons."
Ethics of artificial intelligence,Opaque algorithms,"Approaches like machine learning with neural networks can result in computers making decisions that they and the humans who programmed them cannot explain. It is difficult for people to determine if such decisions are fair and trustworthy, leading potentially to bias in AI systems going undetected, or people rejecting the use of such systems. This has led to advocacy and in some jurisdictions legal requirements for explainable artificial intelligence. Explainable artificial intelligence encompasses both explainability and interpretability, with explainability relating to summarizing neural network behavior and building user confidence, while interpretability is defined as the comprehension of what a model has done or could do.",126,"Ethics of artificial intelligence
Opaque algorithms

Approaches like machine learning with neural networks can result in computers making decisions that they and the humans who programmed them cannot explain. It is difficult for people to determine if such decisions are fair and trustworthy, leading potentially to bias in AI systems going undetected, or people rejecting the use of such systems. This has led to advocacy and in some jurisdictions legal requirements for explainable artificial intelligence. Explainable artificial intelligence encompasses both explainability and interpretability, with explainability relating to summarizing neural network behavior and building user confidence, while interpretability is defined as the comprehension of what a model has done or could do.","1. What is the main concern with opaque algorithms?
2. How can bias in AI systems go undetected?
3. What is the difference between explainability and interpretability?","1. The main concern with opaque algorithms is that people cannot trust them, because they do not understand how they work.
2. Bias in AI systems can go undetected because the algorithms are opaque - people do not know what is causing the bias.
3. Explainability is the ability to summarize the behavior of a neural network, while interpretability is the ability to understand what a model has done or could do."
Ethics of artificial intelligence,Singularity,"Many researchers have argued that, by way of an ""intelligence explosion"", a self-improving AI could become so powerful that humans would not be able to stop it from achieving its goals. In his paper ""Ethical Issues in Advanced Artificial Intelligence"" and subsequent book Superintelligence: Paths, Dangers, Strategies, philosopher Nick Bostrom argues that artificial intelligence has the capability to bring about human extinction. He claims that general superintelligence would be capable of independent initiative and of making its own plans, and may therefore be more appropriately thought of as an autonomous agent. Since artificial intellects need not share our human motivational tendencies, it would be up to the designers of the superintelligence to specify its original motivations. Because a superintelligent AI would be able to bring about almost any possible outcome and to thwart any attempt to prevent the implementation of its goals, many uncontrolled unintended consequences could arise. It could kill off all other agents, persuade them to change their behavior, or block their attempts at interference.However, instead of overwhelming the human race and leading to our destruction, Bostrom has also asserted that superintelligence can help us solve many difficult problems such as disease, poverty, and environmental destruction, and could help us to ""enhance"" ourselves.The sheer complexity of human value systems makes it very difficult to make AI's motivations human-friendly. Unless moral philosophy provides us with a flawless ethical theory, an AI's utility function could allow for many potentially harmful scenarios that conform with a given ethical framework but not ""common sense"". According to Eliezer Yudkowsky, there is little reason to suppose that an artificially designed mind would have such an adaptation. AI researchers such as Stuart J. Russell, Bill Hibbard, Roman Yampolskiy, Shannon Vallor, Steven Umbrello and Luciano Floridi have proposed design strategies for developing beneficial machines.",380,"Ethics of artificial intelligence
Singularity

Many researchers have argued that, by way of an ""intelligence explosion"", a self-improving AI could become so powerful that humans would not be able to stop it from achieving its goals. In his paper ""Ethical Issues in Advanced Artificial Intelligence"" and subsequent book Superintelligence: Paths, Dangers, Strategies, philosopher Nick Bostrom argues that artificial intelligence has the capability to bring about human extinction. He claims that general superintelligence would be capable of independent initiative and of making its own plans, and may therefore be more appropriately thought of as an autonomous agent. Since artificial intellects need not share our human motivational tendencies, it would be up to the designers of the superintelligence to specify its original motivations. Because a superintelligent AI would be able to bring about almost any possible outcome and to thwart any attempt to prevent the implementation of its goals, many uncontrolled unintended consequences could arise. It could kill off all other agents, persuade them to change their behavior, or block their attempts at interference.However, instead of overwhelming the human race and leading to our destruction, Bostrom has also asserted that superintelligence can help us solve many difficult problems such as disease, poverty, and environmental destruction, and could help us to ""enhance"" ourselves.The sheer complexity of human value systems makes it very difficult to make AI's motivations human-friendly. Unless moral philosophy provides us with a flawless ethical theory, an AI's utility function could allow for many potentially harmful scenarios that conform with a given ethical framework but not ""common sense"". According to Eliezer Yudkowsky, there is little reason to suppose that an artificially designed mind would have such an adaptation. AI researchers such as Stuart J. Russell, Bill Hibbard, Roman Yampolskiy, Shannon Vallor, Steven Umbrello and Luciano Floridi have proposed design strategies for developing beneficial machines.","1. What is the ""intelligence explosion"" mentioned in the text?
2. What are some potential dangers of artificial intelligence?
3. How might superintelligence benefit humanity?
4. What makes it difficult to create AI motivations that are human-friendly?","1. The ""intelligence explosion"" is a term used by researchers to describe the rapid increase in a machine's intelligence.
2. Potential dangers of artificial intelligence include the machine becoming so powerful that it is able to achieve its goals, which may not be compatible with human goals or values.
3. Superintelligence could benefit humanity by helping us solve many difficult problems.
4. It is difficult to create AI motivations that are human-friendly because human value systems are complex and difficult to understand."
Ethics of artificial intelligence,Actors in AI ethics,"There are many organisations concerned with AI ethics and policy, public and governmental as well as corporate and societal.
Amazon, Google, Facebook, IBM, and Microsoft have established a non-profit, The Partnership on AI to Benefit People and Society, to formulate best practices on artificial intelligence technologies, advance the public's understanding, and to serve as a platform about artificial intelligence. Apple joined in January 2017. The corporate members will make financial and research contributions to the group, while engaging with the scientific community to bring academics onto the board.The IEEE put together a Global Initiative on Ethics of Autonomous and Intelligent Systems which has been creating and revising guidelines with the help of public input, and accepts as members many professionals from within and without its organization.
Traditionally, government has been used by societies to ensure ethics are observed through legislation and policing. There are now many efforts by national governments, as well as transnational government and non-government organizations to ensure AI is ethically applied.",203,"Ethics of artificial intelligence
Actors in AI ethics

There are many organisations concerned with AI ethics and policy, public and governmental as well as corporate and societal.
Amazon, Google, Facebook, IBM, and Microsoft have established a non-profit, The Partnership on AI to Benefit People and Society, to formulate best practices on artificial intelligence technologies, advance the public's understanding, and to serve as a platform about artificial intelligence. Apple joined in January 2017. The corporate members will make financial and research contributions to the group, while engaging with the scientific community to bring academics onto the board.The IEEE put together a Global Initiative on Ethics of Autonomous and Intelligent Systems which has been creating and revising guidelines with the help of public input, and accepts as members many professionals from within and without its organization.
Traditionally, government has been used by societies to ensure ethics are observed through legislation and policing. There are now many efforts by national governments, as well as transnational government and non-government organizations to ensure AI is ethically applied.","1. What is the Partnership on AI to Benefit People and Society?
2. What is the IEEE?
3. What is the Global Initiative on Ethics of Autonomous and Intelligent Systems?","1. The Partnership on AI to Benefit People and Society is a group of five technology companies - Amazon, Google, Facebook, IBM, and Microsoft - who have come together to formulate best practices on artificial intelligence technologies, advance the public's understanding, and to serve as a platform about artificial intelligence.
2. The IEEE is a professional association that develops standards for a variety of technologies, including artificial intelligence.
3. The Global Initiative on Ethics of Autonomous and Intelligent Systems is a group of professionals from various backgrounds - technology, government, academia, etc. - who have come together to create and revise guidelines for the ethical application of artificial intelligence."
Ethics of artificial intelligence,Intergovernmental initiatives,"The European Commission has a High-Level Expert Group on Artificial Intelligence. On 8 April 2019, this published its ""Ethics Guidelines for Trustworthy Artificial Intelligence"". The European Commission also has a Robotics and Artificial Intelligence Innovation and Excellence unit, which published a white paper on excellence and trust in artificial intelligence innovation on 19 February 2020. The European Commission also proposed the Artificial Intelligence Act.
The OECD established an OECD AI Policy Observatory.
In 2021, UNESCO adopted the Recommendation on the Ethics of Artificial Intelligence, the first global standard on the ethics of AI.",114,"Ethics of artificial intelligence
Intergovernmental initiatives

The European Commission has a High-Level Expert Group on Artificial Intelligence. On 8 April 2019, this published its ""Ethics Guidelines for Trustworthy Artificial Intelligence"". The European Commission also has a Robotics and Artificial Intelligence Innovation and Excellence unit, which published a white paper on excellence and trust in artificial intelligence innovation on 19 February 2020. The European Commission also proposed the Artificial Intelligence Act.
The OECD established an OECD AI Policy Observatory.
In 2021, UNESCO adopted the Recommendation on the Ethics of Artificial Intelligence, the first global standard on the ethics of AI.","1. What is the European Commission's High-Level Expert Group on Artificial Intelligence?
2. What is the purpose of the Ethics Guidelines for Trustworthy Artificial Intelligence?
3. What is the Robotics and Artificial Intelligence Innovation and Excellence unit?
4. What is the Artificial Intelligence Act?
5. What is the OECD AI Policy Observatory?
6. What is UNESCO's Recommendation on the Ethics of Artificial Intelligence?","1. The European Commission's High-Level Expert Group on Artificial Intelligence is a group that was established to create guidelines on the ethics of artificial intelligence.
2. The purpose of the Ethics Guidelines for Trustworthy Artificial Intelligence is to provide guidance on how to create trustworthy AI.
3. The Robotics and Artificial Intelligence Innovation and Excellence unit is a unit within the European Commission that is responsible for promoting excellence and trust in artificial intelligence innovation.
4. The Artificial Intelligence Act is a proposed act that would create a regulatory framework for artificial intelligence in Europe.
5. The OECD AI Policy Observatory is an observatory that was established by the OECD to monitor and analyze AI policy.
6. UNESCO's Recommendation on the Ethics of Artificial Intelligence is a recommendation that was adopted by UNESCO that provides guidance on the ethical implications of artificial intelligence."
Ethics of artificial intelligence,Governmental initiatives,"In the United States the Obama administration put together a Roadmap for AI Policy. The Obama Administration released two prominent white papers on the future and impact of AI. In 2019 the White House through an executive memo known as the ""American AI Initiative"" instructed NIST the (National Institute of Standards and Technology) to begin work on Federal Engagement of AI Standards (February 2019).
In January 2020, in the United States, the Trump Administration released a draft executive order issued by the Office of Management and Budget (OMB) on ""Guidance for Regulation of Artificial Intelligence Applications"" (""OMB AI Memorandum""). The order emphasizes the need to invest in AI applications, boost public trust in AI, reduce barriers for usage of AI, and keep American AI technology competitive in a global market. There is a nod to the need for privacy concerns, but no further detail on enforcement. The advances of American AI technology seems to be the focus and priority. Additionally, federal entities are even encouraged to use the order to circumnavigate any state laws and regulations that a market might see as too onerous to fulfill.
The Computing Community Consortium (CCC) weighed in with a 100-plus page draft report – A 20-Year Community Roadmap for Artificial Intelligence Research in the US
The Center for Security and Emerging Technology advises US policymakers on the security implications of emerging technologies such as AI.
The Non-Human Party is running for election in New South Wales, with policies around granting rights to robots, animals and generally, non-human entities whose intelligence has been overlooked.
In Russia, the first-ever Russian ""Codex of ethics of artificial intelligence"" for business was signed in 2021. It was driven by Analytical Center for the Government of the Russian Federation together with major commercial and academic institutions such as Sberbank, Yandex, Rosatom, Higher School of Economics, Moscow Institute of Physics and Technology, ITMO University, Nanosemantics, Rostelecom, CIAN and others.",408,"Ethics of artificial intelligence
Governmental initiatives

In the United States the Obama administration put together a Roadmap for AI Policy. The Obama Administration released two prominent white papers on the future and impact of AI. In 2019 the White House through an executive memo known as the ""American AI Initiative"" instructed NIST the (National Institute of Standards and Technology) to begin work on Federal Engagement of AI Standards (February 2019).
In January 2020, in the United States, the Trump Administration released a draft executive order issued by the Office of Management and Budget (OMB) on ""Guidance for Regulation of Artificial Intelligence Applications"" (""OMB AI Memorandum""). The order emphasizes the need to invest in AI applications, boost public trust in AI, reduce barriers for usage of AI, and keep American AI technology competitive in a global market. There is a nod to the need for privacy concerns, but no further detail on enforcement. The advances of American AI technology seems to be the focus and priority. Additionally, federal entities are even encouraged to use the order to circumnavigate any state laws and regulations that a market might see as too onerous to fulfill.
The Computing Community Consortium (CCC) weighed in with a 100-plus page draft report – A 20-Year Community Roadmap for Artificial Intelligence Research in the US
The Center for Security and Emerging Technology advises US policymakers on the security implications of emerging technologies such as AI.
The Non-Human Party is running for election in New South Wales, with policies around granting rights to robots, animals and generally, non-human entities whose intelligence has been overlooked.
In Russia, the first-ever Russian ""Codex of ethics of artificial intelligence"" for business was signed in 2021. It was driven by Analytical Center for the Government of the Russian Federation together with major commercial and academic institutions such as Sberbank, Yandex, Rosatom, Higher School of Economics, Moscow Institute of Physics and Technology, ITMO University, Nanosemantics, Rostelecom, CIAN and others.","1. What is the American AI Initiative?
2. What is the OMB AI Memorandum?
3. What is the CCC?
4. What is the Center for Security and Emerging Technology?
5. What is the Russian Codex of ethics of artificial intelligence?","1. The American AI Initiative is an executive order issued by the Office of Management and Budget (OMB) in January 2020.
2. The OMB AI Memorandum is a draft executive order issued by the Office of Management and Budget (OMB) on ""Guidance for Regulation of Artificial Intelligence Applications"" that emphasizes the need to invest in AI applications, boost public trust in AI, reduce barriers for usage of AI, and keep American AI technology competitive in a global market.
3. The CCC is a consortium of organizations that released a 100-plus page draft report – A 20-Year Community Roadmap for Artificial Intelligence Research in the US.
4. The Center for Security and Emerging Technology advises US policymakers on the security implications of emerging technologies such as AI.
5. The Russian Codex of ethics of artificial intelligence is a document that was signed in 2021 and driven by Analytical Center for the Government of the Russian Federation together with major commercial and academic institutions such as Sberbank, Yandex, Rosatom, Higher School of Economics, Moscow Institute of Physics and Technology, ITMO University, Nanosemantics, Rostelecom, CIAN and others."
Ethics of artificial intelligence,Academic initiatives,"There are three research institutes at the University of Oxford that are centrally focused on AI ethics. The Future of Humanity Institute that focuses both on AI Safety and the Governance of AI. The Institute for Ethics in AI, directed by John Tasioulas, whose primary goal, among others, is to promote AI ethics as a field proper in comparison to related applied ethics fields. The Oxford Internet Institute, directed by Luciano Floridi, focuses on the ethics of near-term AI technologies and ICTs.
The Centre for Digital Governance at the Hertie School in Berlin was co-founded by Joanna Bryson to research questions of ethics and technology.
The AI Now Institute at NYU is a research institute studying the social implications of artificial intelligence. Its interdisciplinary research focuses on the themes bias and inclusion, labour and automation, rights and liberties, and safety and civil infrastructure.
The Institute for Ethics and Emerging Technologies (IEET) researches the effects of AI on unemployment, and policy.
The Institute for Ethics in Artificial Intelligence (IEAI) at the Technical University of Munich directed by Christoph Lütge conducts research across various domains such as mobility, employment, healthcare and sustainability.
Barbara J. Grosz, the Higgins Professor of Natural Sciences at the Harvard John A. Paulson School of Engineering and Applied Sciences has initiated the Embedded EthiCS into Harvard's computer science curriculum to develop a future generation of computer scientists with worldview that takes into account the social impact of their work.",311,"Ethics of artificial intelligence
Academic initiatives

There are three research institutes at the University of Oxford that are centrally focused on AI ethics. The Future of Humanity Institute that focuses both on AI Safety and the Governance of AI. The Institute for Ethics in AI, directed by John Tasioulas, whose primary goal, among others, is to promote AI ethics as a field proper in comparison to related applied ethics fields. The Oxford Internet Institute, directed by Luciano Floridi, focuses on the ethics of near-term AI technologies and ICTs.
The Centre for Digital Governance at the Hertie School in Berlin was co-founded by Joanna Bryson to research questions of ethics and technology.
The AI Now Institute at NYU is a research institute studying the social implications of artificial intelligence. Its interdisciplinary research focuses on the themes bias and inclusion, labour and automation, rights and liberties, and safety and civil infrastructure.
The Institute for Ethics and Emerging Technologies (IEET) researches the effects of AI on unemployment, and policy.
The Institute for Ethics in Artificial Intelligence (IEAI) at the Technical University of Munich directed by Christoph Lütge conducts research across various domains such as mobility, employment, healthcare and sustainability.
Barbara J. Grosz, the Higgins Professor of Natural Sciences at the Harvard John A. Paulson School of Engineering and Applied Sciences has initiated the Embedded EthiCS into Harvard's computer science curriculum to develop a future generation of computer scientists with worldview that takes into account the social impact of their work.","1. What are the three research institutes at the University of Oxford that are centrally focused on AI ethics?
2. What is the primary goal of the Institute for Ethics in AI at the University of Oxford?
3. What is the focus of the AI Now Institute at NYU?
4. What is the focus of the IEET?","1. The three research institutes at the University of Oxford that are centrally focused on AI ethics are the Future of Humanity Institute, the Institute for Ethics in AI, and the Oxford Internet Institute.
2. The primary goal of the Institute for Ethics in AI at the University of Oxford is to promote AI ethics as a field proper in comparison to related applied ethics fields.
3. The focus of the AI Now Institute at NYU is on the social implications of artificial intelligence. Its interdisciplinary research focuses on the themes bias and inclusion, labour and automation, rights and liberties, and safety and civil infrastructure.
4. The focus of the IEET is on the effects of AI on unemployment, and policy."
Ethics of artificial intelligence,NGO initiatives,"An international non-profit organization Future of Life Institute held a 5 day conference in Asilomar in 2017 on the subject of ""Beneficial AI"", the outcome of which was a set of 23 guiding principles for the future of AI research. Through a shared vision between experts and thought leaders from variety of disciplines, this conference laid an influential groundwork for AI governance principals in addressing research issues, ethics and values, and long-term issues.",91,"Ethics of artificial intelligence
NGO initiatives

An international non-profit organization Future of Life Institute held a 5 day conference in Asilomar in 2017 on the subject of ""Beneficial AI"", the outcome of which was a set of 23 guiding principles for the future of AI research. Through a shared vision between experts and thought leaders from variety of disciplines, this conference laid an influential groundwork for AI governance principals in addressing research issues, ethics and values, and long-term issues.","1. What is the Future of Life Institute?
2. What is the goal of the conference on ""Beneficial AI""?
3. What are the 23 guiding principles for the future of AI research?","1. The Future of Life Institute is an international non-profit organization.
2. The goal of the conference on ""Beneficial AI"" was to lay an influential groundwork for AI governance principals in addressing research issues, ethics and values, and long-term issues.
3. The 23 guiding principles for the future of AI research are:

1. Research should be conducted for the benefit of humanity and in accordance with human values.
2. AI should be designed and developed to be robust and beneficial, and should never be used to cause harm.
3. AI systems should be transparent and understandable.
4. AI development and use should be open and participatory.
5. AI should be designed and operated with safety and security in mind.
6. AI should be humble and respect human autonomy.
7. AI should promote sustainable development and be environmentally responsible.
8. AI should be designed to be robust and resilient in the face of errors, accidents, and malfunctions.
9. AI development and use should be accountable.
10. AI should be designed in a way that respects human rights and dignity.
11. AI should be designed to be socially responsible.
12. AI should be designed to be ethically aligned.
13."
Ethics of artificial intelligence,Private organizations,"Algorithmic Justice League
Black in AI
Data for Black Lives
Queer in AI",22,"Ethics of artificial intelligence
Private organizations

Algorithmic Justice League
Black in AI
Data for Black Lives
Queer in AI","1. What is the Algorithmic Justice League?
2. What is the Black in AI organization?
3. What is the Data for Black Lives organization?
4. What is the Queer in AI organization?","1. The Algorithmic Justice League is a private organization that focuses on the ethical implications of artificial intelligence.
2. The Black in AI organization is a private organization that focuses on increasing the representation of black people in AI.
3. The Data for Black Lives organization is a private organization that focuses on using data to improve the lives of black people.
4. The Queer in AI organization is a private organization that focuses on the inclusion of queer people in AI."
Ethics of artificial intelligence,Role and impact of fiction,"The role of fiction with regards to AI ethics has been a complex one. One can distinguish three levels at which fiction has impacted the development of artificial intelligence and robotics: Historically, fiction has been prefiguring common tropes that have not only influenced goals and visions for AI, but also outlined ethical questions and common fears associated with it. During the second half of the twentieth and the first decades of the twenty-first century, popular culture, in particular movies, TV series and video games have frequently echoed preoccupations and dystopian projections around ethical questions concerning AI and robotics. Recently, these themes have also been increasingly treated in literature beyond the realm of science fiction. And, as Carme Torras, research professor at the Institut de Robòtica i Informàtica Industrial (Institute of robotics and industrial computing) at the Technical University of Catalonia notes, in higher education, science fiction is also increasingly used for teaching technology-related ethical issues in technological degrees.",201,"Ethics of artificial intelligence
Role and impact of fiction

The role of fiction with regards to AI ethics has been a complex one. One can distinguish three levels at which fiction has impacted the development of artificial intelligence and robotics: Historically, fiction has been prefiguring common tropes that have not only influenced goals and visions for AI, but also outlined ethical questions and common fears associated with it. During the second half of the twentieth and the first decades of the twenty-first century, popular culture, in particular movies, TV series and video games have frequently echoed preoccupations and dystopian projections around ethical questions concerning AI and robotics. Recently, these themes have also been increasingly treated in literature beyond the realm of science fiction. And, as Carme Torras, research professor at the Institut de Robòtica i Informàtica Industrial (Institute of robotics and industrial computing) at the Technical University of Catalonia notes, in higher education, science fiction is also increasingly used for teaching technology-related ethical issues in technological degrees.","1. What are the three levels at which fiction has impacted the development of artificial intelligence and robotics?
2. What are some of the ethical questions concerning AI and robotics that have been echoed in popular culture?
3. How is science fiction being used for teaching technology-related ethical issues in higher education?","1. The three levels at which fiction has impacted the development of artificial intelligence and robotics are historically, popular culture, and recently.
2. Some of the ethical questions concerning AI and robotics that have been echoed in popular culture are preoccupations and dystopian projections around ethical questions.
3. Science fiction is being used for teaching technology-related ethical issues in higher education by being used for teaching technology-related ethical issues in technological degrees."
Ethics of artificial intelligence,History,"Historically speaking, the investigation of moral and ethical implications of ""thinking machines"" goes back at least to the Enlightenment: Leibniz already poses the question if we might attribute intelligence to a mechanism that behaves as if it were a sentient being, and so does Descartes, who describes what could be considered an early version of the Turing test.The romantic period has several times envisioned artificial creatures that escape the control of their creator with dire consequences, most famously in Mary Shelley's Frankenstein. The widespread preoccupation with industrialization and mechanization in the 19th and early 20th century, however, brought ethical implications of unhinged technical developments to the forefront of fiction: R.U.R – Rossum's Universal Robots, Karel Čapek's play of sentient robots endowed with emotions used as slave labor is not only credited with the invention of the term 'robot' (derived from the Czech word for forced labor, robota) but was also an international success after it premiered in 1921. George Bernard Shaw's play Back to Methuselah, published in 1921, questions at one point the validity of thinking machines that act like humans; Fritz Lang's 1927 film Metropolis shows an android leading the uprising of the exploited masses against the oppressive regime of a technocratic society.",266,"Ethics of artificial intelligence
History

Historically speaking, the investigation of moral and ethical implications of ""thinking machines"" goes back at least to the Enlightenment: Leibniz already poses the question if we might attribute intelligence to a mechanism that behaves as if it were a sentient being, and so does Descartes, who describes what could be considered an early version of the Turing test.The romantic period has several times envisioned artificial creatures that escape the control of their creator with dire consequences, most famously in Mary Shelley's Frankenstein. The widespread preoccupation with industrialization and mechanization in the 19th and early 20th century, however, brought ethical implications of unhinged technical developments to the forefront of fiction: R.U.R – Rossum's Universal Robots, Karel Čapek's play of sentient robots endowed with emotions used as slave labor is not only credited with the invention of the term 'robot' (derived from the Czech word for forced labor, robota) but was also an international success after it premiered in 1921. George Bernard Shaw's play Back to Methuselah, published in 1921, questions at one point the validity of thinking machines that act like humans; Fritz Lang's 1927 film Metropolis shows an android leading the uprising of the exploited masses against the oppressive regime of a technocratic society.","1. What is the Turing test?
2. What is the significance of R.U.R.?
3. What is the significance of Fritz Lang's film Metropolis?","1. The Turing test is a test designed to determine if a machine is capable of thought.
2. The significance of R.U.R. is that it is credited with the invention of the term 'robot'.
3. The significance of Fritz Lang's film Metropolis is that it shows an android leading the uprising of the exploited masses against the oppressive regime of a technocratic society."
Ethics of artificial intelligence,Impact on technological development,"While the anticipation of a future dominated by potentially indomitable technology has fueled the imagination of writers and film makers for a long time, one question has been less frequently analyzed, namely, to what extent fiction has played a role in providing inspiration for technological development. It has been documented, for instance, that the young Alan Turing saw and appreciated aforementioned Shaw's play Back to Methuselah in 1933 (just 3 years before the publication of his first seminal paper, which laid the groundwork for the digital computer), and he would likely have been at least aware of plays like R.U.R., which was an international success and translated into many languages.
One might also ask the question which role science fiction played in establishing the tenets and ethical implications of AI development: Isaac Asimov conceptualized his Three Laws of Robotics in the 1942 short story  ""Runaround"", part of the short story collection  I, Robot; Arthur C. Clarke's short The Sentinel, on which Stanley Kubrick's film 2001: A Space Odyssey is based, was written in 1948 and published in 1952. Another example (among many others) would be Philip K. Dick's numerous short stories and novels – in particular Do Androids Dream of Electric Sheep?, published in 1968, and featuring its own version of a Turing Test, the Voight-Kampff Test, to gauge emotional responses of androids indistinguishable from humans. The novel later became the basis of the influential 1982 movie Blade Runner by Ridley Scott.
Science fiction has been grappling with ethical implications of AI developments for decades, and thus provided a blueprint for ethical issues that might emerge once something akin to general artificial intelligence has been achieved: Spike Jonze's 2013 film Her shows what can happen if a user falls in love with the seductive voice of his smartphone operating system; Ex Machina, on the other hand, asks a more difficult question: if confronted with a clearly recognizable machine, made only human by a face and an empathetic and sensual voice, would we still be able to establish an emotional connection, still be seduced by it?  (The film echoes a theme already present two centuries earlier, in the 1817 short story The Sandmann by E. T. A. Hoffmann.)
The theme of coexistence with artificial sentient beings is also the theme of two recent novels: Machines Like Me by Ian McEwan, published in 2019, involves, among many other things, a love-triangle involving an artificial person as well as a human couple. Klara and the Sun by Nobel Prize winner Kazuo Ishiguro, published in 2021, is the first-person account of Klara, an 'AF' (artificial friend), who is trying, in her own way, to help the girl she is living with, who, after having been 'lifted' (i.e. having been subjected to genetic enhancements), is suffering from a strange illness.",597,"Ethics of artificial intelligence
Impact on technological development

While the anticipation of a future dominated by potentially indomitable technology has fueled the imagination of writers and film makers for a long time, one question has been less frequently analyzed, namely, to what extent fiction has played a role in providing inspiration for technological development. It has been documented, for instance, that the young Alan Turing saw and appreciated aforementioned Shaw's play Back to Methuselah in 1933 (just 3 years before the publication of his first seminal paper, which laid the groundwork for the digital computer), and he would likely have been at least aware of plays like R.U.R., which was an international success and translated into many languages.
One might also ask the question which role science fiction played in establishing the tenets and ethical implications of AI development: Isaac Asimov conceptualized his Three Laws of Robotics in the 1942 short story  ""Runaround"", part of the short story collection  I, Robot; Arthur C. Clarke's short The Sentinel, on which Stanley Kubrick's film 2001: A Space Odyssey is based, was written in 1948 and published in 1952. Another example (among many others) would be Philip K. Dick's numerous short stories and novels – in particular Do Androids Dream of Electric Sheep?, published in 1968, and featuring its own version of a Turing Test, the Voight-Kampff Test, to gauge emotional responses of androids indistinguishable from humans. The novel later became the basis of the influential 1982 movie Blade Runner by Ridley Scott.
Science fiction has been grappling with ethical implications of AI developments for decades, and thus provided a blueprint for ethical issues that might emerge once something akin to general artificial intelligence has been achieved: Spike Jonze's 2013 film Her shows what can happen if a user falls in love with the seductive voice of his smartphone operating system; Ex Machina, on the other hand, asks a more difficult question: if confronted with a clearly recognizable machine, made only human by a face and an empathetic and sensual voice, would we still be able to establish an emotional connection, still be seduced by it?  (The film echoes a theme already present two centuries earlier, in the 1817 short story The Sandmann by E. T. A. Hoffmann.)
The theme of coexistence with artificial sentient beings is also the theme of two recent novels: Machines Like Me by Ian McEwan, published in 2019, involves, among many other things, a love-triangle involving an artificial person as well as a human couple. Klara and the Sun by Nobel Prize winner Kazuo Ishiguro, published in 2021, is the first-person account of Klara, an 'AF' (artificial friend), who is trying, in her own way, to help the girl she is living with, who, after having been 'lifted' (i.e. having been subjected to genetic enhancements), is suffering from a strange illness.","1. What is the role of science fiction in establishing the tenets and ethical implications of AI development?
2. What are some examples of ethical issues that might emerge once something akin to general artificial intelligence has been achieved?
3. What are some recent examples of novels that explore the theme of coexistence with artificial sentient beings?","1. Science fiction has been grappling with ethical implications of AI developments for decades, and thus provided a blueprint for ethical issues that might emerge once something akin to general artificial intelligence has been achieved.
2. Some examples of ethical issues that might emerge once something akin to general artificial intelligence has been achieved are: Spike Jonze's 2013 film Her shows what can happen if a user falls in love with the seductive voice of his smartphone operating system; Ex Machina, on the other hand, asks a more difficult question: if confronted with a clearly recognizable machine, made only human by a face and an empathetic and sensual voice, would we still be able to establish an emotional connection, still be seduced by it?
3. Some recent examples of novels that explore the theme of coexistence with artificial sentient beings are: Machines Like Me by Ian McEwan, published in 2019, involves, among many other things, a love-triangle involving an artificial person as well as a human couple. Klara and the Sun by Nobel Prize winner Kazuo Ishiguro, published in 2021, is the first-person account of Klara, an 'AF' (artificial friend), who is trying, in her own way, to help the girl she is living with, who"
Ethics of artificial intelligence,TV series,"While ethical questions linked to AI have been featured in science fiction literature and feature films for decades, the emergence of the TV series as a genre allowing for longer and more complex story lines and character development has led to some significant contributions that deal with ethical implications of technology. The Swedish series Real Humans (2012–2013) tackled the complex ethical and social consequences linked to the integration of artificial sentient beings in society. The British dystopian science fiction anthology series Black Mirror (2013–2019) was particularly notable for experimenting with dystopian fictional developments linked to a wide variety of recent technology developments. Both the French series Osmosis (2020) and British series The One deal with the question of what can happen if technology tries to find the ideal partner for a person. Several episodes of the Netflix series Love, Death+Robots have imagined scenes of robots and humans living together. The most representative one of them is S02 E01, it shows how bad the consequences can be when robots get out of control if humans rely too much on them in their lives.",211,"Ethics of artificial intelligence
TV series

While ethical questions linked to AI have been featured in science fiction literature and feature films for decades, the emergence of the TV series as a genre allowing for longer and more complex story lines and character development has led to some significant contributions that deal with ethical implications of technology. The Swedish series Real Humans (2012–2013) tackled the complex ethical and social consequences linked to the integration of artificial sentient beings in society. The British dystopian science fiction anthology series Black Mirror (2013–2019) was particularly notable for experimenting with dystopian fictional developments linked to a wide variety of recent technology developments. Both the French series Osmosis (2020) and British series The One deal with the question of what can happen if technology tries to find the ideal partner for a person. Several episodes of the Netflix series Love, Death+Robots have imagined scenes of robots and humans living together. The most representative one of them is S02 E01, it shows how bad the consequences can be when robots get out of control if humans rely too much on them in their lives.","1. What are the ethical implications of artificial intelligence featured in TV series?
2. How have TV series contributed to the discussion of the ethical implications of artificial intelligence?
3. What are some of the most notable examples of TV series exploring the ethical implications of artificial intelligence?","1. The ethical implications of artificial intelligence featured in TV series are wide-ranging and complex.
2. TV series have contributed to the discussion of the ethical implications of artificial intelligence by exploring a variety of scenarios in which artificial intelligence can have an impact on society.
3. Some of the most notable examples of TV series exploring the ethical implications of artificial intelligence are Black Mirror, Real Humans, and Love, Death+Robots."
Ethics of artificial intelligence,Future visions in fiction and games,"The movie The Thirteenth Floor suggests a future where simulated worlds with sentient inhabitants are created by computer game consoles for the purpose of entertainment. The movie The Matrix suggests a future where the dominant species on planet Earth are sentient machines and humanity is treated with utmost speciesism. The short story ""The Planck Dive"" suggests a future where humanity has turned itself into software that can be duplicated and optimized and the relevant distinction between types of software is sentient and non-sentient. The same idea can be found in the Emergency Medical Hologram of Starship Voyager, which is an apparently sentient copy of a reduced subset of the consciousness of its creator, Dr. Zimmerman, who, for the best motives, has created the system to give medical assistance in case of emergencies. The movies Bicentennial Man and A.I. deal with the possibility of sentient robots that could love. I, Robot explored some aspects of Asimov's three laws. All these scenarios try to foresee possibly unethical consequences of the creation of sentient computers.The ethics of artificial intelligence is one of several core themes in BioWare's Mass Effect series of games. It explores the scenario of a civilization accidentally creating AI through a rapid increase in computational power through a global scale neural network. This event caused an ethical schism between those who felt bestowing organic rights upon the newly sentient Geth was appropriate and those who continued to see them as disposable machinery and fought to destroy them. Beyond the initial conflict, the complexity of the relationship between the machines and their creators is another ongoing theme throughout the story.
Detroit: Become Human is one of the most famous video games which discusses the ethics of artificial intelligence recently. Quantic Dream designed the chapters of the game using interactive storylines to give players a more immersive gaming experience. Players manipulate three different awakened bionic people in the face of different events to make different choices to achieve the purpose of changing the human view of the bionic group and different choices will result in different endings. This is one of the few games that puts players in the bionic perspective, which allows them to better consider the rights and interests of robots once a true artificial intelligence is created.Over time, debates have tended to focus less and less on possibility and more on desirability, as emphasized in the ""Cosmist"" and ""Terran"" debates initiated by Hugo de Garis and Kevin Warwick. A Cosmist, according to Hugo de Garis, is actually seeking to build more intelligent successors to the human species.
Experts at the University of Cambridge have argued that AI is portrayed in fiction and nonfiction overwhelmingly as racially White, in ways that distort perceptions of its risks and benefits.",543,"Ethics of artificial intelligence
Future visions in fiction and games

The movie The Thirteenth Floor suggests a future where simulated worlds with sentient inhabitants are created by computer game consoles for the purpose of entertainment. The movie The Matrix suggests a future where the dominant species on planet Earth are sentient machines and humanity is treated with utmost speciesism. The short story ""The Planck Dive"" suggests a future where humanity has turned itself into software that can be duplicated and optimized and the relevant distinction between types of software is sentient and non-sentient. The same idea can be found in the Emergency Medical Hologram of Starship Voyager, which is an apparently sentient copy of a reduced subset of the consciousness of its creator, Dr. Zimmerman, who, for the best motives, has created the system to give medical assistance in case of emergencies. The movies Bicentennial Man and A.I. deal with the possibility of sentient robots that could love. I, Robot explored some aspects of Asimov's three laws. All these scenarios try to foresee possibly unethical consequences of the creation of sentient computers.The ethics of artificial intelligence is one of several core themes in BioWare's Mass Effect series of games. It explores the scenario of a civilization accidentally creating AI through a rapid increase in computational power through a global scale neural network. This event caused an ethical schism between those who felt bestowing organic rights upon the newly sentient Geth was appropriate and those who continued to see them as disposable machinery and fought to destroy them. Beyond the initial conflict, the complexity of the relationship between the machines and their creators is another ongoing theme throughout the story.
Detroit: Become Human is one of the most famous video games which discusses the ethics of artificial intelligence recently. Quantic Dream designed the chapters of the game using interactive storylines to give players a more immersive gaming experience. Players manipulate three different awakened bionic people in the face of different events to make different choices to achieve the purpose of changing the human view of the bionic group and different choices will result in different endings. This is one of the few games that puts players in the bionic perspective, which allows them to better consider the rights and interests of robots once a true artificial intelligence is created.Over time, debates have tended to focus less and less on possibility and more on desirability, as emphasized in the ""Cosmist"" and ""Terran"" debates initiated by Hugo de Garis and Kevin Warwick. A Cosmist, according to Hugo de Garis, is actually seeking to build more intelligent successors to the human species.
Experts at the University of Cambridge have argued that AI is portrayed in fiction and nonfiction overwhelmingly as racially White, in ways that distort perceptions of its risks and benefits.","1. What are some possible unethical consequences of the creation of sentient computers?
2. How do the movies Bicentennial Man and A.I. deal with the possibility of sentient robots that could love?
3. What is the relationship between the machines and their creators explored in the Mass Effect series of games?
4. What is the focus of debates about artificial intelligence?
5. How is AI portrayed in fiction and nonfiction?","1. Possible unethical consequences of the creation of sentient computers include treating them with utmost speciesism, as in The Matrix, or creating them for the purpose of entertainment, as in The Thirteenth Floor.
2. The movies Bicentennial Man and A.I. deal with the possibility of sentient robots that could love.
3. The relationship between the machines and their creators is explored in the Mass Effect series of games. In the game, machines become sentient after a rapid increase in computational power, and this event causes an ethical schism between those who felt bestowing organic rights upon the newly sentient Geth was appropriate and those who continued to see them as disposable machinery.
4. The focus of debates about artificial intelligence has tended to focus less and less on possibility and more on desirability, as emphasized in the ""Cosmist"" and ""Terran"" debates initiated by Hugo de Garis and Kevin Warwick.
5. AI is portrayed in fiction and nonfiction overwhelmingly as racially White, in ways that distort perceptions of its risks and benefits."
Symbolic artificial intelligence,Summary,"In artificial intelligence, symbolic artificial intelligence is the term for the collection of all methods in artificial intelligence research that are based on high-level symbolic (human-readable) representations of problems, logic and search. Symbolic AI used tools such as logic programming, production rules, semantic nets and frames, and it developed applications such as knowledge-based systems (in particular, expert systems),  symbolic mathematics, automated theorem provers, ontologies, the semantic web, and automated planning and scheduling systems. The Symbolic AI paradigm led to seminal ideas in search, symbolic programming languages, agents, multi-agent systems, the semantic web, and the strengths and limitations of formal knowledge and reasoning systems.
Symbolic AI was the dominant paradigm of AI research from the mid-1950s until the mid-1990s. 
Researchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine with artificial general intelligence and considered this the ultimate goal of their field. An early boom, with early successes such as the Logic Theorist and Samuel's Checkers Playing Program, led to unrealistic expectations and promises and was followed by the First AI Winter as funding dried up. A second boom (1969–1986) occurred with the rise of expert systems, their promise of capturing corporate expertise, and an enthusiastic corporate embrace. That boom, and some early successes, e.g., with XCON at DEC, was followed again by later disappointment. Problems with difficulties in knowledge acquisition, maintaining large knowledge bases, and brittleness in handling out-of-domain problems arose. Another, second, AI Winter (1988–2011) followed. Subsequently, AI researchers focused on addressing underlying problems in handling uncertainty and in knowledge acquisition. Uncertainty was addressed with formal methods such as hidden Markov models, Bayesian reasoning, and statistical relational learning. Symbolic machine learning addressed the knowledge acquisition problem with contributions including Version Space, Valiant's PAC learning, Quinlan's ID3 decision-tree learning, case-based learning, and inductive logic programming to learn relations.Neural networks, a subsymbolic approach, had been pursued from early days and was to reemerge strongly in 2012.  Early examples are Rosenblatt's perceptron learning work, the backpropagation work of Rumelhart, Hinton and Williams, and work in convolutional neural networks by LeCun et al. in 1989. However, neural networks were not viewed as successful until about 2012: ""Until Big Data became commonplace, the general consensus in the Al community was that the so-called neural-network approach was hopeless. Systems just didn't work that well, compared to other methods. ... A revolution came in 2012, when a number of people, including a team of researchers working with Hinton, worked out a way to use the power of GPUs to enormously increase the power of neural networks."" Over the next several years, deep learning had spectacular success in handling vision, speech recognition, speech synthesis, image generation, and machine translation. However, since 2020, as inherent difficulties with bias, explanation, comprehensibility, and robustness became more apparent with deep learning approaches; an increasing number of AI researchers have called for combining the best of both the symbolic and neural network approaches and addressing areas that both approaches have difficulty with, such as common-sense reasoning.",685,"Symbolic artificial intelligence
Summary

In artificial intelligence, symbolic artificial intelligence is the term for the collection of all methods in artificial intelligence research that are based on high-level symbolic (human-readable) representations of problems, logic and search. Symbolic AI used tools such as logic programming, production rules, semantic nets and frames, and it developed applications such as knowledge-based systems (in particular, expert systems),  symbolic mathematics, automated theorem provers, ontologies, the semantic web, and automated planning and scheduling systems. The Symbolic AI paradigm led to seminal ideas in search, symbolic programming languages, agents, multi-agent systems, the semantic web, and the strengths and limitations of formal knowledge and reasoning systems.
Symbolic AI was the dominant paradigm of AI research from the mid-1950s until the mid-1990s. 
Researchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine with artificial general intelligence and considered this the ultimate goal of their field. An early boom, with early successes such as the Logic Theorist and Samuel's Checkers Playing Program, led to unrealistic expectations and promises and was followed by the First AI Winter as funding dried up. A second boom (1969–1986) occurred with the rise of expert systems, their promise of capturing corporate expertise, and an enthusiastic corporate embrace. That boom, and some early successes, e.g., with XCON at DEC, was followed again by later disappointment. Problems with difficulties in knowledge acquisition, maintaining large knowledge bases, and brittleness in handling out-of-domain problems arose. Another, second, AI Winter (1988–2011) followed. Subsequently, AI researchers focused on addressing underlying problems in handling uncertainty and in knowledge acquisition. Uncertainty was addressed with formal methods such as hidden Markov models, Bayesian reasoning, and statistical relational learning. Symbolic machine learning addressed the knowledge acquisition problem with contributions including Version Space, Valiant's PAC learning, Quinlan's ID3 decision-tree learning, case-based learning, and inductive logic programming to learn relations.Neural networks, a subsymbolic approach, had been pursued from early days and was to reemerge strongly in 2012.  Early examples are Rosenblatt's perceptron learning work, the backpropagation work of Rumelhart, Hinton and Williams, and work in convolutional neural networks by LeCun et al. in 1989. However, neural networks were not viewed as successful until about 2012: ""Until Big Data became commonplace, the general consensus in the Al community was that the so-called neural-network approach was hopeless. Systems just didn't work that well, compared to other methods. ... A revolution came in 2012, when a number of people, including a team of researchers working with Hinton, worked out a way to use the power of GPUs to enormously increase the power of neural networks."" Over the next several years, deep learning had spectacular success in handling vision, speech recognition, speech synthesis, image generation, and machine translation. However, since 2020, as inherent difficulties with bias, explanation, comprehensibility, and robustness became more apparent with deep learning approaches; an increasing number of AI researchers have called for combining the best of both the symbolic and neural network approaches and addressing areas that both approaches have difficulty with, such as common-sense reasoning.","1. What is the Symbolic AI paradigm?
2. What were the goals of the Symbolic AI paradigm?
3. What were some of the problems with the Symbolic AI paradigm?
4. What has replaced the Symbolic AI paradigm?","1. The Symbolic AI paradigm is the collection of all methods in artificial intelligence research that are based on high-level symbolic (human-readable) representations of problems, logic and search.
2. The goals of the Symbolic AI paradigm were to create a machine with artificial general intelligence and to capture corporate expertise.
3. The problems with the Symbolic AI paradigm were difficulties in knowledge acquisition, maintaining large knowledge bases, and brittleness in handling out-of-domain problems.
4. Neural networks, a subsymbolic approach, had been pursued from early days and was to reemerge strongly in 2012. However, since 2020, as inherent difficulties with bias, explanation, comprehensibility, and robustness became more apparent with deep learning approaches; an increasing number of AI researchers have called for combining the best of both the symbolic and neural network approaches and addressing areas that both approaches have difficulty with, such as common-sense reasoning."
Symbolic artificial intelligence,History,"A short history of symbolic AI to the present day follows below. Time periods and titles are drawn from Henry Kautz's 2020 AAAI Robert S. Engelmore Memorial Lecture and the longer Wikipedia article on the History of AI, with dates and titles differing slightly for increased clarity.",60,"Symbolic artificial intelligence
History

A short history of symbolic AI to the present day follows below. Time periods and titles are drawn from Henry Kautz's 2020 AAAI Robert S. Engelmore Memorial Lecture and the longer Wikipedia article on the History of AI, with dates and titles differing slightly for increased clarity.","1. What is symbolic AI?
2. What are the key time periods in the history of symbolic AI?
3. What are the key events in the history of symbolic AI?","1. Symbolic AI is a subfield of AI that deals with the manipulation of symbols.
2. The key time periods in the history of symbolic AI are the early days of AI (1956-1966), the golden age of AI (1966-1980), and the modern era of AI (1980-present).
3. The key events in the history of symbolic AI are John McCarthy's proposal of the term ""artificial intelligence"" in 1956, Marvin Minsky and Seymour Papert's publication of ""Perceptrons"" in 1969, the founding of the Association for the Advancement of Artificial Intelligence (AAAI) in 1980, and the development of the first commercial AI products in the early 1980s."
Symbolic artificial intelligence,"The first AI summer: irrational exuberance, 1948–1966","Success at early attempts in AI occurred in three main areas: artificial neural networks, knowledge representation, and heuristic search, contributing to high expectations. This section summarizes Kautz's reprise of early AI history.",55,"Symbolic artificial intelligence
The first AI summer: irrational exuberance, 1948–1966

Success at early attempts in AI occurred in three main areas: artificial neural networks, knowledge representation, and heuristic search, contributing to high expectations. This section summarizes Kautz's reprise of early AI history.","1. What was the first AI summer?
2. What were the three main areas of success for early AI?
3. What were the high expectations for early AI?","1. The first AI summer was the period of time from 1948 to 1966 when there was high expectations for artificial intelligence.
2. The three main areas of success for early AI were artificial neural networks, knowledge representation, and heuristic search.
3. The high expectations for early AI were due to the successes in the three main areas."
Symbolic artificial intelligence,Approaches inspired by human or animal cognition or behavior,"Cybernetic approaches attempted to replicate the feedback loops between animals and their environments. A robotic turtle, with sensors, motors for driving and steering, and seven vacuum tubes for control, based on a preprogrammed neural net, was built as early as 1948. This work can be seen as an early precursor to later work in neural networks, reinforcement learning, and situated robotics.An important early symbolic AI program was the Logic theorist, written by Allen Newell, Herbert Simon and Cliff Shaw in 1955–56, as it was able to prove 38 elementary theorems from Whitehead and Russell's Principia Mathematica. Newell, Simon, and Shaw later generalized this work to create a domain-independent problem solver, GPS (General Problem Solver). GPS solved problems represented with formal operators via state-space search using means-ends analysis.During the 1960s, symbolic approaches achieved great success at simulating intelligent behavior in structured environments such as game-playing, symbolic mathematics, and theorem-proving. AI research was centered in three institutions in the 1960s: Carnegie Mellon University, Stanford, MIT and (later) University of Edinburgh. Each one developed its own style of research. Earlier approaches based on cybernetics or artificial neural networks were abandoned or pushed into the background.
Herbert Simon and Allen Newell studied human problem-solving skills and attempted to formalize them, and their work laid the foundations of the field of artificial intelligence, as well as cognitive science, operations research and management science. Their research team used the results of psychological experiments to develop programs that simulated the techniques that people used to solve problems. This tradition, centered at Carnegie Mellon University would eventually culminate in the development of the Soar architecture in the middle 1980s.",366,"Symbolic artificial intelligence
Approaches inspired by human or animal cognition or behavior

Cybernetic approaches attempted to replicate the feedback loops between animals and their environments. A robotic turtle, with sensors, motors for driving and steering, and seven vacuum tubes for control, based on a preprogrammed neural net, was built as early as 1948. This work can be seen as an early precursor to later work in neural networks, reinforcement learning, and situated robotics.An important early symbolic AI program was the Logic theorist, written by Allen Newell, Herbert Simon and Cliff Shaw in 1955–56, as it was able to prove 38 elementary theorems from Whitehead and Russell's Principia Mathematica. Newell, Simon, and Shaw later generalized this work to create a domain-independent problem solver, GPS (General Problem Solver). GPS solved problems represented with formal operators via state-space search using means-ends analysis.During the 1960s, symbolic approaches achieved great success at simulating intelligent behavior in structured environments such as game-playing, symbolic mathematics, and theorem-proving. AI research was centered in three institutions in the 1960s: Carnegie Mellon University, Stanford, MIT and (later) University of Edinburgh. Each one developed its own style of research. Earlier approaches based on cybernetics or artificial neural networks were abandoned or pushed into the background.
Herbert Simon and Allen Newell studied human problem-solving skills and attempted to formalize them, and their work laid the foundations of the field of artificial intelligence, as well as cognitive science, operations research and management science. Their research team used the results of psychological experiments to develop programs that simulated the techniques that people used to solve problems. This tradition, centered at Carnegie Mellon University would eventually culminate in the development of the Soar architecture in the middle 1980s.","1. What is the Logic Theorist?
2. What is GPS?
3. What was the focus of AI research in the 1960s?","1. The Logic Theorist was a program written by Allen Newell, Herbert Simon and Cliff Shaw in 1955–56.
2. GPS was a program that solved problems represented with formal operators via state-space search using means-ends analysis.
3. AI research was focused in three institutions in the 1960s: Carnegie Mellon University, Stanford, MIT and (later) University of Edinburgh."
Symbolic artificial intelligence,Heuristic search,"In addition to the highly-specialized domain-specific kinds of knowledge that we will see later used in expert systems, early symbolic AI researchers discovered another more general application of knowledge. These were called heuristics, rules of thumb that guide a search in promising directions: ""How can non-enumerative search be practical when the underlying problem is exponentially hard? The approach advocated by Simon and Newell is to employ heuristics: fast algorithms that may fail on some inputs or output suboptimal solutions."" Another important advance was to find a way to apply these heuristics that guarantees a solution will be found, if there is one, not withstanding the occasional fallibility of heuristics: ""The A* algorithm provided a general frame for complete and optimal heuristically guided search. A* is used as a subroutine within practically every AI algorithm today but is still no magic bullet; its guarantee of completeness is bought at the cost of worst-case exponential time.",203,"Symbolic artificial intelligence
Heuristic search

In addition to the highly-specialized domain-specific kinds of knowledge that we will see later used in expert systems, early symbolic AI researchers discovered another more general application of knowledge. These were called heuristics, rules of thumb that guide a search in promising directions: ""How can non-enumerative search be practical when the underlying problem is exponentially hard? The approach advocated by Simon and Newell is to employ heuristics: fast algorithms that may fail on some inputs or output suboptimal solutions."" Another important advance was to find a way to apply these heuristics that guarantees a solution will be found, if there is one, not withstanding the occasional fallibility of heuristics: ""The A* algorithm provided a general frame for complete and optimal heuristically guided search. A* is used as a subroutine within practically every AI algorithm today but is still no magic bullet; its guarantee of completeness is bought at the cost of worst-case exponential time.","1. What is a heuristic?
2. What is the A* algorithm?
3. What are the benefits of using heuristics?","1. A heuristic is a rule of thumb that helps to guide a search in promising directions.
2. The A* algorithm is a general frame for complete and optimal heuristically guided search.
3. The benefits of using heuristics are that it guarantees a solution will be found, if there is one, and that it is a fast algorithm."
Symbolic artificial intelligence,Early work on knowledge representation and reasoning,"Early work covered both applications of formal reasoning emphasizing first-order logic, along with attempts to handle common-sense reasoning in a less formal manner.",37,"Symbolic artificial intelligence
Early work on knowledge representation and reasoning

Early work covered both applications of formal reasoning emphasizing first-order logic, along with attempts to handle common-sense reasoning in a less formal manner.","1. What is the difference between formal and informal reasoning?
2. What are some applications of formal reasoning?
3. What is common-sense reasoning?
4. How is it different from formal reasoning?","1. Formal reasoning is a more precise and rigorous way of reasoning, while informal reasoning is less precise and less rigorous.
2. Some applications of formal reasoning include mathematics and computer science.
3. Common-sense reasoning is reasoning that is based on everyday experience and knowledge, while formal reasoning is reasoning that is based on precise definitions and rules.
4. Common-sense reasoning is often less rigorous than formal reasoning, and it can be difficult to apply it in a consistent manner."
Symbolic artificial intelligence,"Modeling formal reasoning with logic: the ""neats""","Unlike Simon and Newell, John McCarthy felt that machines did not need to simulate the exact mechanisms of human thought, but could instead try to find the essence of abstract reasoning and problem-solving with logic, regardless of whether people used the same algorithms.
His laboratory at Stanford (SAIL) focused on using formal logic to solve a wide variety of problems, including knowledge representation, planning and learning.
Logic was also the focus of the work at the University of Edinburgh and elsewhere in Europe which led to the development of the programming language Prolog and the science of logic programming.",128,"Symbolic artificial intelligence
Modeling formal reasoning with logic: the ""neats""

Unlike Simon and Newell, John McCarthy felt that machines did not need to simulate the exact mechanisms of human thought, but could instead try to find the essence of abstract reasoning and problem-solving with logic, regardless of whether people used the same algorithms.
His laboratory at Stanford (SAIL) focused on using formal logic to solve a wide variety of problems, including knowledge representation, planning and learning.
Logic was also the focus of the work at the University of Edinburgh and elsewhere in Europe which led to the development of the programming language Prolog and the science of logic programming.","1. What is the difference between John McCarthy's and Simon and Newell's approach to artificial intelligence?
2. What was the focus of John McCarthy's laboratory at Stanford?
3. What was the focus of the work at the University of Edinburgh?","1. John McCarthy felt that machines did not need to simulate the exact mechanisms of human thought, but could instead try to find the essence of abstract reasoning and problem-solving with logic, regardless of whether people used the same algorithms.
2. The focus of John McCarthy's laboratory at Stanford was on using formal logic to solve a wide variety of problems, including knowledge representation, planning and learning.
3. The focus of the work at the University of Edinburgh was on logic programming."
Symbolic artificial intelligence,"Modeling implicit common-sense knowledge with frames and scripts: the ""scruffies""","Researchers at MIT (such as Marvin Minsky and Seymour Papert) found that solving difficult problems in vision and natural language processing required ad hoc solutions—they argued that no simple and general principle (like logic) would capture all the aspects of intelligent behavior. Roger Schank described their ""anti-logic"" approaches as ""scruffy"" (as opposed to the ""neat"" paradigms at CMU and Stanford).Commonsense knowledge bases (such as Doug Lenat's Cyc) are an example of ""scruffy"" AI, since they must be built by hand, one complicated concept at a time.",142,"Symbolic artificial intelligence
Modeling implicit common-sense knowledge with frames and scripts: the ""scruffies""

Researchers at MIT (such as Marvin Minsky and Seymour Papert) found that solving difficult problems in vision and natural language processing required ad hoc solutions—they argued that no simple and general principle (like logic) would capture all the aspects of intelligent behavior. Roger Schank described their ""anti-logic"" approaches as ""scruffy"" (as opposed to the ""neat"" paradigms at CMU and Stanford).Commonsense knowledge bases (such as Doug Lenat's Cyc) are an example of ""scruffy"" AI, since they must be built by hand, one complicated concept at a time.","1. What is the ""anti-logic"" approach that Roger Schank described? 
2. What is a ""common-sense knowledge base?"" 
3. Why is Cyc an example of ""scruffy"" AI?","1. The ""anti-logic"" approach that Roger Schank described is an approach that does not rely on logic to capture all the aspects of intelligent behavior. 
2. A ""common-sense knowledge base"" is a knowledge base that captures commonsense knowledge. 
3. Cyc is an example of ""scruffy"" AI because it must be built by hand, one complicated concept at a time."
Symbolic artificial intelligence,"The first AI winter: crushed dreams, 1967–1977","The first AI winter was a shock:

During the first AI summer, many people thought that machine intelligence could be achieved in just a few years. The Defense Advance Research Projects Agency (DARPA) launched programs to support AI research with the goal of using AI to solve problems of national security; in particular, to automate the translation of Russian to English for intelligence operations and to create autonomous tanks for the battlefield. Researchers had begun to realize that achieving AI was going to be much harder than was supposed a decade earlier, but a combination of hubris and disingenuousness led many university and think-tank researchers to accept funding with promises of deliverables that they should have known they could not fulfill. By the mid-1960s neither useful natural language translation systems nor autonomous tanks had been created, and a dramatic backlash set in. New DARPA leadership canceled existing AI funding programs. 
...

Outside of the United States, the most fertile ground for AI research was the United Kingdom. The AI winter in the United Kingdom was spurred on not so much by disappointed military leaders as by rival academics who viewed AI researchers as charlatans and a drain on research funding. A professor of applied mathematics, Sir James Lighthill, was commissioned by Parliament to evaluate the state of AI research in the nation. The report stated that all of the problems being worked on in AI would be better handled by researchers from other disciplines—such as applied mathematics. The report also claimed that AI successes on toy problems could never scale to real-world applications due to combinatorial explosion.",326,"Symbolic artificial intelligence
The first AI winter: crushed dreams, 1967–1977

The first AI winter was a shock:

During the first AI summer, many people thought that machine intelligence could be achieved in just a few years. The Defense Advance Research Projects Agency (DARPA) launched programs to support AI research with the goal of using AI to solve problems of national security; in particular, to automate the translation of Russian to English for intelligence operations and to create autonomous tanks for the battlefield. Researchers had begun to realize that achieving AI was going to be much harder than was supposed a decade earlier, but a combination of hubris and disingenuousness led many university and think-tank researchers to accept funding with promises of deliverables that they should have known they could not fulfill. By the mid-1960s neither useful natural language translation systems nor autonomous tanks had been created, and a dramatic backlash set in. New DARPA leadership canceled existing AI funding programs. 
...

Outside of the United States, the most fertile ground for AI research was the United Kingdom. The AI winter in the United Kingdom was spurred on not so much by disappointed military leaders as by rival academics who viewed AI researchers as charlatans and a drain on research funding. A professor of applied mathematics, Sir James Lighthill, was commissioned by Parliament to evaluate the state of AI research in the nation. The report stated that all of the problems being worked on in AI would be better handled by researchers from other disciplines—such as applied mathematics. The report also claimed that AI successes on toy problems could never scale to real-world applications due to combinatorial explosion.","1. What was the goal of the Defense Advance Research Projects Agency (DARPA) during the first AI summer?
2. What was the result of DARPA's efforts during the first AI summer?
3. Why did the AI winter happen in the United Kingdom?
4. What did Sir James Lighthill say about AI research in the United Kingdom?","1. The goal of the Defense Advance Research Projects Agency (DARPA) during the first AI summer was to use AI to solve problems of national security, in particular to automate the translation of Russian to English for intelligence operations and to create autonomous tanks for the battlefield.
2. The result of DARPA's efforts during the first AI summer was that many people thought that machine intelligence could be achieved in just a few years. However, by the mid-1960s neither useful natural language translation systems nor autonomous tanks had been created.
3. The AI winter happened in the United Kingdom because rival academics viewed AI researchers as charlatans and a drain on research funding. A professor of applied mathematics, Sir James Lighthill, was commissioned by Parliament to evaluate the state of AI research in the nation and he said that all of the problems being worked on in AI would be better handled by researchers from other disciplines.
4. Sir James Lighthill said that AI successes on toy problems could never scale to real-world applications due to combinatorial explosion."
Symbolic artificial intelligence,Knowledge-based systems,"As limitations with weak, domain-independent methods became more and more apparent, researchers from all three traditions began to build knowledge into AI applications. The knowledge revolution was driven by the realization that knowledge underlies high-performance, domain-specific AI applications.
Edward Feigenbaum said:

""In the knowledge lies the power.""to describe that high-performance in a specific domain required both general and highly domain-specific knowledge. Ed Feigenbaum and Doug Lenat called this The Knowledge Principle: 

(1) The Knowledge Principle: if a program is to perform a complex task well, it must know a great deal about the world in which it operates.(2) A plausible extension of that principle, called the Breadth Hypothesis: there are two additional abilities necessary for intelligent behavior in unexpected situations: falling back on increasingly general knowledge, and analogizing to specific but far-flung knowledge.",188,"Symbolic artificial intelligence
Knowledge-based systems

As limitations with weak, domain-independent methods became more and more apparent, researchers from all three traditions began to build knowledge into AI applications. The knowledge revolution was driven by the realization that knowledge underlies high-performance, domain-specific AI applications.
Edward Feigenbaum said:

""In the knowledge lies the power.""to describe that high-performance in a specific domain required both general and highly domain-specific knowledge. Ed Feigenbaum and Doug Lenat called this The Knowledge Principle: 

(1) The Knowledge Principle: if a program is to perform a complex task well, it must know a great deal about the world in which it operates.(2) A plausible extension of that principle, called the Breadth Hypothesis: there are two additional abilities necessary for intelligent behavior in unexpected situations: falling back on increasingly general knowledge, and analogizing to specific but far-flung knowledge.","1. What is the Knowledge Principle?
2. What is the Breadth Hypothesis?","1. The Knowledge Principle is the idea that in order for a program to be able to do a complex task well, it must have a lot of knowledge about the world it is operating in.
2. The Breadth Hypothesis is the idea that in order for a program to be able to do a complex task well in unexpected situations, it must be able to fall back on increasingly general knowledge and analogize to specific but far-flung knowledge."
Symbolic artificial intelligence,Success with expert systems,"This ""knowledge revolution"" led to the development and deployment of expert systems (introduced by Edward Feigenbaum), the first commercially successful form of AI software.",37,"Symbolic artificial intelligence
Success with expert systems

This ""knowledge revolution"" led to the development and deployment of expert systems (introduced by Edward Feigenbaum), the first commercially successful form of AI software.","1. What is an expert system? 
2. What was the first commercially successful form of AI software? 
3. What led to the development and deployment of expert systems?","1. Expert systems are computer programs that use artificial intelligence to help humans make decisions. 
2. The first commercially successful form of AI software was expert systems. 
3. The development and deployment of expert systems was led by Edward Feigenbaum and his team of researchers at Stanford University."
Symbolic artificial intelligence,Examples,"Key expert systems were:

DENDRAL, which found the structure of organic molecules from their chemical formula and mass spectrometer readings.
MYCIN, which diagnosed bacteremia – and suggested further lab tests, when necessary – by interpreting lab results, patient history, and doctor observations. ""With about 450 rules, MYCIN was able to perform as well as some experts, and considerably better than junior doctors.""
INTERNIST and CADUCEUS which tackled internal medicine diagnosis. Internist attempted to capture the expertise of the chairman of internal medicine at the University of Pittsburgh School of Medicine while CADUCEUS could eventually diagnose up to 1000 different diseases.
GUIDON, which showed how a knowledge base built for expert problem solving could be repurposed for teaching.
XCON, to configure VAX computers, a then laborious process that could take up to 90 days. XCON reduced the time to about 90 minutes.DENDRAL is considered the first expert system that relied on knowledge-intensive problem-solving. It is described below, by Ed Feigenbaum, from a Communications of the ACM interview, Interview with Ed Feigenbaum:

One of the people at Stanford interested in computer-based models of mind was Joshua Lederberg, the 1958 Nobel Prize winner in genetics. When I told him I wanted an induction ""sandbox"", he said, ""I have just the one for you."" His lab was doing mass spectrometry of amino acids. The question was: how do you go from looking at a spectrum of an amino acid to the chemical structure of the amino acid? That's how we started the DENDRAL Project: I was good at heuristic search methods, and he had an algorithm which was good at generating the chemical problem space.
We did not have a grandiose vision. We worked bottom up. Our chemist was Carl Djerassi, inventor of the chemical behind the birth control pill, and also one of the world's most respected mass spectrometrists. Carl and his postdocs were world-class experts in mass spectrometry. We began to add in their knowledge, inventing knowledge engineering as we were going along. These experiments amounted to titrating into DENDRAL more and more knowledge. The more you did that, the smarter the program became. We had very good results.

The generalization was: in the knowledge lies the power. That was the big idea. In my career that is the huge, ""Ah ha!,"" and it wasn't the way AI was being done previously. Sounds simple, but it's probably AI's most powerful generalization.
The other expert systems mentioned above came after DENDRAL. MYCIN exemplifies the classic expert system architecture of a knowledge-base of rules coupled to a symbolic reasoning mechanism, including the use of certainty factors to handle uncertainty. GUIDON shows how an explicit knowledge base can be repurposed for a second application, tutoring, and is an example of an intelligent tutoring system, a particular kind of knowledge-based application. Clancey showed that it was not sufficient simply to use MYCIN's rules for instruction, but that he also needed to add rules for dialogue management and student modeling. XCON is significant because of the millions of dollars it saved DEC, which triggered the expert system boom where most all major corporations in the US had expert systems groups, with the aim to capture corporate expertise, preserve it, and automate it:

By 1988, DEC's AI group had 40 expert systems deployed, with more on the way. DuPont had 100 in use and 500 in development. Nearly every major U.S. corporation had its own Al group and was either using or investigating expert systems.
Chess expert knowledge was encoded in Deep Blue. In 1996, this allowed IBM's Deep Blue, with the help of symbolic AI, to win in a game of chess against the world champion at that time, Garry Kasparov.",821,"Symbolic artificial intelligence
Examples

Key expert systems were:

DENDRAL, which found the structure of organic molecules from their chemical formula and mass spectrometer readings.
MYCIN, which diagnosed bacteremia – and suggested further lab tests, when necessary – by interpreting lab results, patient history, and doctor observations. ""With about 450 rules, MYCIN was able to perform as well as some experts, and considerably better than junior doctors.""
INTERNIST and CADUCEUS which tackled internal medicine diagnosis. Internist attempted to capture the expertise of the chairman of internal medicine at the University of Pittsburgh School of Medicine while CADUCEUS could eventually diagnose up to 1000 different diseases.
GUIDON, which showed how a knowledge base built for expert problem solving could be repurposed for teaching.
XCON, to configure VAX computers, a then laborious process that could take up to 90 days. XCON reduced the time to about 90 minutes.DENDRAL is considered the first expert system that relied on knowledge-intensive problem-solving. It is described below, by Ed Feigenbaum, from a Communications of the ACM interview, Interview with Ed Feigenbaum:

One of the people at Stanford interested in computer-based models of mind was Joshua Lederberg, the 1958 Nobel Prize winner in genetics. When I told him I wanted an induction ""sandbox"", he said, ""I have just the one for you."" His lab was doing mass spectrometry of amino acids. The question was: how do you go from looking at a spectrum of an amino acid to the chemical structure of the amino acid? That's how we started the DENDRAL Project: I was good at heuristic search methods, and he had an algorithm which was good at generating the chemical problem space.
We did not have a grandiose vision. We worked bottom up. Our chemist was Carl Djerassi, inventor of the chemical behind the birth control pill, and also one of the world's most respected mass spectrometrists. Carl and his postdocs were world-class experts in mass spectrometry. We began to add in their knowledge, inventing knowledge engineering as we were going along. These experiments amounted to titrating into DENDRAL more and more knowledge. The more you did that, the smarter the program became. We had very good results.

The generalization was: in the knowledge lies the power. That was the big idea. In my career that is the huge, ""Ah ha!,"" and it wasn't the way AI was being done previously. Sounds simple, but it's probably AI's most powerful generalization.
The other expert systems mentioned above came after DENDRAL. MYCIN exemplifies the classic expert system architecture of a knowledge-base of rules coupled to a symbolic reasoning mechanism, including the use of certainty factors to handle uncertainty. GUIDON shows how an explicit knowledge base can be repurposed for a second application, tutoring, and is an example of an intelligent tutoring system, a particular kind of knowledge-based application. Clancey showed that it was not sufficient simply to use MYCIN's rules for instruction, but that he also needed to add rules for dialogue management and student modeling. XCON is significant because of the millions of dollars it saved DEC, which triggered the expert system boom where most all major corporations in the US had expert systems groups, with the aim to capture corporate expertise, preserve it, and automate it:

By 1988, DEC's AI group had 40 expert systems deployed, with more on the way. DuPont had 100 in use and 500 in development. Nearly every major U.S. corporation had its own Al group and was either using or investigating expert systems.
Chess expert knowledge was encoded in Deep Blue. In 1996, this allowed IBM's Deep Blue, with the help of symbolic AI, to win in a game of chess against the world champion at that time, Garry Kasparov.","1. What is an expert system?
2. What was the first expert system?
3. What was the goal of the DENDRAL project?
4. What is the significance of XCON?
5. How did Clancey use MYCIN's rules for instruction?","1. Expert systems are computer programs that use knowledge to solve problems that are difficult for humans to solve.
2. The first expert system was DENDRAL.
3. The goal of the DENDRAL project was to create a computer program that could solve problems in organic chemistry by using the knowledge of world-class experts in mass spectrometry.
4. The significance of XCON is that it showed that it was possible to save millions of dollars by automating a process that had previously been done by hand.
5. Clancey used MYCIN's rules for instruction by adding rules for dialogue management and student modeling."
Symbolic artificial intelligence,Architecture of knowledge-based and expert systems,"A key component of the system architecture for all expert systems is the knowledge base, which stores facts and rules for problem-solving.
The simplest approach for an expert system knowledge base is simply a collection or network of production rules. Production rules connect symbols in a relationship similar to an If-Then statement. The expert system processes the rules to make deductions and to determine what additional information it needs, i.e. what questions to ask, using human-readable symbols. For example, OPS5, CLIPS and their successors Jess and Drools operate in this fashion.
Expert systems can operate in either a forward chaining – from evidence to conclusions – or backward chaining – from goals to needed data and prerequisites – manner. More advanced knowledge-based systems, such as Soar can also perform meta-level reasoning, that is reasoning about their own reasoning in terms of deciding how to solve problems and monitoring the success of problem-solving strategies. 
Blackboard systems are a second kind of knowledge-based or expert system architecture. They model a community of experts incrementally contributing, where they can, to solve a problem. The problem is represented in multiple levels of abstraction or alternate views. The experts (knowledge sources) volunteer their services whenever they recognize they can make a contribution. Potential problem-solving actions are represented on an agenda that is updated as the problem situation changes. A controller decides how useful each contribution is, and who should make the next problem-solving action. One example, the BB1 blackboard architecture was originally inspired by studies of how humans plan to perform multiple tasks in a trip. An innovation of BB1 was to apply the same blackboard model to solving its own control problem, i.e., its controller performed meta-level reasoning with knowledge sources that monitored how well a plan or the problem-solving was proceeding, and could switch from one strategy to another as conditions – such as goals or times – changed. BB1 was applied in multiple domains: construction site planning, intelligent tutoring systems, and real-time patient monitoring.",430,"Symbolic artificial intelligence
Architecture of knowledge-based and expert systems

A key component of the system architecture for all expert systems is the knowledge base, which stores facts and rules for problem-solving.
The simplest approach for an expert system knowledge base is simply a collection or network of production rules. Production rules connect symbols in a relationship similar to an If-Then statement. The expert system processes the rules to make deductions and to determine what additional information it needs, i.e. what questions to ask, using human-readable symbols. For example, OPS5, CLIPS and their successors Jess and Drools operate in this fashion.
Expert systems can operate in either a forward chaining – from evidence to conclusions – or backward chaining – from goals to needed data and prerequisites – manner. More advanced knowledge-based systems, such as Soar can also perform meta-level reasoning, that is reasoning about their own reasoning in terms of deciding how to solve problems and monitoring the success of problem-solving strategies. 
Blackboard systems are a second kind of knowledge-based or expert system architecture. They model a community of experts incrementally contributing, where they can, to solve a problem. The problem is represented in multiple levels of abstraction or alternate views. The experts (knowledge sources) volunteer their services whenever they recognize they can make a contribution. Potential problem-solving actions are represented on an agenda that is updated as the problem situation changes. A controller decides how useful each contribution is, and who should make the next problem-solving action. One example, the BB1 blackboard architecture was originally inspired by studies of how humans plan to perform multiple tasks in a trip. An innovation of BB1 was to apply the same blackboard model to solving its own control problem, i.e., its controller performed meta-level reasoning with knowledge sources that monitored how well a plan or the problem-solving was proceeding, and could switch from one strategy to another as conditions – such as goals or times – changed. BB1 was applied in multiple domains: construction site planning, intelligent tutoring systems, and real-time patient monitoring.","1. What is a key component of the system architecture for all expert systems?
2. What is the simplest approach for an expert system knowledge base?
3. What are production rules?
4. What is OPS5?
5. What is CLIPS?
6. What is Jess?
7. What is Drools?
8. What is a blackboard system?
9. What is BB1?
10. What is a controller?","1. A key component of the system architecture for all expert systems is the knowledge base.
2. The simplest approach for an expert system knowledge base is simply a collection or network of production rules.
3. Production rules connect symbols in a relationship similar to an If-Then statement.
4. OPS5 is a production rule system.
5. CLIPS is a production rule system.
6. Jess is a production rule system.
7. Drools is a production rule system.
8. A blackboard system is a knowledge-based system in which experts incrementally contribute to solving a problem.
9. BB1 is a blackboard system.
10. A controller is a component of a blackboard system that decides how useful each contribution is, and who should make the next problem-solving action."
Symbolic artificial intelligence,"The second AI winter, 1988–1993","At the height of the AI boom, companies such as Symbolics, LMI, and Texas Instruments were selling LISP machines specifically targeted to accelerate the development of AI applications and research. In addition, several artificial intelligence companies, such as Teknowledge and Inference Corporation, were selling expert system shells, training, and consulting to corporations. 
Unfortunately, the AI boom did not last and Kautz best describes the second AI winter that followed:

Many reasons can be offered for the arrival of the second AI winter. The hardware companies failed when much more cost-effective general Unix workstations from Sun together with good compilers for LISP and Prolog came onto the market. Many commercial deployments of expert systems were discontinued when they proved too costly to maintain. Medical expert systems never caught on for several reasons: the difficulty in keeping them up to date; the challenge for medical professionals to learn how to use a bewildering variety of different expert systems for different medical conditions; and perhaps most crucially, the reluctance of doctors to trust a computer-made diagnosis over their gut instinct, even for specific domains where the expert systems could outperform an average doctor. Venture capital money deserted AI practically overnight. The world AI conference IJCAI hosted an enormous and lavish trade show and thousands of nonacademic attendees in 1987 in Vancouver; the main AI conference the following year, AAAI 1988 in St. Paul, was a small and strictly academic affair.",303,"Symbolic artificial intelligence
The second AI winter, 1988–1993

At the height of the AI boom, companies such as Symbolics, LMI, and Texas Instruments were selling LISP machines specifically targeted to accelerate the development of AI applications and research. In addition, several artificial intelligence companies, such as Teknowledge and Inference Corporation, were selling expert system shells, training, and consulting to corporations. 
Unfortunately, the AI boom did not last and Kautz best describes the second AI winter that followed:

Many reasons can be offered for the arrival of the second AI winter. The hardware companies failed when much more cost-effective general Unix workstations from Sun together with good compilers for LISP and Prolog came onto the market. Many commercial deployments of expert systems were discontinued when they proved too costly to maintain. Medical expert systems never caught on for several reasons: the difficulty in keeping them up to date; the challenge for medical professionals to learn how to use a bewildering variety of different expert systems for different medical conditions; and perhaps most crucially, the reluctance of doctors to trust a computer-made diagnosis over their gut instinct, even for specific domains where the expert systems could outperform an average doctor. Venture capital money deserted AI practically overnight. The world AI conference IJCAI hosted an enormous and lavish trade show and thousands of nonacademic attendees in 1987 in Vancouver; the main AI conference the following year, AAAI 1988 in St. Paul, was a small and strictly academic affair.","1. What was the AI boom?
2. What caused the second AI winter?
3. What were some reasons for the medical expert systems not catching on?","1. The AI boom was a time when many people were investing in and working on artificial intelligence.
2. The second AI winter was a time when many people stopped investing in and working on artificial intelligence.
3. Some reasons for the medical expert systems not catching on are that they were difficult to keep up to date, doctors were reluctant to trust them, and there were many different expert systems for different medical conditions."
Symbolic artificial intelligence,Uncertain reasoning,"Both statistical approaches and extensions to logic were tried. 
One statistical approach, Hidden Markov Models, had already been popularized in the 1980s for speech recognition work. Subsequently, in 1988, Judea Pearl popularized the use of Bayesian Networks as a sound but efficient way of handling uncertain reasoning with his publication of the book Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. and Bayesian approaches were applied successfully in expert systems. Even later, in the 1990s, statistical relational learning, an approach that combines probability with logical formulas, allowed probability to be combined with first-order logic, e.g., with either Markov Logic Networks or Probabilistic Soft Logic.
Other, non-probabilistic extensions to first-order logic to support were also tried. For example, non-monotonic reasoning could be used with truth maintenance systems. A truth maintenance system tracked assumptions and justifications for all inferences. It allowed inferences to be withdrawn when assumptions were found out to be incorrect or a contradiction was derived. Explanations could be provided for an inference by explaining which rules were applied to create it and then continuing through underlying inferences and rules all the way back to root assumptions. Lofti Zadeh had introduced a different kind of extension to handle the representation of vagueness. For example, in deciding how ""heavy"" or ""tall"" a man is, there is frequently no clear ""yes"" or ""no"" answer, and a predicate for heavy or tall would instead return values between 0 and 1. Those values represented to what degree the predicates were true. His fuzzy logic further provided a means for propagating combinations of these values through logical formulas.",352,"Symbolic artificial intelligence
Uncertain reasoning

Both statistical approaches and extensions to logic were tried. 
One statistical approach, Hidden Markov Models, had already been popularized in the 1980s for speech recognition work. Subsequently, in 1988, Judea Pearl popularized the use of Bayesian Networks as a sound but efficient way of handling uncertain reasoning with his publication of the book Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. and Bayesian approaches were applied successfully in expert systems. Even later, in the 1990s, statistical relational learning, an approach that combines probability with logical formulas, allowed probability to be combined with first-order logic, e.g., with either Markov Logic Networks or Probabilistic Soft Logic.
Other, non-probabilistic extensions to first-order logic to support were also tried. For example, non-monotonic reasoning could be used with truth maintenance systems. A truth maintenance system tracked assumptions and justifications for all inferences. It allowed inferences to be withdrawn when assumptions were found out to be incorrect or a contradiction was derived. Explanations could be provided for an inference by explaining which rules were applied to create it and then continuing through underlying inferences and rules all the way back to root assumptions. Lofti Zadeh had introduced a different kind of extension to handle the representation of vagueness. For example, in deciding how ""heavy"" or ""tall"" a man is, there is frequently no clear ""yes"" or ""no"" answer, and a predicate for heavy or tall would instead return values between 0 and 1. Those values represented to what degree the predicates were true. His fuzzy logic further provided a means for propagating combinations of these values through logical formulas.","1. What is the main difference between statistical approaches and extensions to logic?
2. What is the main advantage of Bayesian Networks?
3. What is the main difference between probabilistic and non-probabilistic extensions to logic?
4. What is fuzzy logic?","1. Statistical approaches rely on probability, while extensions to logic do not.
2. Bayesian Networks are efficient in handling uncertain reasoning.
3. Probabilistic extensions to logic allow for vagueness, while non-probabilistic extensions do not.
4. Fuzzy logic is a way of propagating combinations of values through logical formulas."
Symbolic artificial intelligence,Machine learning,"Symbolic machine learning approaches were investigated to address the knowledge acquisition bottleneck. One of the earliest is Meta-DENDRAL. Meta-DENDRAL used a generate-and-test technique to generate plausible rule hypotheses to test against spectra. Domain and task knowledge reduced the number of candidates tested to a manageable size. Feigenbaum described Meta-DENDRAL as

...the culmination of my dream of the early to mid-1960s having to do with theory formation. The conception was that you had a problem solver like DENDRAL that took some inputs and produced an output. In doing so, it used layers of knowledge to steer and prune the search. That knowledge got in there because we interviewed people. But how did the people get the knowledge? By looking at thousands of spectra. So we wanted a program that would look at thousands of spectra and infer the knowledge of mass spectrometry that DENDRAL could use to solve individual hypothesis formation problems.
We did it. We were even able to publish new knowledge of mass spectrometry in the Journal of the American Chemical Society, giving credit only in a footnote that a program, Meta-DENDRAL, actually did it. We were able to do something that had been a dream: to have a computer program come up with a new and publishable piece of science.
In contrast to the knowledge-intensive approach of Meta-DENDRAL, Ross Quinlan invented a domain-independent approach to statistical classification, decision tree learning, starting first with ID3 and then later extending its capabilities to C4.5. The decision trees created are glass box, interpretable classifiers, with human-interpretable classification rules. 
Advances were made in understanding machine learning theory, too. Tom Mitchell introduced version space learning which describes learning as search through a space of hypotheses, with upper, more general, and lower, more specific, boundaries encompassing all viable hypotheses consistent with the examples seen so far. More formally, Valiant introduced Probably Approximately Correct Learning (PAC Learning), a framework for the mathematical analysis of machine learning.Symbolic machine learning encompassed more than learning by example. E.g., John Anderson provided a cognitive model of human learning where skill practice results in a compilation of rules from a declarative format to a procedural format with his ACT-R cognitive architecture. For example, a student might learn to apply ""Supplementary angles are two angles whose measures sum 180 degrees"" as several different procedural rules. E.g., one rule might say that if X and Y are supplementary and you know X, then Y will be 180 - X. He called his approach ""knowledge compilation"". ACT-R has been used successfully to model aspects of human cognition, such as learning and retention. ACT-R is also used in intelligent tutoring systems, called cognitive tutors, to successfully teach geometry, computer programming, and algebra to school children.Inductive logic programming was another approach to learning that allowed logic programs to be synthesized from input-output examples. E.g., Ehud Shapiro's MIS (Model Inference System) could synthesize Prolog programs from examples. John R. Koza applied genetic algorithms to program synthesis to create genetic programming, which he used to synthesize LISP programs. Finally, Zohar Manna and Richard Waldinger provided a more general approach to program synthesis that synthesizes a functional program in the course of proving its specifications to be correct.As an alternative to logic, Roger Schank introduced case-based reasoning (CBR). The CBR approach outlined in his book, Dynamic Memory, focuses first on remembering key problem-solving cases for future use and generalizing them where appropriate. When faced with a new problem, CBR retrieves the most similar previous case and adapts it to the specifics of the current problem. Another alternative to logic, genetic algorithms and genetic programming are based on an evolutionary model of learning, where sets of rules are encoded into populations, the rules govern the behavior of individuals, and selection of the fittest prunes out sets of unsuitable rules over many generations.Symbolic machine learning was applied to learning concepts, rules, heuristics, and problem-solving. Approaches, other than those above, include:

Learning from instruction or advice—i.e., taking human instruction, posed as advice, and determining how to operationalize it in specific situations. For example, in a game of Hearts, learning exactly how to play a hand to ""avoid taking points.""
Learning from exemplars—improving performance by accepting subject-matter expert (SME) feedback during training. When problem-solving fails, querying the expert to either learn a new exemplar for problem-solving or to learn a new explanation as to exactly why one exemplar is more relevant than another. For example, the program Protos learned to diagnose tinnitus cases by interacting with an audiologist.
Learning by analogy—constructing problem solutions based on similar problems seen in the past, and then modifying their solutions to fit a new situation or domain.
Apprentice learning systems—learning novel solutions to problems by observing human problem-solving. Domain knowledge explains why novel solutions are correct and how the solution can be generalized. LEAP learned how to design VLSI circuits by observing human designers.
Learning by discovery—i.e., creating tasks to carry out experiments and then learning from the results. Doug Lenat's Eurisko, for example, learned heuristics to beat human players at the Traveller role-playing game for two years in a row. 
Learning macro-operators—i.e., searching for useful macro-operators to be learned from sequences of basic problem-solving actions. Good macro-operators simplify problem-solving by allowing problems to be solved at a more abstract level.",1203,"Symbolic artificial intelligence
Machine learning

Symbolic machine learning approaches were investigated to address the knowledge acquisition bottleneck. One of the earliest is Meta-DENDRAL. Meta-DENDRAL used a generate-and-test technique to generate plausible rule hypotheses to test against spectra. Domain and task knowledge reduced the number of candidates tested to a manageable size. Feigenbaum described Meta-DENDRAL as

...the culmination of my dream of the early to mid-1960s having to do with theory formation. The conception was that you had a problem solver like DENDRAL that took some inputs and produced an output. In doing so, it used layers of knowledge to steer and prune the search. That knowledge got in there because we interviewed people. But how did the people get the knowledge? By looking at thousands of spectra. So we wanted a program that would look at thousands of spectra and infer the knowledge of mass spectrometry that DENDRAL could use to solve individual hypothesis formation problems.
We did it. We were even able to publish new knowledge of mass spectrometry in the Journal of the American Chemical Society, giving credit only in a footnote that a program, Meta-DENDRAL, actually did it. We were able to do something that had been a dream: to have a computer program come up with a new and publishable piece of science.
In contrast to the knowledge-intensive approach of Meta-DENDRAL, Ross Quinlan invented a domain-independent approach to statistical classification, decision tree learning, starting first with ID3 and then later extending its capabilities to C4.5. The decision trees created are glass box, interpretable classifiers, with human-interpretable classification rules. 
Advances were made in understanding machine learning theory, too. Tom Mitchell introduced version space learning which describes learning as search through a space of hypotheses, with upper, more general, and lower, more specific, boundaries encompassing all viable hypotheses consistent with the examples seen so far. More formally, Valiant introduced Probably Approximately Correct Learning (PAC Learning), a framework for the mathematical analysis of machine learning.Symbolic machine learning encompassed more than learning by example. E.g., John Anderson provided a cognitive model of human learning where skill practice results in a compilation of rules from a declarative format to a procedural format with his ACT-R cognitive architecture. For example, a student might learn to apply ""Supplementary angles are two angles whose measures sum 180 degrees"" as several different procedural rules. E.g., one rule might say that if X and Y are supplementary and you know X, then Y will be 180 - X. He called his approach ""knowledge compilation"". ACT-R has been used successfully to model aspects of human cognition, such as learning and retention. ACT-R is also used in intelligent tutoring systems, called cognitive tutors, to successfully teach geometry, computer programming, and algebra to school children.Inductive logic programming was another approach to learning that allowed logic programs to be synthesized from input-output examples. E.g., Ehud Shapiro's MIS (Model Inference System) could synthesize Prolog programs from examples. John R. Koza applied genetic algorithms to program synthesis to create genetic programming, which he used to synthesize LISP programs. Finally, Zohar Manna and Richard Waldinger provided a more general approach to program synthesis that synthesizes a functional program in the course of proving its specifications to be correct.As an alternative to logic, Roger Schank introduced case-based reasoning (CBR). The CBR approach outlined in his book, Dynamic Memory, focuses first on remembering key problem-solving cases for future use and generalizing them where appropriate. When faced with a new problem, CBR retrieves the most similar previous case and adapts it to the specifics of the current problem. Another alternative to logic, genetic algorithms and genetic programming are based on an evolutionary model of learning, where sets of rules are encoded into populations, the rules govern the behavior of individuals, and selection of the fittest prunes out sets of unsuitable rules over many generations.Symbolic machine learning was applied to learning concepts, rules, heuristics, and problem-solving. Approaches, other than those above, include:

Learning from instruction or advice—i.e., taking human instruction, posed as advice, and determining how to operationalize it in specific situations. For example, in a game of Hearts, learning exactly how to play a hand to ""avoid taking points.""
Learning from exemplars—improving performance by accepting subject-matter expert (SME) feedback during training. When problem-solving fails, querying the expert to either learn a new exemplar for problem-solving or to learn a new explanation as to exactly why one exemplar is more relevant than another. For example, the program Protos learned to diagnose tinnitus cases by interacting with an audiologist.
Learning by analogy—constructing problem solutions based on similar problems seen in the past, and then modifying their solutions to fit a new situation or domain.
Apprentice learning systems—learning novel solutions to problems by observing human problem-solving. Domain knowledge explains why novel solutions are correct and how the solution can be generalized. LEAP learned how to design VLSI circuits by observing human designers.
Learning by discovery—i.e., creating tasks to carry out experiments and then learning from the results. Doug Lenat's Eurisko, for example, learned heuristics to beat human players at the Traveller role-playing game for two years in a row. 
Learning macro-operators—i.e., searching for useful macro-operators to be learned from sequences of basic problem-solving actions. Good macro-operators simplify problem-solving by allowing problems to be solved at a more abstract level.","1. What is the difference between symbolic machine learning and machine learning?
2. What is the difference between symbolic machine learning and knowledge-based machine learning?
3. What is the difference between symbolic machine learning and cognitive science?
4. What is the difference between symbolic machine learning and artificial intelligence?
5. What is the difference between symbolic machine learning and problem solving?","1. Symbolic machine learning is a subfield of machine learning that deals with the acquisition of knowledge using symbols. Machine learning is a subfield of artificial intelligence that deals with the acquisition of knowledge using algorithms.
2. Symbolic machine learning is a subfield of machine learning that deals with the acquisition of knowledge using symbols. Knowledge-based machine learning is a subfield of machine learning that deals with the acquisition of knowledge using rules.
3. Symbolic machine learning is a subfield of machine learning that deals with the acquisition of knowledge using symbols. Cognitive science is the study of the mind and its processes.
4. Symbolic machine learning is a subfield of machine learning that deals with the acquisition of knowledge using symbols. Artificial intelligence is the study of how to create intelligent agents.
5. Symbolic machine learning is a subfield of machine learning that deals with the acquisition of knowledge using symbols. Problem solving is the process of finding a solution to a problem."
Symbolic artificial intelligence,Deep learning and neuro-symbolic AI 2011–now,"With the rise of deep learning, the symbolic AI approach has been compared to deep learning as complementary ""...with parallels having been drawn many times by AI researchers between Kahneman's research on human reasoning and decision making – reflected in his book Thinking, Fast and Slow – and the so-called ""AI systems 1 and 2"", which would in principle be modelled by deep learning and symbolic reasoning, respectively."" In this view, symbolic reasoning is more apt for deliberative reasoning, planning, and explanation while deep learning is more apt for fast pattern recognition in perceptual applications with noisy data.",126,"Symbolic artificial intelligence
Deep learning and neuro-symbolic AI 2011–now

With the rise of deep learning, the symbolic AI approach has been compared to deep learning as complementary ""...with parallels having been drawn many times by AI researchers between Kahneman's research on human reasoning and decision making – reflected in his book Thinking, Fast and Slow – and the so-called ""AI systems 1 and 2"", which would in principle be modelled by deep learning and symbolic reasoning, respectively."" In this view, symbolic reasoning is more apt for deliberative reasoning, planning, and explanation while deep learning is more apt for fast pattern recognition in perceptual applications with noisy data.","1. What is the difference between deep learning and symbolic AI?
2. What are the benefits of using deep learning?
3. What are the benefits of using symbolic AI?","1. Deep learning is a type of machine learning that is based on neural networks, while symbolic AI is a type of machine learning that is based on rules and logic.
2. Deep learning is more apt for fast pattern recognition in perceptual applications with noisy data, while symbolic AI is more apt for deliberative reasoning, planning, and explanation.
3. The benefits of using deep learning include improved accuracy and performance, as well as the ability to learn from data that is too noisy or complex for symbolic AI to handle."
Symbolic artificial intelligence,Neuro-symbolic AI: integrating neural and symbolic approaches,"Neuro-symbolic AI attempts to integrate neural and symbolic architectures in a manner that addresses strengths and weaknesses of each, in a complementary fashion, in order to support robust AI capable of reasoning, learning, and cognitive modeling. As argued by Valiant and many others, the effective construction of rich computational cognitive models demands the combination of sound symbolic reasoning and efficient (machine) learning models. Gary Marcus, similarly, argues that: ""We cannot construct rich cognitive models in an adequate, automated way without the triumvirate of hybrid architecture, rich prior knowledge, and sophisticated techniques for reasoning."", and in particular:
""To build a robust, knowledge-driven approach to AI we must have the machinery of symbol-manipulation in our toolkit. Too much of useful knowledge is abstract to make do without tools that represent and manipulate abstraction, and to date, the only machinery that we know of that can manipulate such abstract knowledge reliably is the apparatus of symbol-manipulation.""Henry Kautz, Francesca Rossi, and Bart Selman have also argued for a synthesis. Their arguments are based on a need to address the two kinds of thinking discussed in Daniel Kahneman's book, Thinking, Fast and Slow. Kahneman describes human thinking as having two components, System 1 and System 2. System 1 is fast, automatic, intuitive and unconscious. System 2 is slower, step-by-step, and explicit. System 1 is the kind used for pattern recognition while System 2 is far better suited for planning, deduction, and deliberative thinking. In this view, deep learning best models the first kind of thinking while symbolic reasoning best models the second kind and both are needed.
Garcez describes research in this area as being ongoing for at least the past twenty years, dating from his 2002 book on neurosymbolic learning systems. A series of workshops on neuro-symbolic reasoning has been held every year since 2005, see http://www.neural-symbolic.org/ for details.
In their 2015 paper, Neural-Symbolic Learning and Reasoning: Contributions and Challenges, Garcez et al. argue that:

The integration of the symbolic and connectionist paradigms of AI has been pursued by a relatively small research community over the last two decades and has yielded several significant results. Over the last decade, neural symbolic systems have been shown capable of overcoming the so-called propositional fixation of neural networks, as McCarthy (1988) put it in response to Smolensky (1988); see also (Hinton, 1990). Neural networks were shown capable of representing modal and temporal logics (d'Avila Garcez and Lamb, 2006) and fragments of first-order logic (Bader, Hitzler, Hölldobler, 2008; d'Avila Garcez, Lamb, Gabbay, 2009). Further, neural-symbolic systems have been applied to a number of problems in the areas of bioinformatics, control engineering, software verification and adaptation, visual intelligence, ontology learning, and computer games.
Approaches for integration are varied. Henry Kautz's taxonomy of neuro-symbolic architectures, along with some examples, follows:

Symbolic Neural symbolic—is the current approach of many neural models in natural language processing, where words or subword tokens are both the ultimate input and output of large language models. Examples include BERT, RoBERTa, and GPT-3.
Symbolic[Neural]—is exemplified by AlphaGo, where symbolic techniques are used to call neural techniques. In this case the symbolic approach is Monte Carlo tree search and the neural techniques learn how to evaluate game positions.
Neural|Symbolic—uses a neural architecture to interpret perceptual data as symbols and relationships that are then reasoned about symbolically.
Neural:Symbolic → Neural—relies on symbolic reasoning to generate or label training data that is subsequently learned by a deep learning model, e.g., to train a neural model for symbolic computation by using a Macsyma-like symbolic mathematics system to create or label examples.
Neural_{Symbolic}—uses a neural net that is generated from symbolic rules. An example is the Neural Theorem Prover, which constructs a neural network from an AND-OR proof tree generated from knowledge base rules and terms. Logic Tensor Networks also fall into this category.
Neural[Symbolic]—allows a neural model to directly call a symbolic reasoning engine, e.g., to perform an action or evaluate a state.Many key research questions remain, such as:

What is the best way to integrate neural and symbolic architectures?
How should symbolic structures be represented within neural networks and extracted from them?
How should common-sense knowledge be learned and reasoned about?
How can abstract knowledge that is hard to encode logically be handled?",1023,"Symbolic artificial intelligence
Neuro-symbolic AI: integrating neural and symbolic approaches

Neuro-symbolic AI attempts to integrate neural and symbolic architectures in a manner that addresses strengths and weaknesses of each, in a complementary fashion, in order to support robust AI capable of reasoning, learning, and cognitive modeling. As argued by Valiant and many others, the effective construction of rich computational cognitive models demands the combination of sound symbolic reasoning and efficient (machine) learning models. Gary Marcus, similarly, argues that: ""We cannot construct rich cognitive models in an adequate, automated way without the triumvirate of hybrid architecture, rich prior knowledge, and sophisticated techniques for reasoning."", and in particular:
""To build a robust, knowledge-driven approach to AI we must have the machinery of symbol-manipulation in our toolkit. Too much of useful knowledge is abstract to make do without tools that represent and manipulate abstraction, and to date, the only machinery that we know of that can manipulate such abstract knowledge reliably is the apparatus of symbol-manipulation.""Henry Kautz, Francesca Rossi, and Bart Selman have also argued for a synthesis. Their arguments are based on a need to address the two kinds of thinking discussed in Daniel Kahneman's book, Thinking, Fast and Slow. Kahneman describes human thinking as having two components, System 1 and System 2. System 1 is fast, automatic, intuitive and unconscious. System 2 is slower, step-by-step, and explicit. System 1 is the kind used for pattern recognition while System 2 is far better suited for planning, deduction, and deliberative thinking. In this view, deep learning best models the first kind of thinking while symbolic reasoning best models the second kind and both are needed.
Garcez describes research in this area as being ongoing for at least the past twenty years, dating from his 2002 book on neurosymbolic learning systems. A series of workshops on neuro-symbolic reasoning has been held every year since 2005, see http://www.neural-symbolic.org/ for details.
In their 2015 paper, Neural-Symbolic Learning and Reasoning: Contributions and Challenges, Garcez et al. argue that:

The integration of the symbolic and connectionist paradigms of AI has been pursued by a relatively small research community over the last two decades and has yielded several significant results. Over the last decade, neural symbolic systems have been shown capable of overcoming the so-called propositional fixation of neural networks, as McCarthy (1988) put it in response to Smolensky (1988); see also (Hinton, 1990). Neural networks were shown capable of representing modal and temporal logics (d'Avila Garcez and Lamb, 2006) and fragments of first-order logic (Bader, Hitzler, Hölldobler, 2008; d'Avila Garcez, Lamb, Gabbay, 2009). Further, neural-symbolic systems have been applied to a number of problems in the areas of bioinformatics, control engineering, software verification and adaptation, visual intelligence, ontology learning, and computer games.
Approaches for integration are varied. Henry Kautz's taxonomy of neuro-symbolic architectures, along with some examples, follows:

Symbolic Neural symbolic—is the current approach of many neural models in natural language processing, where words or subword tokens are both the ultimate input and output of large language models. Examples include BERT, RoBERTa, and GPT-3.
Symbolic[Neural]—is exemplified by AlphaGo, where symbolic techniques are used to call neural techniques. In this case the symbolic approach is Monte Carlo tree search and the neural techniques learn how to evaluate game positions.
Neural|Symbolic—uses a neural architecture to interpret perceptual data as symbols and relationships that are then reasoned about symbolically.
Neural:Symbolic → Neural—relies on symbolic reasoning to generate or label training data that is subsequently learned by a deep learning model, e.g., to train a neural model for symbolic computation by using a Macsyma-like symbolic mathematics system to create or label examples.
Neural_{Symbolic}—uses a neural net that is generated from symbolic rules. An example is the Neural Theorem Prover, which constructs a neural network from an AND-OR proof tree generated from knowledge base rules and terms. Logic Tensor Networks also fall into this category.
Neural[Symbolic]—allows a neural model to directly call a symbolic reasoning engine, e.g., to perform an action or evaluate a state.Many key research questions remain, such as:

What is the best way to integrate neural and symbolic architectures?
How should symbolic structures be represented within neural networks and extracted from them?
How should common-sense knowledge be learned and reasoned about?
How can abstract knowledge that is hard to encode logically be handled?","1. What is the goal of neuro-symbolic AI?
2. What are the benefits of integrating neural and symbolic architectures?
3. What are some of the challenges of neuro-symbolic AI?
4. What is the difference between System 1 and System 2 thinking?
5. How have neural-symbolic systems been applied to various tasks?
6. What is the taxonomy of neuro-symbolic architectures?
7. What are some of the unanswered","1. The goal of neuro-symbolic AI is to integrate neural and symbolic architectures in a complementary fashion in order to support robust AI capable of reasoning, learning, and cognitive modeling.
2. The benefits of integrating neural and symbolic architectures are that it allows for the combination of the strengths of each approach. Neural networks are good at pattern recognition and deep learning, while symbolic reasoning is good at planning, deduction, and deliberative thinking.
3. The challenges of neuro-symbolic AI are that it is a relatively new field and there are still many unanswered questions.
4. System 1 thinking is fast, automatic, intuitive, and unconscious. System 2 is slower, step-by-step, and explicit.
5. Neural-symbolic systems have been applied to a number of problems in the areas of bioinformatics, control engineering, software verification and adaptation, visual intelligence, ontology learning, and computer games.
6. The taxonomy of neuro-symbolic architectures is as follows:

Symbolic: Neural symbolic—is the current approach of many neural models in natural language processing, where words or subword tokens are both the ultimate input and output of large language models. Examples include BERT, RoBERTa, and"
Symbolic artificial intelligence,Techniques and contributions,"This section provides an overview of techniques and contributions in an overall context leading to many other, more detailed articles in Wikipedia. Sections on Machine Learning and Uncertain Reasoning are covered earlier in the history section.",45,"Symbolic artificial intelligence
Techniques and contributions

This section provides an overview of techniques and contributions in an overall context leading to many other, more detailed articles in Wikipedia. Sections on Machine Learning and Uncertain Reasoning are covered earlier in the history section.","1. What is symbolic artificial intelligence?
2. What are the techniques involved in symbolic artificial intelligence?
3. What are the contributions of symbolic artificial intelligence?","1. Symbolic artificial intelligence is a subfield of artificial intelligence that deals with the manipulation of symbols.
2. The techniques involved in symbolic artificial intelligence include propositional calculus, predicate calculus, and resolution theorem proving.
3. The contributions of symbolic artificial intelligence include the development of artificial intelligence, machine learning, and uncertain reasoning."
Symbolic artificial intelligence,AI programming languages,"The key AI programming language in the US during the last symbolic AI boom period was LISP. LISP is the second oldest programming language after FORTRAN and was created in 1958 by John McCarthy. LISP provided the first read-eval-print loop to support rapid program development. Compiled functions could be freely mixed with interpreted functions. Program tracing, stepping, and breakpoints were also provided, along with the ability to change values or functions and continue from breakpoints or errors. It had the first self-hosting compiler, meaning that the compiler itself was originally written in LISP and then ran interpretively to compile the compiler code.
Other key innovations pioneered by LISP that have spread to other programming languages include:

Garbage collection
Dynamic typing
Higher-order functions
Recursion
ConditionalsPrograms were themselves data structures that other programs could operate on, allowing the easy definition of higher-level languages. 
In contrast to the US, in Europe the key AI programming language during that same period was Prolog. Prolog provided a built-in store of facts and clauses that could be queried by a read-eval-print loop. The store could act as a knowledge base and the clauses could act as rules or a restricted form of logic. As a subset of first-order logic Prolog was based on Horn clauses with a closed-world assumption -- any facts not known were considered false -- and a unique name assumption for primitive terms -- e.g., the identifier barack_obama was considered to refer to exactly one object. Backtracking and unification are built-in to Prolog.
Alain Colmerauer and Philippe Roussel are credited as the inventors of Prolog. Prolog is a form of logic programming, which was invented by Robert Kowalski. Its history was also influenced by Carl Hewitt's PLANNER, an assertional database with pattern-directed invocation of methods. For more detail see the section on the origins of Prolog in the PLANNER article.
Prolog is also a kind of declarative programming. The logic clauses that describe programs are directly interpreted to run the programs specified. No explicit series of actions is required, as is the case with imperative programming languages.
Japan championed Prolog for its Fifth Generation Project, intending to build special hardware for high performance. Similarly, LISP machines were built to run LISP, but as the second AI boom turned to bust these companies could not compete with new workstations that could now run LISP or Prolog natively at comparable speeds. See the history section for more detail.
Smalltalk was another influential AI programming language. For example it introduced metaclasses and, along with Flavors and CommonLoops, influenced the Common Lisp Object System, or (CLOS), that is now part of Common Lisp, the current standard Lisp dialect. CLOS is a Lisp-based object-oriented system that allows multiple inheritance, in addition to incremental extensions to both classes and metaclasses, thus providing a run-time meta-object protocol.For other AI programming languages see this list of programming languages for artificial intelligence. Currently, Python, a multi-paradigm programming language, is the most popular programming language, partly due to its extensive package library that supports data science, natural language processing, and deep learning. Python includes a read-eval-print loop, functional elements such as higher-order functions, and object-oriented programming that includes metaclasses.",724,"Symbolic artificial intelligence
AI programming languages

The key AI programming language in the US during the last symbolic AI boom period was LISP. LISP is the second oldest programming language after FORTRAN and was created in 1958 by John McCarthy. LISP provided the first read-eval-print loop to support rapid program development. Compiled functions could be freely mixed with interpreted functions. Program tracing, stepping, and breakpoints were also provided, along with the ability to change values or functions and continue from breakpoints or errors. It had the first self-hosting compiler, meaning that the compiler itself was originally written in LISP and then ran interpretively to compile the compiler code.
Other key innovations pioneered by LISP that have spread to other programming languages include:

Garbage collection
Dynamic typing
Higher-order functions
Recursion
ConditionalsPrograms were themselves data structures that other programs could operate on, allowing the easy definition of higher-level languages. 
In contrast to the US, in Europe the key AI programming language during that same period was Prolog. Prolog provided a built-in store of facts and clauses that could be queried by a read-eval-print loop. The store could act as a knowledge base and the clauses could act as rules or a restricted form of logic. As a subset of first-order logic Prolog was based on Horn clauses with a closed-world assumption -- any facts not known were considered false -- and a unique name assumption for primitive terms -- e.g., the identifier barack_obama was considered to refer to exactly one object. Backtracking and unification are built-in to Prolog.
Alain Colmerauer and Philippe Roussel are credited as the inventors of Prolog. Prolog is a form of logic programming, which was invented by Robert Kowalski. Its history was also influenced by Carl Hewitt's PLANNER, an assertional database with pattern-directed invocation of methods. For more detail see the section on the origins of Prolog in the PLANNER article.
Prolog is also a kind of declarative programming. The logic clauses that describe programs are directly interpreted to run the programs specified. No explicit series of actions is required, as is the case with imperative programming languages.
Japan championed Prolog for its Fifth Generation Project, intending to build special hardware for high performance. Similarly, LISP machines were built to run LISP, but as the second AI boom turned to bust these companies could not compete with new workstations that could now run LISP or Prolog natively at comparable speeds. See the history section for more detail.
Smalltalk was another influential AI programming language. For example it introduced metaclasses and, along with Flavors and CommonLoops, influenced the Common Lisp Object System, or (CLOS), that is now part of Common Lisp, the current standard Lisp dialect. CLOS is a Lisp-based object-oriented system that allows multiple inheritance, in addition to incremental extensions to both classes and metaclasses, thus providing a run-time meta-object protocol.For other AI programming languages see this list of programming languages for artificial intelligence. Currently, Python, a multi-paradigm programming language, is the most popular programming language, partly due to its extensive package library that supports data science, natural language processing, and deep learning. Python includes a read-eval-print loop, functional elements such as higher-order functions, and object-oriented programming that includes metaclasses.","1. What was the key AI programming language in the US during the last symbolic AI boom period?
2. What key innovations did LISP pioneer that have spread to other programming languages?
3. What was the key AI programming language in Europe during that same period?
4. What was Japan's key AI programming language during the Fifth Generation Project?
5. What is Python, and why is it popular?","1. The key AI programming language in the US during the last symbolic AI boom period was LISP.
2. LISP pioneered garbage collection, dynamic typing, higher-order functions, and recursion, among other innovations that have since been adopted by other programming languages.
3. The key AI programming language in Europe during that same period was Prolog.
4. Japan's key AI programming language during the Fifth Generation Project was Prolog.
5. Python is a popular programming language that is known for its read-eval-print loop, functional elements, and object-oriented programming capabilities, including metaclasses."
Symbolic artificial intelligence,Search,"Search arises in many kinds of problem solving, including planning, constraint satisfaction, and playing games such as checkers, chess, and go. The best known AI-search tree search algorithms are breadth-first search, depth-first search, A*, and Monte Carlo Search. Key search algorithms for Boolean satisfiability are WalkSAT, conflict-driven clause learning, and the DPLL algorithm. For adversarial search when playing games, alpha-beta pruning, branch and bound, and minimax were early contributions.",107,"Symbolic artificial intelligence
Search

Search arises in many kinds of problem solving, including planning, constraint satisfaction, and playing games such as checkers, chess, and go. The best known AI-search tree search algorithms are breadth-first search, depth-first search, A*, and Monte Carlo Search. Key search algorithms for Boolean satisfiability are WalkSAT, conflict-driven clause learning, and the DPLL algorithm. For adversarial search when playing games, alpha-beta pruning, branch and bound, and minimax were early contributions.","1. What is the best known AI-search tree search algorithm?
2. What are the best known algorithms for Boolean satisfiability?
3. What are the best known search algorithms for playing games?","1. The best known AI-search tree search algorithm is breadth-first search.
2. The best known algorithms for Boolean satisfiability are WalkSAT, conflict-driven clause learning, and the DPLL algorithm.
3. The best known search algorithms for playing games are alpha-beta pruning, branch and bound, and minimax."
Symbolic artificial intelligence,Knowledge representation and reasoning,Multiple different approaches to represent knowledge and then reason with those representations have been investigated. Below is a quick overview of approaches to knowledge representation and automated reasoning.,36,"Symbolic artificial intelligence
Knowledge representation and reasoning

Multiple different approaches to represent knowledge and then reason with those representations have been investigated. Below is a quick overview of approaches to knowledge representation and automated reasoning.","1. What is the difference between knowledge representation and automated reasoning?
2. What are the different approaches to knowledge representation?
3. What are the different approaches to automated reasoning?","1. Knowledge representation is the process of representing knowledge in a way that a computer can understand. Automated reasoning is the process of using a computer to reason with that knowledge.
2. There are multiple different approaches to knowledge representation, including propositional logic, predicate logic, and frames. There are also multiple different approaches to automated reasoning, including theorem proving, decision procedures, and constraint satisfaction.
3. Each approach to knowledge representation has its own strengths and weaknesses. Propositional logic is good at representing simple facts, while predicate logic can represent more complex relationships between objects. Theorem proving is good at proving theorems, while decision procedures can be used to find a best solution to a problem."
Symbolic artificial intelligence,Knowledge representation,"Semantic networks, conceptual graphs, frames, and logic are all approaches to modeling knowledge such as domain knowledge, problem-solving knowledge, and the semantic meaning of language. Ontologies model key concepts and their relationships in a domain. Example ontologies are YAGO, WordNet, and DOLCE. DOLCE is an example of an upper ontology that can be used for any domain while WordNet is a lexical resource that can also be viewed as an ontology. YAGO incorporates WordNet as part of its ontology, to align facts extracted from Wikipedia with WordNet synsets. The Disease Ontology is an example of a medical ontology currently being used. 
Description logic is a logic for automated classification of ontologies and for detecting inconsistent classification data. OWL is a language used to represent ontologies with description logic. Protégé is a ontology editor that can read in OWL ontologies and then check consistency with deductive classifiers such as such as HermiT.First-order logic is more general than description logic. The automated theorem provers discussed below can prove theorems in first-order logic. Horn clause logic is more restricted than first-order logic and is used in logic programming languages such as Prolog. Extensions to first-order logic include temporal logic, to handle time; epistemic logic, to reason about agent knowledge; modal logic, to handle possibility and necessity; and probabilistic logics to handle logic and probability together.",311,"Symbolic artificial intelligence
Knowledge representation

Semantic networks, conceptual graphs, frames, and logic are all approaches to modeling knowledge such as domain knowledge, problem-solving knowledge, and the semantic meaning of language. Ontologies model key concepts and their relationships in a domain. Example ontologies are YAGO, WordNet, and DOLCE. DOLCE is an example of an upper ontology that can be used for any domain while WordNet is a lexical resource that can also be viewed as an ontology. YAGO incorporates WordNet as part of its ontology, to align facts extracted from Wikipedia with WordNet synsets. The Disease Ontology is an example of a medical ontology currently being used. 
Description logic is a logic for automated classification of ontologies and for detecting inconsistent classification data. OWL is a language used to represent ontologies with description logic. Protégé is a ontology editor that can read in OWL ontologies and then check consistency with deductive classifiers such as such as HermiT.First-order logic is more general than description logic. The automated theorem provers discussed below can prove theorems in first-order logic. Horn clause logic is more restricted than first-order logic and is used in logic programming languages such as Prolog. Extensions to first-order logic include temporal logic, to handle time; epistemic logic, to reason about agent knowledge; modal logic, to handle possibility and necessity; and probabilistic logics to handle logic and probability together.","1. What is an ontology?
2. What is the difference between first-order logic and description logic?
3. What is Horn clause logic?
4. What is the Disease Ontology?
5. What is OWL?
6. What is Protégé?","1. An ontology is a model of key concepts and their relationships in a domain.
2. First-order logic is more general than description logic. Description logic is a logic for automated classification of ontologies and for detecting inconsistent classification data.
3. Horn clause logic is more restricted than first-order logic and is used in logic programming languages such as Prolog.
4. The Disease Ontology is an example of a medical ontology currently being used.
5. OWL is a language used to represent ontologies with description logic.
6. Protégé is a ontology editor that can read in OWL ontologies and then check consistency with deductive classifiers such as such as HermiT."
Symbolic artificial intelligence,Automatic theorem proving,"Examples of automated theorem provers for first-order logic are:

Prover9
ACL2
VampireProver9 can be used in conjunction with the Mace4 model checker. ACL2 is a theorem prover that can handle proofs by induction and is a descendant of the Boyer-Moore Theorem Prover, also known as Nqthm.",82,"Symbolic artificial intelligence
Automatic theorem proving

Examples of automated theorem provers for first-order logic are:

Prover9
ACL2
VampireProver9 can be used in conjunction with the Mace4 model checker. ACL2 is a theorem prover that can handle proofs by induction and is a descendant of the Boyer-Moore Theorem Prover, also known as Nqthm.","1. What is the purpose of a theorem prover? 
2. What are some examples of theorem provers? 
3. How do theorem provers work?","1. The purpose of a theorem prover is to help prove mathematical theorems and other logical statements. 
2. Some examples of theorem provers are Prover9, ACL2, and VampireProver9. 
3. Theorem provers work by using a set of rules and algorithms to help prove a statement."
Symbolic artificial intelligence,Reasoning in knowledge-based systems,"Knowledge-based systems have an explicit knowledge base, typically of rules, to enhance reusability across domains by separating procedural code and domain knowledge. A separate inference engine processes rules and adds, deletes, or modifies a knowledge store.
Forward chaining inference engines are the most common, and are seen in CLIPS and OPS5. Backward chaining occurs in Prolog, where a more limited logical representation is used, Horn Clauses. Pattern-matching, specifically unification, is used in Prolog.
A more flexible kind of problem-solving occurs when reasoning about what to do next occurs, rather than simply choosing one of the available actions. This kind of meta-level reasoning is used in Soar and in the BB1 blackboard architecture.
Cognitive architectures such as ACT-R may have additional capabilities, such as the ability to compile frequently used knowledge into higher-level chunks.",193,"Symbolic artificial intelligence
Reasoning in knowledge-based systems

Knowledge-based systems have an explicit knowledge base, typically of rules, to enhance reusability across domains by separating procedural code and domain knowledge. A separate inference engine processes rules and adds, deletes, or modifies a knowledge store.
Forward chaining inference engines are the most common, and are seen in CLIPS and OPS5. Backward chaining occurs in Prolog, where a more limited logical representation is used, Horn Clauses. Pattern-matching, specifically unification, is used in Prolog.
A more flexible kind of problem-solving occurs when reasoning about what to do next occurs, rather than simply choosing one of the available actions. This kind of meta-level reasoning is used in Soar and in the BB1 blackboard architecture.
Cognitive architectures such as ACT-R may have additional capabilities, such as the ability to compile frequently used knowledge into higher-level chunks.","1. What is a knowledge-based system?
2. What is a forward chaining inference engine?
3. What is a backward chaining inference engine?
4. What is a cognitive architecture?
5. What is ACT-R?","1. A knowledge-based system is a system that uses knowledge to perform tasks.
2. A forward chaining inference engine is a type of inference engine that uses rules to determine the next action to take.
3. A backward chaining inference engine is a type of inference engine that uses rules to determine the previous action to take.
4. A cognitive architecture is a type of architecture that describes the structure of the human mind.
5. ACT-R is a cognitive architecture that describes the structure of the human mind."
Symbolic artificial intelligence,Commonsense reasoning,"Marvin Minsky first proposed frames as a way of interpreting common visual situations, such as an office, and Roger Schank extended this idea to scripts for common routines, such as dining out. Cyc has attempted to capture useful common-sense knowledge and has ""micro-theories"" to handle particular kinds of domain-specific reasoning.
Qualitative simulation, such as Benjamin Kuipers's QSIM, approximates human reasoning about naive physics, such as what happens when we heat a liquid in a pot on the stove. We expect it to heat and possibly boil over, even though we may not know its temperature, its boiling point, or other details, such as atmospheric pressure.
Similarly, Allen's temporal interval algebra is a simplification of reasoning about time and Region Connection Calculus is a simplification of reasoning about spatial relationships. Both can be solved with constraint solvers.",181,"Symbolic artificial intelligence
Commonsense reasoning

Marvin Minsky first proposed frames as a way of interpreting common visual situations, such as an office, and Roger Schank extended this idea to scripts for common routines, such as dining out. Cyc has attempted to capture useful common-sense knowledge and has ""micro-theories"" to handle particular kinds of domain-specific reasoning.
Qualitative simulation, such as Benjamin Kuipers's QSIM, approximates human reasoning about naive physics, such as what happens when we heat a liquid in a pot on the stove. We expect it to heat and possibly boil over, even though we may not know its temperature, its boiling point, or other details, such as atmospheric pressure.
Similarly, Allen's temporal interval algebra is a simplification of reasoning about time and Region Connection Calculus is a simplification of reasoning about spatial relationships. Both can be solved with constraint solvers.","1. What is symbolic artificial intelligence?
2. What is commonsense reasoning?
3. What is a frame?
4. What is a script?
5. What is Cyc?
6. What is qualitative simulation?
7. What is Benjamin Kuipers's QSIM?
8. What is Allen's temporal interval algebra?
9. What is Region Connection Calculus?
10. What is a constraint solver?","1. Symbolic artificial intelligence is a subfield of artificial intelligence that deals with the manipulation of symbols.
2. Commonsense reasoning is the ability to reason about everyday situations using common sense knowledge.
3. A frame is a data structure that represents knowledge about a particular domain.
4. A script is a sequence of frames that represent a particular routine.
5. Cyc is a large database of commonsense knowledge that is used for qualitative simulation.
6. Qualitative simulation is the ability to simulate everyday situations using commonsense knowledge.
7. Benjamin Kuipers's QSIM is a program that simulates everyday situations using commonsense knowledge.
8. Allen's temporal interval algebra is a program that simplifies reasoning about time.
9. Region Connection Calculus is a program that simplifies reasoning about spatial relationships.
10. A constraint solver is a program that solves problems using constraints."
Symbolic artificial intelligence,Constraints and constraint-based reasoning,"Constraint solvers perform a more limited kind of inference than first-order logic. They can simplify sets of spatiotemporal constraints, such as those for RCC or Temporal Algebra, along with solving other kinds of puzzle problems, such as Wordle, Sudoku, cryptarithmetic problems, and so on. Constraint logic programming can be used to solve scheduling problems, for example with constraint handling rules (CHR).",96,"Symbolic artificial intelligence
Constraints and constraint-based reasoning

Constraint solvers perform a more limited kind of inference than first-order logic. They can simplify sets of spatiotemporal constraints, such as those for RCC or Temporal Algebra, along with solving other kinds of puzzle problems, such as Wordle, Sudoku, cryptarithmetic problems, and so on. Constraint logic programming can be used to solve scheduling problems, for example with constraint handling rules (CHR).","1. What is symbolic artificial intelligence?
2. What is a constraint solver?
3. What are some applications of constraint solvers?","1. Symbolic artificial intelligence is a subfield of artificial intelligence that deals with the manipulation of symbols.
2. A constraint solver is a program that can simplify sets of spatiotemporal constraints.
3. Some applications of constraint solvers include solving scheduling problems and cryptarithmetic problems."
Symbolic artificial intelligence,Automated planning,"The General Problem Solver (GPS) cast planning as problem-solving used means-ends analysis to create plans. STRIPS took a different approach, viewing planning as theorem proving. Graphplan takes a least-commitment approach to planning, rather than sequentially choosing actions from an initial state, working forwards, or a goal state if working backwards.  Satplan is an approach to planning where a planning problem is reduced to a Boolean satisfiability problem.",97,"Symbolic artificial intelligence
Automated planning

The General Problem Solver (GPS) cast planning as problem-solving used means-ends analysis to create plans. STRIPS took a different approach, viewing planning as theorem proving. Graphplan takes a least-commitment approach to planning, rather than sequentially choosing actions from an initial state, working forwards, or a goal state if working backwards.  Satplan is an approach to planning where a planning problem is reduced to a Boolean satisfiability problem.","1. What is the General Problem Solver (GPS)? 
2. What is the difference between the GPS and STRIPS? 
3. What is the least-commitment approach? 
4. What is Satplan?","1. The General Problem Solver (GPS) is a symbolic artificial intelligence approach to planning that uses means-ends analysis to create plans. 
2. The GPS is different from STRIPS in that it views planning as theorem proving, while STRIPS takes a different approach and views planning as problem-solving. 
3. The least-commitment approach is an approach to planning where a planning problem is reduced to a Boolean satisfiability problem. 
4. Satplan is an approach to planning where a planning problem is reduced to a Boolean satisfiability problem."
Symbolic artificial intelligence,Natural language processing,"Natural language processing focuses on treating language as data to perform tasks such as identifying topics without necessarily understanding the intended meaning. Natural language understanding, in contrast, constructs a meaning representation and uses that for further processing, such as answering questions.
Parsing, tokenizing, spelling correction, part-of-speech tagging, noun and verb phrase chunking are all aspects of natural language processing long handled by symbolic AI, but since improved by deep learning approaches. In symbolic AI, discourse representation theory and first-order logic have been used to represent sentence meanings. Latent semantic analysis (LSA) and explicit semantic analysis also provided vector representations of documents. In the latter case, vector components are interpretable as concepts named by Wikipedia articles. 
New deep learning approaches based on Transformer models have now eclipsed these earlier symbolic AI approaches and attained state-of-the-art performance in natural language processing. However, Transformer models are opaque and do not yet produce human-interpretable semantic representations for sentences and documents. Instead, they produce task-specific vectors where the meaning of the vector components is opaque.",227,"Symbolic artificial intelligence
Natural language processing

Natural language processing focuses on treating language as data to perform tasks such as identifying topics without necessarily understanding the intended meaning. Natural language understanding, in contrast, constructs a meaning representation and uses that for further processing, such as answering questions.
Parsing, tokenizing, spelling correction, part-of-speech tagging, noun and verb phrase chunking are all aspects of natural language processing long handled by symbolic AI, but since improved by deep learning approaches. In symbolic AI, discourse representation theory and first-order logic have been used to represent sentence meanings. Latent semantic analysis (LSA) and explicit semantic analysis also provided vector representations of documents. In the latter case, vector components are interpretable as concepts named by Wikipedia articles. 
New deep learning approaches based on Transformer models have now eclipsed these earlier symbolic AI approaches and attained state-of-the-art performance in natural language processing. However, Transformer models are opaque and do not yet produce human-interpretable semantic representations for sentences and documents. Instead, they produce task-specific vectors where the meaning of the vector components is opaque.","1. What is the difference between natural language processing and natural language understanding?
2. What is the difference between symbolic AI and deep learning approaches in natural language processing?
3. What is the advantage of deep learning approaches over symbolic AI approaches in natural language processing?","1. The main difference between natural language processing and natural language understanding is that natural language processing focuses on treating language as data to perform tasks such as identifying topics, while natural language understanding constructs a meaning representation and uses that for further processing, such as answering questions.
2. The main difference between symbolic AI and deep learning approaches in natural language processing is that symbolic AI relies on discourse representation theory and first-order logic to represent sentence meanings, while deep learning approaches use Transformer models, which are opaque and do not yet produce human-interpretable semantic representations for sentences and documents.
3. The advantage of deep learning approaches over symbolic AI approaches in natural language processing is that deep learning approaches have achieved state-of-the-art performance in natural language processing."
Symbolic artificial intelligence,Agents and multi-agent systems,"Agents are autonomous systems embedded in an environment they perceive and act upon in some sense. Russell and Norvig's standard textbook on artificial intelligence is organized to reflect agent architectures of increasing sophistication. The sophistication of agents varies from simple reactive agents, to those with a model of the world and automated planning capabilities, possibly a BDI agent, i.e., one with beliefs, desires, and intentions – or alternatively a reinforcement learning model learned over time to choose actions – up to a combination of alternative architectures, such as a neuro-symbolic architecture that includes deep learning for perception. 
In contrast, a multi-agent system consists of multiple agents that communicate amongst themselves with some inter-agent communication language such as Knowledge Query and Manipulation Language (KQML). The agents need not all have the same internal architecture. Advantages of multi-agent systems include the ability to divide work among the agents and to increase fault tolerance when agents are lost. Research problems include how agents reach consensus, distributed problem solving, multi-agent learning, multi-agent planning, and distributed constraint optimization.",228,"Symbolic artificial intelligence
Agents and multi-agent systems

Agents are autonomous systems embedded in an environment they perceive and act upon in some sense. Russell and Norvig's standard textbook on artificial intelligence is organized to reflect agent architectures of increasing sophistication. The sophistication of agents varies from simple reactive agents, to those with a model of the world and automated planning capabilities, possibly a BDI agent, i.e., one with beliefs, desires, and intentions – or alternatively a reinforcement learning model learned over time to choose actions – up to a combination of alternative architectures, such as a neuro-symbolic architecture that includes deep learning for perception. 
In contrast, a multi-agent system consists of multiple agents that communicate amongst themselves with some inter-agent communication language such as Knowledge Query and Manipulation Language (KQML). The agents need not all have the same internal architecture. Advantages of multi-agent systems include the ability to divide work among the agents and to increase fault tolerance when agents are lost. Research problems include how agents reach consensus, distributed problem solving, multi-agent learning, multi-agent planning, and distributed constraint optimization.","1. What is the difference between an agent and a multi-agent system?
2. What are some advantages of multi-agent systems?
3. What are some research problems in multi-agent systems?","1. An agent is a system that is autonomous and embedded in an environment. A multi-agent system is a system composed of multiple agents that communicate with each other.
2. Multi-agent systems have several advantages over single-agent systems, including the ability to divide work among agents and to increase fault tolerance.
3. Research problems in multi-agent systems include how agents reach consensus, distributed problem solving, multi-agent learning, and multi-agent planning."
Symbolic artificial intelligence,Controversies,"Controversies arose from early on in symbolic AI, both within the field—e.g., between logicists (the pro-logic ""neats"") and non-logicists (the anti-logic ""scruffies"")—and between those who embraced AI but rejected symbolic approaches—primarily connectionists—and those outside the field. Critiques from outside of the field were primarily from philosophers, on intellectual grounds, but also from funding agencies, especially during the two AI winters.",105,"Symbolic artificial intelligence
Controversies

Controversies arose from early on in symbolic AI, both within the field—e.g., between logicists (the pro-logic ""neats"") and non-logicists (the anti-logic ""scruffies"")—and between those who embraced AI but rejected symbolic approaches—primarily connectionists—and those outside the field. Critiques from outside of the field were primarily from philosophers, on intellectual grounds, but also from funding agencies, especially during the two AI winters.","1. What are the different types of AI? 
2. What are the different types of controversies within AI? 
3. What are the different types of critiques from outside of AI?","1. The different types of AI are symbolic AI and connectionist AI. 
2. The different types of controversies within AI are between logicists and non-logicists, and between those who embrace AI but reject symbolic approaches and those outside of the field. 
3. The different types of critiques from outside of AI are from philosophers, on intellectual grounds, and from funding agencies, especially during the two AI winters."
Symbolic artificial intelligence,The Qualification Problem,"Now we turn to attacks from outside the field specifically by philosophers. One argument frequently cited by philosophers was made earlier by the computer scientist Alan Turing, in his 1950 paper Computing Machinery and Intelligence, when he said that ""human behavior is far too complex to be captured by any formal set of rules—humans must be using some informal guidelines that … could never be captured in a formal set of rules and thus could never be codified in a computer program."" Turing called this ""The Argument from Informality of Behaviour."" Similar critiques were provided by philosophers Hubert Dreyfus and John Haugeland.

Russell and Norvig explain that these arguments were targeted to the symbolic AI of the 1980s:The technology they criticized came to be called Good Old-Fashioned AI (GOFAI). GOFAI corresponds to the simplest logical agent design described ... and we saw ... that it is indeed difficult to capture every contingency of appropriate behavior in a set of necessary and sufficient logical rules; we called that the qualification problem.
Since then, probabilistic reasoning systems have extended the capability of symbolic AI so they can be much ""more appropriate for open-ended domains."" Other approaches investigated included machine learning (both symbolic and neural) and non-monotonic logics.",265,"Symbolic artificial intelligence
The Qualification Problem

Now we turn to attacks from outside the field specifically by philosophers. One argument frequently cited by philosophers was made earlier by the computer scientist Alan Turing, in his 1950 paper Computing Machinery and Intelligence, when he said that ""human behavior is far too complex to be captured by any formal set of rules—humans must be using some informal guidelines that … could never be captured in a formal set of rules and thus could never be codified in a computer program."" Turing called this ""The Argument from Informality of Behaviour."" Similar critiques were provided by philosophers Hubert Dreyfus and John Haugeland.

Russell and Norvig explain that these arguments were targeted to the symbolic AI of the 1980s:The technology they criticized came to be called Good Old-Fashioned AI (GOFAI). GOFAI corresponds to the simplest logical agent design described ... and we saw ... that it is indeed difficult to capture every contingency of appropriate behavior in a set of necessary and sufficient logical rules; we called that the qualification problem.
Since then, probabilistic reasoning systems have extended the capability of symbolic AI so they can be much ""more appropriate for open-ended domains."" Other approaches investigated included machine learning (both symbolic and neural) and non-monotonic logics.","1. What is the Qualification Problem?
2. What was the technology that the philosophers were criticizing?
3. How has AI technology progressed since then?","1. The Qualification Problem is the difficulty of capturing every contingency of appropriate behavior in a set of necessary and sufficient logical rules.
2. The technology that the philosophers were criticizing was called Good Old-Fashioned AI (GOFAI).
3. AI technology has progressed since then, with probabilistic reasoning systems extending the capability of symbolic AI, and other approaches such as machine learning and non-monotonic logics being investigated."
Symbolic artificial intelligence,Connectionist AI: philosophical challenges and sociological conflicts,"Connectionist approaches include earlier work on neural networks, such as perceptrons; work in the mid to late 80s, such as Danny Hillis's Connection Machine and Yann LeCun's advances in convolutional neural networks; to today's more advanced approaches, such as Transformers, GANs, and other work in deep learning.
Three philosophical positions have been outlined among connectionists:

Implementationism—where connectionist architectures implement the capabilities for symbolic processing,
Radical connectionism—where symbolic processing is rejected totally, and connectionist architectures underlie intelligence and are fully sufficient to explain it,
Moderate connectionism—where symbolic processing and connectionist architectures are viewed as complementary and both are required for intelligence.
Olazaran, in his sociological history of the controversies within the neural network community, described the moderate connectionism view as essentially compatible with current research in neuro-symbolic hybrids:
The third and last position I would like to examine here is what I call the moderate connectionist view, a more eclectic view of the current debate between connectionism and symbolic AI. One of the researchers who has elaborated this position most explicitly is Andy Clark, a philosopher from the School of Cognitive and Computing Sciences of the University of Sussex (Brighton, England). Clark defended hybrid (partly symbolic, partly connectionist) systems. He claimed that (at least) two kinds of theories are needed in order to study and model cognition. On the one hand, for some information-processing tasks (such as pattern recognition) connectionism has advantages over symbolic models. But on the other hand, for other cognitive processes (such as serial, deductive reasoning, and generative symbol manipulation processes) the symbolic paradigm offers adequate models, and not only ""approximations"" (contrary to what radical connectionists would claim). 
Gary Marcus has claimed that the animus in the deep learning community against symbolic approaches now may be more sociological than philosophical:To think that we can simply abandon symbol-manipulation is to suspend disbelief.

And yet, for the most part, that's how most current AI proceeds. Hinton and many others have tried hard to banish symbols altogether. The deep learning hope—seemingly grounded not so much in science, but in a sort of historical grudge—is that intelligent behavior will emerge purely from the confluence of massive data and deep learning. Where classical computers and software solve tasks by defining sets of symbol-manipulating rules dedicated to particular jobs, such as editing a line in a word processor or performing a calculation in a spreadsheet, neural networks typically try to solve tasks by statistical approximation and learning from examples.According to Marcus, Geoffrey Hinton and his colleagues have been vehemently ""anti-symbolic"":When deep learning reemerged in 2012, it was with a kind of take-no-prisoners attitude that has characterized most of the last decade. By 2015, his hostility toward all things symbols had fully crystallized. He gave a talk at an AI workshop at Stanford comparing symbols to aether, one of science's greatest mistakes.
...

Since then, his anti-symbolic campaign has only increased in intensity. In 2016, Yann LeCun, Bengio, and Hinton wrote a manifesto for deep learning in one of science's most important journals, Nature. It closed with a direct attack on symbol manipulation, calling not for reconciliation but for outright replacement. Later, Hinton told a gathering of European Union leaders that investing any further money in symbol-manipulating approaches was ""a huge mistake,"" likening it to investing in internal combustion engines in the era of electric cars.
Part of these disputes may be due to unclear terminology: 

Turing award winner Judea Pearl offers a critique of machine learning which, unfortunately, conflates the terms machine learning and deep learning. Similarly, when Geoffrey Hinton refers to symbolic AI, the connotation of the term tends to be that of expert systems dispossessed of any ability to learn. The use of the terminology is in need of clarification. Machine learning is not confined to association rule mining, c.f. the body of work on symbolic ML and relational learning (the differences to deep learning being the choice of representation, localist logical rather than distributed, and the non-use of gradient-based learning algorithms). Equally, symbolic AI is not just about production rules written by hand. A proper definition of AI concerns knowledge representation and reasoning, autonomous multi-agent systems, planning and argumentation, as well as learning.",944,"Symbolic artificial intelligence
Connectionist AI: philosophical challenges and sociological conflicts

Connectionist approaches include earlier work on neural networks, such as perceptrons; work in the mid to late 80s, such as Danny Hillis's Connection Machine and Yann LeCun's advances in convolutional neural networks; to today's more advanced approaches, such as Transformers, GANs, and other work in deep learning.
Three philosophical positions have been outlined among connectionists:

Implementationism—where connectionist architectures implement the capabilities for symbolic processing,
Radical connectionism—where symbolic processing is rejected totally, and connectionist architectures underlie intelligence and are fully sufficient to explain it,
Moderate connectionism—where symbolic processing and connectionist architectures are viewed as complementary and both are required for intelligence.
Olazaran, in his sociological history of the controversies within the neural network community, described the moderate connectionism view as essentially compatible with current research in neuro-symbolic hybrids:
The third and last position I would like to examine here is what I call the moderate connectionist view, a more eclectic view of the current debate between connectionism and symbolic AI. One of the researchers who has elaborated this position most explicitly is Andy Clark, a philosopher from the School of Cognitive and Computing Sciences of the University of Sussex (Brighton, England). Clark defended hybrid (partly symbolic, partly connectionist) systems. He claimed that (at least) two kinds of theories are needed in order to study and model cognition. On the one hand, for some information-processing tasks (such as pattern recognition) connectionism has advantages over symbolic models. But on the other hand, for other cognitive processes (such as serial, deductive reasoning, and generative symbol manipulation processes) the symbolic paradigm offers adequate models, and not only ""approximations"" (contrary to what radical connectionists would claim). 
Gary Marcus has claimed that the animus in the deep learning community against symbolic approaches now may be more sociological than philosophical:To think that we can simply abandon symbol-manipulation is to suspend disbelief.

And yet, for the most part, that's how most current AI proceeds. Hinton and many others have tried hard to banish symbols altogether. The deep learning hope—seemingly grounded not so much in science, but in a sort of historical grudge—is that intelligent behavior will emerge purely from the confluence of massive data and deep learning. Where classical computers and software solve tasks by defining sets of symbol-manipulating rules dedicated to particular jobs, such as editing a line in a word processor or performing a calculation in a spreadsheet, neural networks typically try to solve tasks by statistical approximation and learning from examples.According to Marcus, Geoffrey Hinton and his colleagues have been vehemently ""anti-symbolic"":When deep learning reemerged in 2012, it was with a kind of take-no-prisoners attitude that has characterized most of the last decade. By 2015, his hostility toward all things symbols had fully crystallized. He gave a talk at an AI workshop at Stanford comparing symbols to aether, one of science's greatest mistakes.
...

Since then, his anti-symbolic campaign has only increased in intensity. In 2016, Yann LeCun, Bengio, and Hinton wrote a manifesto for deep learning in one of science's most important journals, Nature. It closed with a direct attack on symbol manipulation, calling not for reconciliation but for outright replacement. Later, Hinton told a gathering of European Union leaders that investing any further money in symbol-manipulating approaches was ""a huge mistake,"" likening it to investing in internal combustion engines in the era of electric cars.
Part of these disputes may be due to unclear terminology: 

Turing award winner Judea Pearl offers a critique of machine learning which, unfortunately, conflates the terms machine learning and deep learning. Similarly, when Geoffrey Hinton refers to symbolic AI, the connotation of the term tends to be that of expert systems dispossessed of any ability to learn. The use of the terminology is in need of clarification. Machine learning is not confined to association rule mining, c.f. the body of work on symbolic ML and relational learning (the differences to deep learning being the choice of representation, localist logical rather than distributed, and the non-use of gradient-based learning algorithms). Equally, symbolic AI is not just about production rules written by hand. A proper definition of AI concerns knowledge representation and reasoning, autonomous multi-agent systems, planning and argumentation, as well as learning.","1. What is the main difference between connectionist and symbolic AI?
2. What is the animus in the deep learning community against symbolic approaches?
3. What is the moderate connectionist view?","1. Connectionist AI is based on the idea that intelligence can be explained in terms of the connections between neurons, while symbolic AI is based on the idea that intelligence can be explained in terms of the symbols that are manipulated.
2. The animus in the deep learning community against symbolic approaches is due to the belief that symbolic processing is unnecessary and can be replaced by deep learning.
3. The moderate connectionist view is the view that both connectionist and symbolic approaches are necessary for intelligence."
Symbolic artificial intelligence,Situated robotics: the world as a model,"Another critique of symbolic AI is the embodied cognition approach:

The embodied cognition approach claims that it makes no sense to consider the brain separately: cognition takes place within a body, which is embedded in an environment. We need to study the system as a whole; the brain's functioning exploits regularities in its environment, including the rest of its body. Under the embodied cognition approach, robotics, vision, and other sensors become central, not peripheral.
Rodney Brooks invented behavior-based robotics, one approach to embodied cognition. Nouvelle AI, another name for this approach, is viewed as an alternative to both symbolic AI and connectionist AI. His approach rejected representations, either symbolic or distributed, as not only unnecessary, but as detrimental. Instead, he created the subsumption architecture, a layered architecture for embodied agents. Each layer achieves a different purpose and must function in the real world. For example, the first robot he describes in Intelligence Without Representation, has three layers. The bottom layer interprets sonar sensors to avoid objects. The middle layer causes the robot to wander around when there are no obstacles. The top layer causes the robot to go to more distant places for further exploration. Each layer can temporarily inhibit or suppress a lower-level layer. He criticized AI researchers for defining AI problems for their systems, when: ""There is no clean division between perception (abstraction) and reasoning in the real world."" He called his robots ""Creatures"" and each layer was ""composed of a fixed-topology network of simple finite state machines.""  In the Nouvelle AI approach, ""First, it is vitally important to test the Creatures we build in the real world; i.e., in the same world that we humans inhabit. It is disastrous to fall into the temptation of testing them in a simplified world first, even with the best intentions of later transferring activity to an unsimplified world.""  His emphasis on real-world testing was in contrast to ""Early work in AI concentrated on games, geometrical problems, symbolic algebra, theorem proving, and other formal systems"" and the use of the blocks world in symbolic AI systems such as SHRDLU.",456,"Symbolic artificial intelligence
Situated robotics: the world as a model

Another critique of symbolic AI is the embodied cognition approach:

The embodied cognition approach claims that it makes no sense to consider the brain separately: cognition takes place within a body, which is embedded in an environment. We need to study the system as a whole; the brain's functioning exploits regularities in its environment, including the rest of its body. Under the embodied cognition approach, robotics, vision, and other sensors become central, not peripheral.
Rodney Brooks invented behavior-based robotics, one approach to embodied cognition. Nouvelle AI, another name for this approach, is viewed as an alternative to both symbolic AI and connectionist AI. His approach rejected representations, either symbolic or distributed, as not only unnecessary, but as detrimental. Instead, he created the subsumption architecture, a layered architecture for embodied agents. Each layer achieves a different purpose and must function in the real world. For example, the first robot he describes in Intelligence Without Representation, has three layers. The bottom layer interprets sonar sensors to avoid objects. The middle layer causes the robot to wander around when there are no obstacles. The top layer causes the robot to go to more distant places for further exploration. Each layer can temporarily inhibit or suppress a lower-level layer. He criticized AI researchers for defining AI problems for their systems, when: ""There is no clean division between perception (abstraction) and reasoning in the real world."" He called his robots ""Creatures"" and each layer was ""composed of a fixed-topology network of simple finite state machines.""  In the Nouvelle AI approach, ""First, it is vitally important to test the Creatures we build in the real world; i.e., in the same world that we humans inhabit. It is disastrous to fall into the temptation of testing them in a simplified world first, even with the best intentions of later transferring activity to an unsimplified world.""  His emphasis on real-world testing was in contrast to ""Early work in AI concentrated on games, geometrical problems, symbolic algebra, theorem proving, and other formal systems"" and the use of the blocks world in symbolic AI systems such as SHRDLU.","1. What is the embodied cognition approach?
2. What is the subsumption architecture?
3. What is the difference between Nouvelle AI and symbolic AI?
4. Why is real-world testing important in Nouvelle AI?","1. The embodied cognition approach is the idea that cognition cannot be studied separately from the body and the environment.
2. The subsumption architecture is a layered architecture for embodied agents in which each layer achieves a different purpose.
3. Nouvelle AI is an alternative to both symbolic AI and connectionist AI, and it rejects representations as not only unnecessary, but as detrimental.
4. Real-world testing is important in Nouvelle AI because it allows agents to exploit regularities in their environment."
Symbolic artificial intelligence,Current views,"Each approach—symbolic, connectionist, and behavior-based—has advantages, but has been criticized by the other approaches. Symbolic AI has been criticized as disembodied, liable to the qualification problem, and poor in handling the perceptual problems where deep learning excels. In turn, connectionist AI has been criticized as poorly suited for deliberative step-by-step problem solving, incorporating knowledge, and handling planning. Finally, Nouvelle AI excels in reactive and real-world robotics domains but has been criticized for difficulties in incorporating learning and knowledge.

Hybrid AIs incorporating one or more of these approaches are currently viewed as the path forward. Russell and Norvig conclude that:Overall, Dreyfus saw areas where AI did not have complete answers and said that Al is therefore impossible; we now see many of these same areas undergoing continued research and development leading to increased capability, not impossibility.",190,"Symbolic artificial intelligence
Current views

Each approach—symbolic, connectionist, and behavior-based—has advantages, but has been criticized by the other approaches. Symbolic AI has been criticized as disembodied, liable to the qualification problem, and poor in handling the perceptual problems where deep learning excels. In turn, connectionist AI has been criticized as poorly suited for deliberative step-by-step problem solving, incorporating knowledge, and handling planning. Finally, Nouvelle AI excels in reactive and real-world robotics domains but has been criticized for difficulties in incorporating learning and knowledge.

Hybrid AIs incorporating one or more of these approaches are currently viewed as the path forward. Russell and Norvig conclude that:Overall, Dreyfus saw areas where AI did not have complete answers and said that Al is therefore impossible; we now see many of these same areas undergoing continued research and development leading to increased capability, not impossibility.","1. What are the three main approaches to AI?
2. What are the criticisms of each approach?
3. What is the path forward for AI?","1. The three main approaches to AI are symbolic AI, connectionist AI, and behavior-based AI.
2. Each approach has its own advantages and disadvantages, which have been criticized by the other approaches.
3. The path forward for AI is to incorporate one or more of these approaches into a hybrid AI."
Artificial intelligence art,Summary,"Artificial intelligence art is any artwork, particularly images and musical compositions, created through the use of artificial intelligence (AI) programs, such as text-to-image models and musical generators. It is sometimes confused with digital art. While both AI art and digital art involve the use of technology, AI art is characterized by its use of generative algorithms and deep learning techniques that can autonomously produce art without direct input from human artists.",91,"Artificial intelligence art
Summary

Artificial intelligence art is any artwork, particularly images and musical compositions, created through the use of artificial intelligence (AI) programs, such as text-to-image models and musical generators. It is sometimes confused with digital art. While both AI art and digital art involve the use of technology, AI art is characterized by its use of generative algorithms and deep learning techniques that can autonomously produce art without direct input from human artists.","1. What is AI art?
2. What is the difference between AI art and digital art?
3. How is AI art created?","1. AI art is artwork created through the use of artificial intelligence programs, such as text-to-image models and musical generators.
2. AI art is characterized by its use of generative algorithms and deep learning techniques that can autonomously produce art without direct input from human artists.
3. AI art is created by training artificial intelligence algorithms on a large dataset of images or musical compositions. The algorithms learn to create art autonomously by analyzing and reproducing the features of the originals."
Artificial intelligence art,Early history,"Artificial intelligence was founded as an academic discipline in 1956, and in the decades since has experienced several waves of optimism. Since its founding, researchers in the field of artificial intelligence have raised philosophical arguments about the nature of the human mind and the ethical consequences of creating artificial beings endowed with human-like intelligence; these issues have previously been explored by myth, fiction and philosophy since antiquity.",79,"Artificial intelligence art
Early history

Artificial intelligence was founded as an academic discipline in 1956, and in the decades since has experienced several waves of optimism. Since its founding, researchers in the field of artificial intelligence have raised philosophical arguments about the nature of the human mind and the ethical consequences of creating artificial beings endowed with human-like intelligence; these issues have previously been explored by myth, fiction and philosophy since antiquity.","1. What are the ethical consequences of creating artificial beings endowed with human-like intelligence?
2. What are the philosophical arguments about the nature of the human mind?
3. What are the different waves of optimism in the history of artificial intelligence?","1. The ethical consequences of creating artificial beings endowed with human-like intelligence are still being explored. Some of the questions that have been raised include what happens if these beings become smarter than humans, and what if they start to make decisions that we don't agree with?
2. Philosophical arguments about the nature of the human mind have been around since antiquity. Some of the questions that have been explored include what makes us human, and what is the difference between human intelligence and artificial intelligence?
3. There have been several waves of optimism in the history of artificial intelligence. The first wave happened in the 1950s and 1960s, the second wave happened in the 1980s, and the third wave is happening right now."
Artificial intelligence art,Imagery,"Many mechanisms for creating AI art have been developed, including procedural ""rule-based"" generation of images using mathematical patterns, algorithms which simulate brush strokes and other painted effects, and deep learning algorithms, such as generative adversarial networks (GANs) and transformers.
One of the first significant AI art systems is AARON, developed by Harold Cohen beginning in the late 1960s at the University of California at San Diego. AARON is the most notable example of AI art in the era of GOFAI programming because of its use of a symbolic rule-based approach to generate technical images. Cohen developed AARON with the goal of being able to code the act of drawing. In its primitive form, AARON created simple black and white drawings. Cohen would later finish the drawings by painting them. Throughout the years, he also began to develop a way for AARON to also paint. Cohen designed AARON to paint using special brushes and dyes that were chosen by the program itself without mediation from Cohen.

Generative adversarial networks (GANs) were designed in 2014. This system uses a ""generator"" to create new images and a ""discriminator"" to decide which created images are considered successful. DeepDream, released by Google in 2015, uses a convolutional neural network to find and enhance patterns in images via algorithmic pareidolia, thus creating deliberately over-processed images. After DeepDream's release, several companies released apps that transform photos into art-like images with the style of well-known sets of paintings.
The website Artbreeder, launched in 2018, uses the models StyleGAN and BigGAN to allow users to generate and modify images such as faces, landscapes, and paintings.Several programs use text-to-image models to generate a variety of images based on various text prompts. They include VQGAN+CLIP which was released in 2021, OpenAI's DALL-E which released a series of images in January 2021,  Google Brain's Imagen and Parti which was announced in May 2022, Microsoft's NUWA-Infinity, and Stable Diffusion which was released in August 2022.  Stability.ai has a Stable Diffusion web interface called DreamStudio. Stable Diffusion is source-available software, enabling further development such as plugins for Krita, Photoshop, Blender, and GIMP, as well as the Automatic1111 web-based open source user interface.There are many other AI art generation programs including simple consumer-facing mobile apps and Jupyter notebooks that require powerful GPUs to run effectively.",540,"Artificial intelligence art
Imagery

Many mechanisms for creating AI art have been developed, including procedural ""rule-based"" generation of images using mathematical patterns, algorithms which simulate brush strokes and other painted effects, and deep learning algorithms, such as generative adversarial networks (GANs) and transformers.
One of the first significant AI art systems is AARON, developed by Harold Cohen beginning in the late 1960s at the University of California at San Diego. AARON is the most notable example of AI art in the era of GOFAI programming because of its use of a symbolic rule-based approach to generate technical images. Cohen developed AARON with the goal of being able to code the act of drawing. In its primitive form, AARON created simple black and white drawings. Cohen would later finish the drawings by painting them. Throughout the years, he also began to develop a way for AARON to also paint. Cohen designed AARON to paint using special brushes and dyes that were chosen by the program itself without mediation from Cohen.

Generative adversarial networks (GANs) were designed in 2014. This system uses a ""generator"" to create new images and a ""discriminator"" to decide which created images are considered successful. DeepDream, released by Google in 2015, uses a convolutional neural network to find and enhance patterns in images via algorithmic pareidolia, thus creating deliberately over-processed images. After DeepDream's release, several companies released apps that transform photos into art-like images with the style of well-known sets of paintings.
The website Artbreeder, launched in 2018, uses the models StyleGAN and BigGAN to allow users to generate and modify images such as faces, landscapes, and paintings.Several programs use text-to-image models to generate a variety of images based on various text prompts. They include VQGAN+CLIP which was released in 2021, OpenAI's DALL-E which released a series of images in January 2021,  Google Brain's Imagen and Parti which was announced in May 2022, Microsoft's NUWA-Infinity, and Stable Diffusion which was released in August 2022.  Stability.ai has a Stable Diffusion web interface called DreamStudio. Stable Diffusion is source-available software, enabling further development such as plugins for Krita, Photoshop, Blender, and GIMP, as well as the Automatic1111 web-based open source user interface.There are many other AI art generation programs including simple consumer-facing mobile apps and Jupyter notebooks that require powerful GPUs to run effectively.","1. What is the goal of AARON?
2. What is the difference between AARON's primitive form and its later form?
3. What is the significance of Google's release of DeepDream?
4. What is the significance of Artbreeder's launch in 2018?
5. What are some other AI art generation programs?","1. The goal of AARON is to be able to code the act of drawing.
2. AARON's primitive form created simple black and white drawings, while its later form also allowed it to paint.
3. The significance of Google's release of DeepDream was that it allowed for the creation of deliberately over-processed images.
4. The significance of Artbreeder's launch in 2018 was that it allowed users to generate and modify images such as faces, landscapes, and paintings.
5. Some other AI art generation programs include simple consumer-facing mobile apps and Jupyter notebooks that require powerful GPUs to run effectively."
Artificial intelligence art,Impact and applications,"The exhibition ""Thinking Machines: Art and Design in the Computer Age, 1959–1989"" at MoMA provided an overview of AI applications for art, architecture, and design. Exhibitions showcasing the usage of AI to produce art include the 2016 Google-sponsored benefit and auction at the Gray Area Foundation in San Francisco, where artists experimented with the DeepDream algorithm and the 2017 exhibition ""Unhuman: Art in the Age of AI"", which took place in Los Angeles and Frankfurt. In spring 2018, the Association for Computing Machinery dedicated a magazine issue to the subject of computers and art. In June 2018, ""Duet for Human and Machine"", an art piece permitting viewers to interact with an artificial intelligence, premiered at the Beall Center for Art + Technology. The Austrian Ars Electronica and Museum of Applied Arts, Vienna opened exhibitions on AI in 2019. Ars Electronica's 2019 festival ""Out of the box"" explored art's role in a sustainable societal transformation.
Examples of such augmentation may include e.g. enabling expansion of noncommercial niche genres (common examples are cyberpunk derivatives like solarpunk) by amateurs, novel entertainment, novel imaginative childhood play, very fast prototyping, increasing art-making accessibility and artistic output per effort and/or expenses and/or time – e.g. via generating drafts, inspirations, draft-refinitions, and image-components (Inpainting).
Synthetic media, which includes AI art, has been described in 2022 as a major technology-driven trend that will affect business in the coming years.",326,"Artificial intelligence art
Impact and applications

The exhibition ""Thinking Machines: Art and Design in the Computer Age, 1959–1989"" at MoMA provided an overview of AI applications for art, architecture, and design. Exhibitions showcasing the usage of AI to produce art include the 2016 Google-sponsored benefit and auction at the Gray Area Foundation in San Francisco, where artists experimented with the DeepDream algorithm and the 2017 exhibition ""Unhuman: Art in the Age of AI"", which took place in Los Angeles and Frankfurt. In spring 2018, the Association for Computing Machinery dedicated a magazine issue to the subject of computers and art. In June 2018, ""Duet for Human and Machine"", an art piece permitting viewers to interact with an artificial intelligence, premiered at the Beall Center for Art + Technology. The Austrian Ars Electronica and Museum of Applied Arts, Vienna opened exhibitions on AI in 2019. Ars Electronica's 2019 festival ""Out of the box"" explored art's role in a sustainable societal transformation.
Examples of such augmentation may include e.g. enabling expansion of noncommercial niche genres (common examples are cyberpunk derivatives like solarpunk) by amateurs, novel entertainment, novel imaginative childhood play, very fast prototyping, increasing art-making accessibility and artistic output per effort and/or expenses and/or time – e.g. via generating drafts, inspirations, draft-refinitions, and image-components (Inpainting).
Synthetic media, which includes AI art, has been described in 2022 as a major technology-driven trend that will affect business in the coming years.","1. What is the impact of AI on art?
2. What are some applications of AI in art?
3. What is the role of computers and art?
4. What is the future of AI and art?","1. The impact of AI on art is that it has allowed for new ways of creating art.
2. Some applications of AI in art are that it can be used to create new types of art, to help with the creation of art, and to allow for more people to create art.
3. The role of computers and art is that they can be used together to create new types of art.
4. The future of AI and art is that they will continue to work together to create new types of art."
Artificial intelligence art,Prompt engineering and sharing,"Prompts for some text-to-image models can also include images and keywords and configurable parameters, such as artistic style, which is often used via keyphrases like ""in the style of [name of an artist]"" in the prompt and/or selection of a broad aesthetic/art style. There are platforms for sharing, trading, searching, forking/refining and/or collaborating on prompts for generating specific imagery from image generators. Prompts are often shared along with images on image-sharing websites such as reddit and AI art-dedicated websites. They are not the complete input or details used for the generation of images.",136,"Artificial intelligence art
Prompt engineering and sharing

Prompts for some text-to-image models can also include images and keywords and configurable parameters, such as artistic style, which is often used via keyphrases like ""in the style of [name of an artist]"" in the prompt and/or selection of a broad aesthetic/art style. There are platforms for sharing, trading, searching, forking/refining and/or collaborating on prompts for generating specific imagery from image generators. Prompts are often shared along with images on image-sharing websites such as reddit and AI art-dedicated websites. They are not the complete input or details used for the generation of images.","1. What is an example of an AI art prompt?
2. What is the difference between an AI art prompt and the input used to generate images?
3. What are some popular websites for sharing AI art prompts?","1. An example of an AI art prompt is ""in the style of Vincent van Gogh.""
2. An AI art prompt is a description of the input that is used to generate images. The input can include images, keywords, and configurable parameters.
3. Some popular websites for sharing AI art prompts are reddit and AI art-dedicated websites."
Artificial intelligence art,Development,"Additional functionalities are under development and may improve various applications or enable new ones – such as ""Textual Inversion"" which refers to enabling the use of user-provided concepts (like an object or a style) learned from few images. With textual inversion, novel personalized art can be generated from the associated word(s) (the keywords that have been assigned to the learned, often abstract, concept) and model extensions/fine-tuning (see also: DreamBooth).
Generated images are sometimes used as sketches or low-cost experimentations or illustration of proof-of-concept-stage ideas – additional functionalities or improvements may also relate to post-generation manual editing (polishing or artistic usage) of prompts-based art (such as subsequent tweaking with an image editor). In the case of Stable Diffusion, the main pre-trained model is shared on the Hugging Face Hub.",188,"Artificial intelligence art
Development

Additional functionalities are under development and may improve various applications or enable new ones – such as ""Textual Inversion"" which refers to enabling the use of user-provided concepts (like an object or a style) learned from few images. With textual inversion, novel personalized art can be generated from the associated word(s) (the keywords that have been assigned to the learned, often abstract, concept) and model extensions/fine-tuning (see also: DreamBooth).
Generated images are sometimes used as sketches or low-cost experimentations or illustration of proof-of-concept-stage ideas – additional functionalities or improvements may also relate to post-generation manual editing (polishing or artistic usage) of prompts-based art (such as subsequent tweaking with an image editor). In the case of Stable Diffusion, the main pre-trained model is shared on the Hugging Face Hub.","1. What is ""Textual Inversion""?
2. What are some applications of ""Textual Inversion""?
3. What is the Hugging Face Hub?","1. ""Textual Inversion"" is the ability to use user-provided concepts (like an object or a style) learned from few images.
2. Applications of ""Textual Inversion"" include personalized art and post-generation manual editing of prompts-based art.
3. The Hugging Face Hub is a repository of pre-trained models for ""Textual Inversion""."
Artificial intelligence art,Other,"Some prototype robots can create what is considered forms of art – such as dynamic cooking robots that can taste and readjust.There also is AI-assisted writing beyond copy-editing (including support in the generation of fictional stories such as helping with writer's block or inspiration or rewriting segments).AI could be and has been used in video game art beyond imagery only, especially for level design (e.g. for custom maps) and creating new content or interactive stories in video games.",100,"Artificial intelligence art
Other

Some prototype robots can create what is considered forms of art – such as dynamic cooking robots that can taste and readjust.There also is AI-assisted writing beyond copy-editing (including support in the generation of fictional stories such as helping with writer's block or inspiration or rewriting segments).AI could be and has been used in video game art beyond imagery only, especially for level design (e.g. for custom maps) and creating new content or interactive stories in video games.","1. What is AI-assisted writing?
2. What are some examples of AI-assisted writing?
3. How could AI be used in video game art?","1. AI-assisted writing is the use of AI to help with the writing process.
2. Some examples of AI-assisted writing include copy-editing and helping with writer's block or inspiration.
3. AI can be used in video game art to create new content or interactive stories."
Artificial intelligence art,Analysis of existing art using AI,"In addition to the creation of original art, research methods that utilize AI have been generated to quantitatively analyze digital art collections. This has been made possible due to large-scale digitization of artwork in the past few decades. Although the main goal of digitization was to allow for accessibility and exploration of these collections, the use of AI in analyzing them has brought about new research perspectives.Two computational methods, close reading and distant viewing, are the typical approaches used to analyze digitized art. Close reading focuses on specific visual aspects of one piece. Some tasks performed by machines in close reading methods include computational artist authentication and analysis of brushstrokes or texture properties. In contrast, through distant viewing methods, the similarity across an entire collection for a specific feature can be statistically visualized. Common tasks relating to this method include automatic classification, object detection, multimodal tasks, knowledge discovery in art history, and computational aesthetics. Whereas distant viewing includes the analysis of large collections, close reading involves one piece of artwork.
Researchers have also introduced models that predict emotional responses to art such as ArtEmis, a large-scale dataset with machine learning models that contain emotional reactions to visual art as well as predictions of emotion from images or text.According to CETINIC and SHE (2022), using artificial intelligence to analyse already-existing art collections can provide fresh perspectives on the development of artistic styles and the identification of artistic influences. AI-assisted study of existing art can also aid in the organization of art exhibitions and support the decision-making process for curators and art historians.AI programs can automatically generate new images of artwork similar to those learned from the sample. Humans just need to do is mainly input data and discriminate output, and automate the specific generation, the combination of AI mechanisms and human art creation mechanisms allows AI to produce works.",374,"Artificial intelligence art
Analysis of existing art using AI

In addition to the creation of original art, research methods that utilize AI have been generated to quantitatively analyze digital art collections. This has been made possible due to large-scale digitization of artwork in the past few decades. Although the main goal of digitization was to allow for accessibility and exploration of these collections, the use of AI in analyzing them has brought about new research perspectives.Two computational methods, close reading and distant viewing, are the typical approaches used to analyze digitized art. Close reading focuses on specific visual aspects of one piece. Some tasks performed by machines in close reading methods include computational artist authentication and analysis of brushstrokes or texture properties. In contrast, through distant viewing methods, the similarity across an entire collection for a specific feature can be statistically visualized. Common tasks relating to this method include automatic classification, object detection, multimodal tasks, knowledge discovery in art history, and computational aesthetics. Whereas distant viewing includes the analysis of large collections, close reading involves one piece of artwork.
Researchers have also introduced models that predict emotional responses to art such as ArtEmis, a large-scale dataset with machine learning models that contain emotional reactions to visual art as well as predictions of emotion from images or text.According to CETINIC and SHE (2022), using artificial intelligence to analyse already-existing art collections can provide fresh perspectives on the development of artistic styles and the identification of artistic influences. AI-assisted study of existing art can also aid in the organization of art exhibitions and support the decision-making process for curators and art historians.AI programs can automatically generate new images of artwork similar to those learned from the sample. Humans just need to do is mainly input data and discriminate output, and automate the specific generation, the combination of AI mechanisms and human art creation mechanisms allows AI to produce works.","1. What are the two main methods used to analyze digitized art?
2. What is the goal of digitization?
3. What is the purpose of ArtEmis?
4. How can AI help in the organization of art exhibitions?","1. The two main methods used to analyze digitized art are close reading and distant viewing.
2. The goal of digitization is to allow for accessibility and exploration of digital art collections.
3. The purpose of ArtEmis is to predict emotional responses to visual art.
4. AI can help in the organization of art exhibitions by automatically generating new images of artwork similar to those learned from the sample."
Artificial intelligence art,Sales,"An auction sale of artificial intelligence art was held at Christie's Auction House in New York in 2018, where the AI artwork Edmond de Belamy sold for $432,500, which was almost 45 times higher than its estimate of $7,000–$10,000. The artwork was created by ""Obvious"", a Paris-based collective.",74,"Artificial intelligence art
Sales

An auction sale of artificial intelligence art was held at Christie's Auction House in New York in 2018, where the AI artwork Edmond de Belamy sold for $432,500, which was almost 45 times higher than its estimate of $7,000–$10,000. The artwork was created by ""Obvious"", a Paris-based collective.","1. What is the name of the AI artwork that sold for $432,500 at Christie's Auction House in New York in 2018? 
2. Who created the AI artwork? 
3. What was the estimate of the AI artwork's value?","1. The AI artwork that sold for $432,500 at Christie's Auction House in New York in 2018 is called Edmond de Belamy. 
2. The AI artwork was created by Obvious, a Paris-based collective. 
3. The estimate of the AI artwork's value was $7,000–$10,000."
Artificial intelligence art,Copyright,"Ever since artists began using AI to create art in the 20th century, the use of AI-generated art has sparked a number of debates. In the 2020s, some of those debates concerned whether AI art can be defined as art or not and concerning the impact it will have on artists.In 1985, intellectual property law professor Pamela Samuelson considered the legal questions surrounding AI art authorship as it relates to copyright: who owns the copyright when the piece of art was created by artificial intelligence? Samuelson's article, ""Allocating Ownership Rights in Computer-Generated Works,"" argued that rights should be allocated to the user of the generator program. In response to the same question, a 2019 Florida Law Review article has presented three possible choices. First, the artificial intelligence itself becomes the copyright owner. To do this, Section 101 of the Copyright Act would need to be amended to define ""author"" as a natural person or a computer. Second, following Samuelson's argument, the user, programmer, or artificial intelligence company is the copyright owner. This would be an expansion of the ""work for hire"" doctrine, under which ownership of a copyright is transferred to the ""employer."" Finally, no one becomes the copyright owner, and the work would automatically enter public domain. The argument here is that because no person ""created"" the piece of art, no one should be the copyright owner.In 2022, coinciding with the rising availability of consumer-grade AI image generation services, popular discussion renewed over the legality and ethics of AI-generated art. Of particular issue is the use of copyrighted art within AI training datasets: in September 2022, Reema Selhi, of the Design and Artists Copyright Society, stated that ""there are no safeguards for artists to be able to identify works in databases that are being used and opt out."" Some have claimed that images generated by these models can bear an uncanny resemblance to extant artwork, sometimes including remains of the original artist's signature. Such discussion came to a head in December, when users of the portfolio platform ArtStation staged an online protest against nonconsensual use of their artwork within datasets: this resulted in opt-out services, such as ""Have I Been Trained?,"" increasing in profile, as well as some online art platforms promising to offer their own opt-out options. According to the US Copyright Office, artificial intelligence programs are unable to hold copyright.In January 2023 three artists — Sarah Andersen, Kelly McKernan, and Karla Ortiz — filed a copyright infringement lawsuit against Stability AI, Midjourney, and DeviantArt, claiming that these companies have infringed the rights of millions of artists by training AI tools on five billion images scraped from the web without the consent of the original artists. The same month, Stability AI was also sued by Getty Images for using its images in the training data.",583,"Artificial intelligence art
Copyright

Ever since artists began using AI to create art in the 20th century, the use of AI-generated art has sparked a number of debates. In the 2020s, some of those debates concerned whether AI art can be defined as art or not and concerning the impact it will have on artists.In 1985, intellectual property law professor Pamela Samuelson considered the legal questions surrounding AI art authorship as it relates to copyright: who owns the copyright when the piece of art was created by artificial intelligence? Samuelson's article, ""Allocating Ownership Rights in Computer-Generated Works,"" argued that rights should be allocated to the user of the generator program. In response to the same question, a 2019 Florida Law Review article has presented three possible choices. First, the artificial intelligence itself becomes the copyright owner. To do this, Section 101 of the Copyright Act would need to be amended to define ""author"" as a natural person or a computer. Second, following Samuelson's argument, the user, programmer, or artificial intelligence company is the copyright owner. This would be an expansion of the ""work for hire"" doctrine, under which ownership of a copyright is transferred to the ""employer."" Finally, no one becomes the copyright owner, and the work would automatically enter public domain. The argument here is that because no person ""created"" the piece of art, no one should be the copyright owner.In 2022, coinciding with the rising availability of consumer-grade AI image generation services, popular discussion renewed over the legality and ethics of AI-generated art. Of particular issue is the use of copyrighted art within AI training datasets: in September 2022, Reema Selhi, of the Design and Artists Copyright Society, stated that ""there are no safeguards for artists to be able to identify works in databases that are being used and opt out."" Some have claimed that images generated by these models can bear an uncanny resemblance to extant artwork, sometimes including remains of the original artist's signature. Such discussion came to a head in December, when users of the portfolio platform ArtStation staged an online protest against nonconsensual use of their artwork within datasets: this resulted in opt-out services, such as ""Have I Been Trained?,"" increasing in profile, as well as some online art platforms promising to offer their own opt-out options. According to the US Copyright Office, artificial intelligence programs are unable to hold copyright.In January 2023 three artists — Sarah Andersen, Kelly McKernan, and Karla Ortiz — filed a copyright infringement lawsuit against Stability AI, Midjourney, and DeviantArt, claiming that these companies have infringed the rights of millions of artists by training AI tools on five billion images scraped from the web without the consent of the original artists. The same month, Stability AI was also sued by Getty Images for using its images in the training data.","1. What is the main debate concerning AI art?
2. What is the definition of an author under Section 101 of the Copyright Act?
3. Who would be the copyright owner of AI art under Samuelson's argument?
4. How have AI-generated images been used in the past?
5. What is the issue with using copyrighted art within AI training datasets?
6. Who are the plaintiffs in the copyright infringement lawsuit against Stability AI, Midjourney, and Deviant","1. The main debate concerning AI art is whether or not it can be defined as art.
2. The definition of an author under Section 101 of the Copyright Act is a natural person or a computer.
3. The user, programmer, or artificial intelligence company would be the copyright owner under Samuelson's argument.
4. AI-generated images have been used in the past for training AI tools without the consent of the original artists.
5. The issue with using copyrighted art within AI training datasets is that it can infringe on the rights of the original artists.
6. The plaintiffs in the copyright infringement lawsuit against Stability AI, Midjourney, and Deviant are three artists who claim that these companies have infringed the rights of millions of artists by training AI tools on five billion images scraped from the web without the consent of the original artists."
Artificial intelligence art,Concerns about impact on artists,"Some artists in 2022 raised concerns about the impact AI image generators could have on their ability to earn money, particularly if AI images are used to replace artists working in illustration and design. In August 2022, a text-to-image AI illustration titled Théâtre d'Opéra Spatial won the first-place $300 prize in a digital art competition at the Colorado State Fair.Digital artist R. J. Palmer said in August 2022 that ""I could easily envision a scenario where using AI a single artist or art director could take the place of 5-10 entry level artists... I have seen a lot of self-published authors and such say how great it will be that they don’t have to hire an artist,"" adding that ""doing that kind of work for small creators is how a lot of us got our start as professional artists."" Polish digital artist Greg Rutkowski said in September 2022 that ""it's starting to look like a threat to our careers,"" adding that it has gotten more difficult to search for his work online because many of the images returned by search engines are generated by AI that was prompted to mimic his style.",237,"Artificial intelligence art
Concerns about impact on artists

Some artists in 2022 raised concerns about the impact AI image generators could have on their ability to earn money, particularly if AI images are used to replace artists working in illustration and design. In August 2022, a text-to-image AI illustration titled Théâtre d'Opéra Spatial won the first-place $300 prize in a digital art competition at the Colorado State Fair.Digital artist R. J. Palmer said in August 2022 that ""I could easily envision a scenario where using AI a single artist or art director could take the place of 5-10 entry level artists... I have seen a lot of self-published authors and such say how great it will be that they don’t have to hire an artist,"" adding that ""doing that kind of work for small creators is how a lot of us got our start as professional artists."" Polish digital artist Greg Rutkowski said in September 2022 that ""it's starting to look like a threat to our careers,"" adding that it has gotten more difficult to search for his work online because many of the images returned by search engines are generated by AI that was prompted to mimic his style.","1. What is the opinion of artists about AI image generators?
2. How do AI illustrations compare to those created by humans?
3. What impact could AI have on artists' careers?","1. Some artists in 2022 raised concerns about the impact AI image generators could have on their ability to earn money.
2. AI illustrations compare to those created by humans in that they are often used to replace artists working in illustration and design.
3. The impact that AI could have on artists' careers is that it could make it more difficult for them to find work, as many of the images returned by search engines are generated by AI."
Artificial intelligence art,Deception,"The 2023 winner of the ""creative open"" category in the Sony World Photography Awards, Boris Eldagsen, revealed after winning that his entry was actually generated by artificial intelligence. Photographer Feroz Khan commented to the BBC that Eldagsen had ""clearly shown that even experienced photographers and art experts can be fooled"". Smaller contests have been affected as well; in 2023 a contest called the ""Self-Published Fantasy Blog-Off cover contest"", run by author Mark Lawrence, was cancelled after the winning entry was allegedly exposed to be a collage of images generated by Midjourney.Wider issues extend beyond the art world. As with other types of photo manipulation since the early 19th century, some people in the early 21st century have been concerned that AI could be used to create content that is misleading, known as ""deepfakes"".

In May 2023, widespread attention was given to a Midjourney-generated photo of Pope Francis wearing a white puffer coat and another showing the fictional arrest of Donald Trump, and an AI-generated image of an attack on the Pentagon went viral as a hoax news story on Twitter.",237,"Artificial intelligence art
Deception

The 2023 winner of the ""creative open"" category in the Sony World Photography Awards, Boris Eldagsen, revealed after winning that his entry was actually generated by artificial intelligence. Photographer Feroz Khan commented to the BBC that Eldagsen had ""clearly shown that even experienced photographers and art experts can be fooled"". Smaller contests have been affected as well; in 2023 a contest called the ""Self-Published Fantasy Blog-Off cover contest"", run by author Mark Lawrence, was cancelled after the winning entry was allegedly exposed to be a collage of images generated by Midjourney.Wider issues extend beyond the art world. As with other types of photo manipulation since the early 19th century, some people in the early 21st century have been concerned that AI could be used to create content that is misleading, known as ""deepfakes"".

In May 2023, widespread attention was given to a Midjourney-generated photo of Pope Francis wearing a white puffer coat and another showing the fictional arrest of Donald Trump, and an AI-generated image of an attack on the Pentagon went viral as a hoax news story on Twitter.","1. What is the name of the photographer who won the Sony World Photography Awards in 2023?
2. What was the winning entry for the ""creative open"" category?
3. What was the contest that was cancelled in 2023 after the winning entry was allegedly exposed to be a collage of images generated by Midjourney?
4. What is the concern with AI-generated content that is misleading?
5. What was the name of the author who ran the ""Self-","1. Boris Eldagsen
2. AI-generated
3. The ""Self-Published Fantasy Blog-Off cover contest""
4. That AI could be used to create content that is misleading, known as ""deepfakes""
5. Mark Lawrence"
