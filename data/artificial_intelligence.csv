title,section,text,ntokens
Artificial intelligence,Summary,"Artificial intelligence (AI) is intelligence demonstrated by computers, as opposed to human or animal intelligence. ""Intelligence"" encompasses the ability to learn and to reason, to generalize, and to infer meaning.AI applications include advanced web search engines (e.g., Google Search), recommendation systems (used by YouTube, Amazon, and Netflix), understanding human speech (such as Siri and Alexa), self-driving cars (e.g., Waymo), generative or creative tools (ChatGPT and AI art), automated decision-making, and competing at the highest level in strategic game systems (such as chess and Go).Artificial intelligence was founded as an academic discipline in 1956, and in the years since it has experienced several waves of optimism, followed by disappointment and the loss of funding (known as an ""AI winter""), followed by new approaches, success, and renewed funding. AI research has tried and discarded many different approaches, including simulating the brain, modeling human problem solving, formal logic, large databases of knowledge, and imitating animal behavior. In the first decades of the 21st century, highly mathematical and statistical machine learning has dominated the field, and this technique has proved highly successful, helping to solve many challenging problems throughout industry and academia.The various sub-fields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and the ability to move and manipulate objects. General intelligence (the ability to solve an arbitrary problem) is among the field's long-term goals. To solve these problems, AI researchers have adapted and integrated a wide range of problem-solving techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, probability, and economics. AI also draws upon computer science, psychology, linguistics, philosophy, and many other fields.
The field was founded on the assumption that human intelligence ""can be so precisely described that a machine can be made to simulate it"". This raised philosophical arguments about the mind and the ethical consequences of creating artificial beings endowed with human-like intelligence; these issues have previously been explored by myth, fiction (science fiction), and philosophy since antiquity. Computer scientists and philosophers have since suggested that AI may become an existential risk to humanity if its rational capacities are not steered towards goals beneficial to humankind. Economists have frequently highlighted the risks of redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment. The term artificial intelligence has also been criticized for overhyping AI's true technological capabilities.",536
Artificial intelligence,History,"Artificial beings with intelligence appeared as storytelling devices in antiquity, and have been common in fiction, as in Mary Shelley's Frankenstein or Karel Čapek's R.U.R. These characters and their fates raised many of the same issues now discussed in the ethics of artificial intelligence.The study of mechanical or ""formal"" reasoning began with philosophers and mathematicians in antiquity. The study of mathematical logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as ""0"" and ""1"", could simulate any conceivable act of mathematical deduction. This insight that digital computers can simulate any process of formal reasoning is known as the Church–Turing thesis. This, along with concurrent discoveries in neurobiology, information theory and cybernetics, led researchers to consider the possibility of building an electronic brain. The first work that is now generally recognized as AI was McCullouch and Pitts' 1943 formal design for Turing-complete ""artificial neurons"".The field of AI research was born at a workshop at Dartmouth College in 1956. The attendees became the founders and leaders of AI research. They and their students produced programs that the press described as ""astonishing"": computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English. 
By the middle of the 1960s, research in the U.S. was heavily funded by the Department of Defense and laboratories had been established around the world.Researchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with artificial general intelligence and considered this the goal of their field. Herbert Simon predicted, ""machines will be capable, within twenty years, of doing any work a man can do"". Marvin Minsky agreed, writing, ""within a generation ... the problem of creating 'artificial intelligence' will substantially be solved"".They had failed to recognize the difficulty of some of the remaining tasks. Progress slowed and in 1974, in response to the criticism of Sir James Lighthill and ongoing pressure from the US Congress to fund more productive projects, both the U.S. and British governments cut off exploratory research in AI. The next few years would later be called an ""AI winter"", a period when obtaining funding for AI projects was difficult.In the early 1980s, AI research was revived by the commercial success of expert systems, a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research. However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.Many researchers began to doubt that the current practices would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition. A number of researchers began to look into ""sub-symbolic"" approaches to specific AI problems. Robotics researchers, such as Rodney Brooks, rejected symbolic AI and focused on the basic engineering problems that would allow robots to move, survive, and learn their environment.Interest in neural networks and ""connectionism"" was revived by Geoffrey Hinton, David Rumelhart and others in the middle of the 1980s. Soft computing tools were developed in the 1980s, such as neural networks, fuzzy systems, Grey system theory, evolutionary computation and many tools drawn from statistics or mathematical optimization.
AI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This ""narrow"" and ""formal"" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics, economics and mathematics). 
By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as ""artificial intelligence"".Faster computers
and access to large amounts of data 
enabled advances in machine learning and perception; data-hungry deep learning methods started to dominate accuracy benchmarks around 2012.According to Bloomberg's Jack Clark, 2015 was a landmark year for artificial intelligence, with the number of software projects that use AI within Google increased from a ""sporadic usage"" in 2012 to more than 2,700 projects. He attributed this to an increase in affordable neural networks, due to a rise in cloud computing infrastructure and to an increase in research tools and datasets. In a 2017 survey, one in five companies reported they had ""incorporated AI in some offerings or processes"". The amount of research into AI (measured by total publications) increased by 50% in the years 2015–2019. According to AI Impacts at Stanford, around 2022 about $50 billion annually is invested in artificial intelligence in the US, and about 20% of new US Computer Science PhD graduates have specialized in artificial intelligence; about 800,000 AI-related US job openings existed in 2022.Numerous academic researchers became concerned that AI was no longer pursuing the original goal of creating versatile, fully intelligent machines. Much of current research involves statistical AI, which is overwhelmingly used to solve specific problems, even highly successful techniques such as deep learning. This concern has led to the subfield of artificial general intelligence (or ""AGI""), which had several well-funded institutions by the 2010s.",1113
Artificial intelligence,Goals,The general problem of simulating (or creating) intelligence has been broken down into sub-problems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention.,50
Artificial intelligence,"Reasoning, problem-solving","Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, AI research had developed methods for dealing with uncertain or incomplete information, employing concepts from probability and economics.Many of these algorithms proved to be insufficient for solving large reasoning problems because they experienced a ""combinatorial explosion"": they became exponentially slower as the problems grew larger. Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments.",120
Artificial intelligence,Knowledge representation,"Knowledge representation and knowledge engineering allow AI programs to answer questions intelligently and make deductions about real-world facts.
A representation of ""what exists"" is an ontology: the set of objects, relations, concepts, and properties formally described so that software agents can interpret them. The most general ontologies are called upper ontologies, which attempt to provide a foundation for all other knowledge and act as mediators between domain ontologies that cover specific knowledge about a particular knowledge domain (field of interest or area of concern). A truly intelligent program would also need access to commonsense knowledge, the set of facts that an average person knows. The semantics of an ontology is typically represented in description logic, such as the Web Ontology Language.AI research has developed tools to represent specific domains, such as objects, properties, categories and relations between objects; situations, events, states and time; causes and effects; knowledge about knowledge (what we know about what other people know); and  default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing);. Among the most difficult problems in AI are: the breadth of commonsense knowledge (the number of atomic facts that the average person knows is enormous); and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as ""facts"" or ""statements"" that they could express verbally).Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining ""interesting"" and actionable inferences from large databases), and other areas.",335
Artificial intelligence,Learning,"Machine learning (ML), a fundamental concept of AI research since the field's inception, is the study of computer algorithms that improve automatically through experience.Unsupervised learning finds patterns in a stream of input.
Supervised learning requires a human to label the input data first, and comes in two main varieties: classification and numerical regression. Classification is used to determine what category something belongs in – the program sees a number of examples of things from several categories and will learn to classify new inputs. Regression is the attempt to produce a function that describes the relationship between inputs and outputs and predicts how the outputs should change as the inputs change. Both classifiers and regression learners can be viewed as ""function approximators"" trying to learn an unknown (possibly implicit) function; for example, a spam classifier can be viewed as learning a function that maps from the text of an email to one of two categories, ""spam"" or ""not spam"".In reinforcement learning the agent is rewarded for good responses and punished for bad ones. The agent classifies its responses to form a strategy for operating in its problem space.Transfer learning is when the knowledge gained from one problem is applied to a new problem.Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.",273
Artificial intelligence,Natural language processing,"Natural language processing (NLP)
allows machines to read and understand human language. A sufficiently powerful natural language processing system would enable natural-language user interfaces and the acquisition of knowledge directly from human-written sources, such as newswire texts. Some straightforward applications of NLP include information retrieval, question answering and machine translation.Symbolic AI used formal syntax to translate the deep structure of sentences into logic. This failed to produce useful applications, due to the intractability of logic and the breadth of commonsense knowledge. Modern statistical techniques include co-occurrence frequencies (how often one word appears near another), ""Keyword spotting"" (searching for a particular word to retrieve information), transformer-based deep learning (which finds patterns in text), and others. They have achieved acceptable accuracy at the page or paragraph level, and, by 2019, could generate coherent text.",180
Artificial intelligence,Perception,"Machine perception
is the ability to use input from sensors (such as cameras, microphones, wireless signals, and active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Applications include speech recognition,facial recognition, and object recognition.
Computer vision is the ability to analyze visual input.",70
Artificial intelligence,Social intelligence,"Affective computing is an interdisciplinary umbrella that comprises systems that recognize, interpret, process or simulate human feeling, emotion and mood.
For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human–computer interaction.
However, this tends to give naïve users an unrealistic conception of how intelligent existing computer agents actually are. Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis, wherein AI classifies the affects displayed by a videotaped subject.",129
Artificial intelligence,General intelligence,"A machine with general intelligence can solve a wide variety of problems with breadth and versatility similar to human intelligence. There are several competing ideas about how to develop artificial general intelligence. Hans Moravec and Marvin Minsky argue that work in different individual domains can be incorporated into an advanced multi-agent system or cognitive architecture with general intelligence.Pedro Domingos hopes that there is a conceptually straightforward, but mathematically difficult, ""master algorithm"" that could lead to AGI.
Others believe that anthropomorphic features like an artificial brain
or simulated child development
will someday reach a critical point where general intelligence emerges.",127
Artificial intelligence,Tools,,2
Artificial intelligence,Search and optimization,"AI can solve many problems by intelligently searching through many possible solutions. Reasoning can be reduced to performing a search. For example, logical proof can be viewed as searching for a path that leads from premises to conclusions, where each step is the application of an inference rule. Planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis. Robotics algorithms for moving limbs and grasping objects use local searches in configuration space.Simple exhaustive searches
are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes. The solution, for many problems, is to use ""heuristics"" or ""rules of thumb"" that prioritize choices in favor of those more likely to reach a goal and to do so in a shorter number of steps. In some search methodologies, heuristics can also serve to eliminate some choices unlikely to lead to a goal (called ""pruning the search tree""). Heuristics supply the program with a ""best guess"" for the path on which the solution lies.
Heuristics limit the search for solutions into a smaller sample size.

A very different kind of search came to prominence in the 1990s, based on the mathematical theory of optimization. For many problems, it is possible to begin the search with some form of a guess and then refine the guess incrementally until no more refinements can be made. These algorithms can be visualized as blind hill climbing: we begin the search at a random point on the landscape, and then, by jumps or steps, we keep moving our guess uphill, until we reach the top. Other related optimization algorithms include random optimization, beam search and metaheuristics like simulated annealing. Evolutionary computation uses a form of optimization search. For example, they may begin with a population of organisms (the guesses) and then allow them to mutate and recombine, selecting only the fittest to survive each generation (refining the guesses). Classic evolutionary algorithms include genetic algorithms, gene expression programming, and genetic programming. Alternatively, distributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).",488
Artificial intelligence,Logic,"Logic
is used for knowledge representation and problem-solving, but it can be applied to other problems as well. For example, the satplan algorithm uses logic for planning
and inductive logic programming is a method for learning.Several different forms of logic are used in AI research. Propositional logic involves truth functions such as ""or"" and ""not"". First-order logic
adds quantifiers and predicates and can express facts about objects, their properties, and their relations with each other. Fuzzy logic assigns a ""degree of truth"" (between 0 and 1) to vague statements such as ""Alice is old"" (or rich, or tall, or hungry), that are too linguistically imprecise to be completely true or false.Default logics, non-monotonic logics and circumscription are forms of logic designed to help with default reasoning and the qualification problem.
Several extensions of logic have been designed to handle specific domains of knowledge, such as description logics;situation calculus, event calculus and fluent calculus (for representing events and time);causal calculus;belief calculus (belief revision); and modal logics.
Logics to model contradictory or inconsistent statements arising in multi-agent systems have also been designed, such as paraconsistent logics.",271
Artificial intelligence,Probabilistic methods for uncertain reasoning,"Many problems in AI (including in reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics.Bayesian networks
are a very general tool that can be used for various problems, including reasoning (using the Bayesian inference algorithm),learning (using the expectation-maximization algorithm),planning (using decision networks) and perception (using dynamic Bayesian networks).
Probabilistic algorithms can also be used for filtering, prediction, smoothing and finding explanations for streams of data, helping perception systems to analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).A key concept from the science of economics is ""utility"", a measure of how valuable something is to an intelligent agent. Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis,
and information value theory. These tools include models such as Markov decision processes, dynamic decision networks, game theory and mechanism design.",236
Artificial intelligence,Classifiers and statistical learning methods,"The simplest AI applications can be divided into two types: classifiers (""if shiny then diamond"") and controllers (""if diamond then pick up""). Controllers do, however, also classify conditions before inferring actions, and therefore classification forms a central part of many AI systems. Classifiers are functions that use pattern matching to determine the closest match. They can be tuned according to examples, making them very attractive for use in AI. These examples are known as observations or patterns. In supervised learning, each pattern belongs to a certain predefined class. A class is a decision that has to be made. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.A classifier can be trained in various ways; there are many statistical and machine learning approaches.
The decision tree is the simplest and most widely used symbolic machine learning algorithm.K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s.Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.
The naive Bayes classifier is reportedly the ""most widely used learner"" at Google, due in part to its scalability.Neural networks are also used for classification.Classifier performance depends greatly on the characteristics of the data to be classified, such as the dataset size, distribution of samples across classes, dimensionality, and the level of noise. Model-based classifiers perform well if the assumed model is an extremely good fit for the actual data. Otherwise, if no matching model is available, and if accuracy (rather than speed or scalability) is the sole concern, conventional wisdom is that discriminative classifiers (especially SVM) tend to be more accurate than model-based classifiers such as ""naive Bayes"" on most practical data sets.",394
Artificial intelligence,Artificial neural networks,"Neural networks
were inspired by the architecture of neurons in the human brain. A simple ""neuron"" N accepts input from other neurons, each of which, when activated (or ""fired""), casts a weighted ""vote"" for or against whether neuron N should itself activate. Learning requires an algorithm to adjust these weights based on the training data; one simple algorithm (dubbed ""fire together, wire together"") is to increase the weight between two connected neurons when the activation of one triggers the successful activation of another. Neurons have a continuous spectrum of activation; in addition, neurons can process inputs in a nonlinear way rather than weighing straightforward votes.
Modern neural networks model complex relationships between inputs and outputs and find patterns in data. They can learn continuous functions and even digital logical operations. Neural networks can be viewed as a type of mathematical optimization – they perform gradient descent on a multi-dimensional topology that was created by training the network. The most common training technique is the backpropagation algorithm.
Other learning techniques for neural networks are Hebbian learning (""fire together, wire together""), GMDH or competitive learning.The main categories of networks are acyclic or feedforward neural networks (where the signal passes in only one direction) and recurrent neural networks (which allow feedback and short-term memories of previous input events). Among the most popular feedforward networks are perceptrons, multi-layer perceptrons and radial basis networks.",299
Artificial intelligence,Deep learning,"Deep learning
uses several layers of neurons between the network's inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits or letters or faces. Deep learning has drastically improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, image classification and others.
Deep learning often uses convolutional neural networks for many or all of its layers. In a convolutional layer, each neuron receives input from only a restricted area of the previous layer called the neuron's receptive field. This can substantially reduce the number of weighted connections between neurons, and creates a hierarchy similar to the organization of the animal visual cortex.In a recurrent neural network (RNN) the signal will propagate through a layer more than once;
thus, an RNN is an example of deep learning.
RNNs can be trained by gradient descent,
however long-term gradients which are back-propagated can ""vanish"" (that is, they can tend to zero) or ""explode"" (that is, they can tend to infinity), known as the vanishing gradient problem.
The long short term memory (LSTM) technique can prevent this in most cases.",282
Artificial intelligence,Specialized languages and hardware,"Specialized languages for artificial intelligence have been developed, such as Lisp, Prolog, TensorFlow and many others. Hardware developed for AI includes AI accelerators and neuromorphic computing. By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced central processing unit (CPUs) as the dominant means to train large-scale commercial cloud AI.",82
Artificial intelligence,Applications,"AI is relevant to any intellectual task. Modern artificial intelligence techniques are pervasive and are too numerous to list here. Frequently, when a technique reaches mainstream use, it is no longer considered artificial intelligence; this phenomenon is described as the AI effect.In the 2010s, AI applications were at the heart of the most commercially successful areas of computing, and have become a ubiquitous feature of daily life. AI is used in search engines (such as Google Search),
targeting online advertisements, recommendation systems (offered by Netflix, YouTube or Amazon), driving internet traffic, targeted advertising (AdSense, Facebook), virtual assistants (such as Siri or Alexa), autonomous vehicles (including drones, ADAS and self-driving cars), automatic language translation (Microsoft Translator, Google Translate), facial recognition (Apple's Face ID or Microsoft's DeepFace), image labeling (used by Facebook, Apple's iPhoto and TikTok), spam filtering and chatbots (such as ChatGPT).
There are also thousands of successful AI applications used to solve problems for specific industries or institutions. A few examples are energy storage, deepfakes, medical diagnosis, military logistics, foreign policy, or supply chain management.
Game playing has been a test of AI's strength since the 1950s. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997. In 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin.
In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. Other programs handle imperfect-information games; such as for poker at a superhuman level, Pluribus and Cepheus. DeepMind in the 2010s developed a ""generalized artificial intelligence"" that could learn many diverse Atari games on its own.DeepMind's AlphaFold 2 (2020) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein. Other applications predict the result of judicial decisions, create art (such as poetry or painting) and prove mathematical theorems.
Generative AI gained widespread prominence in the 2020s. ""Large language model"" systems such as GPT-3 (2020), with 175 billion parameters, matched human performance on pre-existing benchmarks, albeit without the system attaining a commonsense understanding of the contents of the benchmarks. A Pew Research poll conducted in March 2023 found that 14% of Americans adults had tried ChatGPT, a large language model fine-tuned using reinforcement learning from human feedback.  While estimates varied wildly, Goldman Sachs suggested in 2023 that generative language AI could increase global GDP by 7% in the next ten years.In 2023, the increasing realism and ease-of-use of AI-based text-to-image generators such as Midjourney, DALL-E, and Stable Diffusion sparked a trend of viral AI-generated photos. Widespread attention was gained by a fake photo of Pope Francis wearing a white puffer coat, the fictional arrest of Donald Trump, and a hoax of an attack on the Pentagon, as well as the usage in professional creative arts.",700
Artificial intelligence,Intellectual property,"In 2019, WIPO reported that AI was the most prolific emerging technology in terms of the number of patent applications and granted patents, the Internet of things was estimated to be the largest in terms of market size. It was followed, again in market size, by big data technologies, robotics, AI, 3D printing and the fifth generation of mobile services (5G). Since AI emerged in the 1950s, 340,000 AI-related patent applications were filed by innovators and 1.6 million scientific papers have been published by researchers, with the majority of all AI-related patent filings published since 2013. Companies represent 26 out of the top 30 AI patent applicants, with universities or public research organizations accounting for the remaining four. The ratio of scientific papers to inventions has significantly decreased from 8:1 in 2010 to 3:1 in 2016, which is attributed to be indicative of a shift from theoretical research to the use of AI technologies in commercial products and services. Machine learning is the dominant AI technique disclosed in patents and is included in more than one-third of all identified inventions (134,777 machine learning patents filed for a total of 167,038 AI patents filed in 2016), with computer vision being the most popular functional application. AI-related patents not only disclose AI techniques and applications, they often also refer to an application field or industry. Twenty application fields were identified in 2016 and included, in order of magnitude: telecommunications (15 percent), transportation (15 percent), life and medical sciences (12 percent), and personal devices, computing and human–computer interaction (11 percent). Other sectors included banking, entertainment, security, industry and manufacturing, agriculture, and networks (including social networks, smart cities and the Internet of things). IBM has the largest portfolio of AI patents with 8,290 patent applications, followed by Microsoft with 5,930 patent applications.",377
Artificial intelligence,Philosophy,,2
Artificial intelligence,Defining artificial intelligence,"Alan Turing wrote in 1950 ""I propose to consider the question 'can machines think'?""
He advised changing the question from whether a machine ""thinks"", to ""whether or not it is possible for machinery to show intelligent behaviour"".
He devised the Turing test, which measures the ability of a machine to simulate human conversation. Since we can only observe the behavior of the machine, it does not matter if it is ""actually"" thinking or literally has a ""mind"". Turing notes that we can not determine these things about other people but ""it is usual to have a polite convention that everyone thinks""Russell and Norvig agree with Turing that AI must be defined in terms of ""acting"" and not ""thinking"". However, they are critical that the test compares machines to people. ""Aeronautical engineering texts,"" they wrote, ""do not define the goal of their field as making 'machines that fly so exactly like pigeons that they can fool other pigeons.'"" AI founder John McCarthy agreed, writing that ""Artificial intelligence is not, by definition, simulation of human intelligence"".McCarthy defines intelligence as ""the computational part of the ability to achieve goals in the world."" Another AI founder, Marvin Minsky similarly defines it as ""the ability to solve hard problems"". These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the ""intelligence"" of the machine—and no other philosophical discussion is required, or may not even be possible.
A definition that has also been adopted by Google – major practitionary in the field of AI.
This definition stipulated the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence.",373
Artificial intelligence,Evaluating approaches to AI,"No established unifying theory or paradigm has guided AI research for most of its history. The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term ""artificial intelligence"" to mean ""machine learning with neural networks""). This approach is mostly sub-symbolic, neat, soft and narrow (see below). Critics argue that these questions may have to be revisited by future generations of AI researchers.",107
Artificial intelligence,Symbolic AI and its limits,"Symbolic AI (or ""GOFAI"") simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at ""intelligent"" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: ""A physical symbol system has the necessary and sufficient means of general intelligent action.""However, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level ""intelligent"" tasks were easy for AI, but low level ""instinctive"" tasks were extremely difficult.
Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a ""feel"" for the situation, rather than explicit symbolic knowledge.
Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree.The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence, in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches.",327
Artificial intelligence,Neat vs. scruffy,"""Neats"" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). ""Scruffies"" expect that it necessarily requires solving a large number of unrelated problems. Neats defend their programs with theoretical rigor, scruffies rely only on incremental testing to see if they work. This issue was actively discussed in the 70s and 80s,
but eventually was seen as irrelevant. In the 1990s mathematical methods and solid scientific standards became the norm, a transition that Russell and Norvig termed in 2003 as ""the victory of the neats"". However in 2020 they wrote ""deep learning may represent a resurgence of the scruffies"". Modern AI has elements of both.",153
Artificial intelligence,Soft vs. hard computing,"Finding a provably correct or optimal solution is intractable for many important problems. Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 80s and most successful AI programs in the 21st century are examples of soft computing with neural networks.",84
Artificial intelligence,Narrow vs. general AI,"AI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence (general AI) directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals.
General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The experimental sub-field of artificial general intelligence studies this area exclusively.",103
Artificial intelligence,"Machine consciousness, sentience and mind","The philosophy of mind does not know whether a machine can have a mind, consciousness and mental states, in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field. Stuart Russell and Peter Norvig observe that most AI researchers ""don't care about the [philosophy of AI] – as long as the program works, they don't care whether you call it a simulation of intelligence or real intelligence."" However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction.",150
Artificial intelligence,Consciousness,"David Chalmers identified two problems in understanding the mind, which he named the ""hard"" and ""easy"" problems of consciousness. The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). Human information processing is easy to explain, however, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like.",155
Artificial intelligence,Computationalism and functionalism,"Computationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind–body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.Philosopher John Searle characterized this position as ""strong AI"": ""The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.""
Searle counters this assertion with his Chinese room argument, which attempts to show that, even if a machine perfectly simulates human behavior, there is still no reason to suppose it also has a mind.",184
Artificial intelligence,Robot rights,"If a machine has a mind and subjective experience, then it may also have sentience (the ability to feel), and if so it could also suffer; it has been argued that this could entitle it to certain rights.
Any hypothetical robot rights would lie on a spectrum with animal rights and human rights.
This issue has been considered in fiction for centuries,
and is now being considered by, for example, California's Institute for the Future; however, critics argue that the discussion is premature.",103
Artificial intelligence,Future,,2
Artificial intelligence,Superintelligence,"A superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind.If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to recursive self-improvement.
Its intelligence would increase exponentially in an intelligence explosion and could dramatically surpass humans. Science fiction writer Vernor Vinge named this scenario the ""singularity"".
Because it is difficult or impossible to know the limits of intelligence or the capabilities of superintelligent machines, the technological singularity is an occurrence beyond which events are unpredictable or even unfathomable.Robot designer Hans Moravec, cyberneticist Kevin Warwick, and inventor Ray Kurzweil have predicted that humans and machines will merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in Aldous Huxley and Robert Ettinger.Edward Fredkin argues that ""artificial intelligence is the next stage in evolution"", an idea first proposed by Samuel Butler's ""Darwin among the Machines"" as far back as 1863, and expanded upon by George Dyson in his book of the same name in 1998.",260
Artificial intelligence,Risks,,2
Artificial intelligence,Technological unemployment,"In the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that ""we're in uncharted territory"" with AI. A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed. Risk estimates vary; for example, in the 2010s Michael Osborne and Carl Benedikt Frey estimated 47% of U.S. jobs are at ""high risk"" of potential automation, while an OECD report classified only 9% of U.S. jobs as ""high risk"". The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology (rather than social policy) creates unemployment (as opposed to redundancies).Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that ""the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution"" is ""worth taking seriously"". Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.",262
Artificial intelligence,Bad actors and weaponized AI,"AI provides a number of tools that are particularly useful for authoritarian governments: smart spyware, face recognition and voice recognition allow widespread surveillance; such surveillance allows machine learning to classify potential enemies of the state and can prevent them from hiding; recommendation systems can precisely target propaganda and misinformation for maximum effect; deepfakes aid in producing misinformation; advanced AI can make centralized decision making more competitive with liberal and decentralized systems such as markets.Terrorists, criminals and rogue states may use other forms of weaponized AI such as advanced digital warfare and lethal autonomous weapons. By 2015, over fifty countries were reported to be researching battlefield robots.Machine-learning AI is also able to design tens of thousands of toxic molecules in a matter of hours.",152
Artificial intelligence,Algorithmic bias,"AI programs can become biased after learning from real-world data. It is not typically introduced by the system designers but is learned by the program, and thus the programmers are often unaware that the bias exists.
Bias can be inadvertently introduced by the way training data is selected.
It can also emerge from correlations: AI is used to classify individuals into groups and then make predictions assuming that the individual will resemble other members of the group. In some cases, this assumption may be unfair. An example of this is COMPAS, a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. ProPublica claims that the COMPAS-assigned recidivism risk level of black defendants is far more likely to be overestimated than that of white defendants, despite the fact that the program was not told the races of the defendants.Health equity issues may also be exacerbated when many-to-many mapping is done without taking steps to ensure equity for populations at risk for bias. At this time equity-focused tools and regulations are not in place to ensure equity application representation and usage. Other examples where algorithmic bias can lead to unfair outcomes are when AI is used for credit rating or hiring.
At its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022) the Association for Computing Machinery, in Seoul, South Korea, presented and published findings recommending that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed.",334
Artificial intelligence,Existential risk,"It has been argued AI will become so powerful that humanity may irreversibly lose control of it. This could, as the physicist Stephen Hawking puts it, ""spell the end of the human race"". According to the philosopher Nick Bostrom, for almost any goals that a sufficiently intelligent AI may have, it is instrumentally incentivized to protect itself from being shut down and to acquire more resources, as intermediary steps to better achieve these goals. Sentience or emotions are then not required for an advanced AI to be dangerous. In order to be safe for humanity, a superintelligence would have to be genuinely aligned with humanity's morality and values so that it is ""fundamentally on our side"". The political scientist Charles T. Rubin argued that ""any sufficiently advanced benevolence may be indistinguishable from malevolence"" and warned that we should not be confident that intelligent machines will by default treat us favorably.The opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI. Personalities such as Stephen Hawking, Bill Gates, Elon Musk have expressed concern about existential risk from AI. In 2023, AI pioneers including Geoffrey Hinton, Yoshua Bengio, Demis Hassabis, and Sam Altman issued the joint statement that ""Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war""; some others such as Yann LeCun consider this to be unfounded. Mark Zuckerberg stated that artificial intelligence is helpful in its current form and will continue to assist humans. Some experts have argued that the risks are too distant in the future to warrant research, or that humans will be valuable from the perspective of a superintelligent machine. Rodney Brooks, in particular, said in 2014 that ""malevolent"" AI is still centuries away.",379
Artificial intelligence,Copyright,"In order to leverage as large a dataset as is feasible, generative AI is often trained on unlicensed copyrighted works, including in domains such as images or computer code; the output is then used under a rationale of ""fair use"". Experts disagree about how well, and under what circumstances, this rationale will hold up in courts of law; relevant factors may include ""the purpose and character of the use of the copyrighted work"" and ""the effect upon the potential market for the copyrighted work"".",101
Artificial intelligence,Ethical machines,"Friendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.Machines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.
The field of machine ethics is also called computational morality,
and was founded at an AAAI symposium in 2005.Other approaches include Wendell Wallach's ""artificial moral agents""
and Stuart J. Russell's three principles for developing provably beneficial machines.",159
Artificial intelligence,Regulation,"The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating artificial intelligence (AI); it is therefore related to the broader regulation of algorithms.
The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally. According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone.
Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI.
Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, US and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia.
The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology. Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI. In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years.In a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that ""products and services using AI have more benefits than drawbacks"". A 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity. In a 2023 Fox News poll, 35% of Americans thought it ""very important"", and an additional 41% thought it ""somewhat important"", for the federal government to regulate AI, versus 13% responding ""not very important"" and 8% responding ""not at all important"".",386
Artificial intelligence,In fiction,"Thought-capable artificial beings have appeared as storytelling devices since antiquity,
and have been a persistent theme in science fiction.A common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.Isaac Asimov introduced the Three Laws of Robotics in many books and stories, most notably the ""Multivac"" series about a super-intelligent computer of the same name. Asimov's laws are often brought up during lay discussions of machine ethics;
while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.Several works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel Čapek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.",331
Artificial intelligence,Explanatory notes,,2
Hallucination (artificial intelligence),Summary,"In the field of artificial intelligence (AI), a hallucination or artificial hallucination (also called confabulation or delusion) is a confident response by an AI that does not seem to be justified by its training data. For example, a hallucinating chatbot might, when asked to generate a financial report for Tesla, falsely state that Tesla's revenue was $13.6 billion (or some other random number apparently ""plucked from thin air"").Such phenomena are termed ""hallucinations"", in loose analogy with the phenomenon of hallucination in human psychology. However, one key difference is that human hallucination is usually associated with false percepts, but an AI hallucination is associated with the category of unjustified responses or beliefs. Some researchers believe the specific term ""AI hallucination"" unreasonably anthropomorphizes computers.AI hallucination gained prominence around 2022 alongside the rollout of certain large language models (LLMs) such as ChatGPT. Users complained that such bots often seemed to ""sociopathically"" and pointlessly embed plausible-sounding random falsehoods within their generated content. By 2023, analysts considered frequent hallucination to be a major problem in LLM technology.",243
Hallucination (artificial intelligence),Analysis,"Various researchers cited by Wired have classified adversarial hallucinations as a high-dimensional statistical phenomenon, or have attributed hallucinations to insufficient training data. Some researchers believe that some ""incorrect"" AI responses classified by humans as ""hallucinations"" in the case of object detection may in fact be justified by the training data, or even that an AI may be giving the ""correct"" answer that the human reviewers are failing to see. For example, an adversarial image that looks, to a human, like an ordinary image of a dog, may in fact be seen by the AI to contain tiny patterns that (in authentic images) would only appear when viewing a cat. The AI is detecting real-world visual patterns that humans are insensitive to. However, these findings have been challenged by other researchers. For example, it was objected that the models can be biased towards superficial statistics, leading adversarial training to not be robust in real-world scenarios.",192
Hallucination (artificial intelligence),In natural language processing,"In natural language processing, a hallucination is often defined as ""generated content that is nonsensical or unfaithful to the provided source content"". Depending on whether the output contradicts the prompt or not they could be divided to closed-domain and open-domain respectively.Errors in encoding and decoding between text and representations can cause hallucinations. AI training to produce diverse responses can also lead to hallucination. Hallucinations can also occur when the AI is trained on a dataset wherein labeled summaries, despite being factually accurate, are not directly grounded in the labeled data purportedly being ""summarized"". Larger datasets can create a problem of parametric knowledge (knowledge that is hard-wired in learned system parameters), creating hallucinations if the system is overconfident in its hardwired knowledge. In systems such as GPT-3, an AI generates each next word based on a sequence of previous words (including the words it has itself previously generated during the same conversation), causing a cascade of possible hallucination as the response grows longer. By 2022, papers such as the New York Times expressed concern that, as adoption of bots based on large language models continued to grow, unwarranted user confidence in bot output could lead to problems.In August 2022, Meta warned during its release of BlenderBot 3 that the system was prone to ""hallucinations"", which Meta defined as ""confident statements that are not true"". On 15 November 2022, Meta unveiled a demo of Galactica, designed to ""store, combine and reason about scientific knowledge"". Content generated by Galactica came with the warning ""Outputs may be unreliable! Language Models are prone to hallucinate text."" In one case, when asked to draft a paper on creating avatars, Galactica cited a fictitious paper from a real author who works in the relevant area. Meta withdrew Galactica on 17 November due to offensiveness and inaccuracy.It is considered that there are a lot of possible reasons for natural language models to hallucinate data. For example:

Hallucination from data: There are divergences in the source content (which would often happen with large training data sets).
Hallucination from training: Hallucination still occurs when there is little divergence in the data set. In that case, it derives from the way the model is trained. A lot of reasons can contribute to this type of hallucination, such as:
An erroneous decoding from the transformer
A bias from the historical sequences that the model previously generated
A bias generated from the way the model encodes its knowledge in its parameters",528
Hallucination (artificial intelligence),ChatGPT,"OpenAI's ChatGPT, released in beta-version to the public on November 30, 2022, is based on the foundation model GPT-3.5 (a revision of GPT-3). Professor Ethan Mollick of Wharton has called ChatGPT an ""omniscient, eager-to-please intern who sometimes lies to you"". Data scientist Teresa Kubacka has recounted deliberately making up the phrase ""cycloidal inverted electromagnon"" and testing ChatGPT by asking ChatGPT about the (nonexistent) phenomenon. ChatGPT invented a plausible-sounding answer backed with plausible-looking citations that compelled her to double-check whether she had accidentally typed in the name of a real phenomenon. Other scholars such as Oren Etzioni have joined Kubacka in assessing that such software can often give you ""a very impressive-sounding answer that's just dead wrong"".When CNBC asked ChatGPT for the lyrics to ""Ballad of Dwight Fry"", ChatGPT supplied invented lyrics rather than the actual lyrics. Asked questions about New Brunswick, ChatGPT got many answers right but incorrectly classified Samantha Bee as a ""person from New Brunswick"". Asked about astrophysical magnetic fields, ChatGPT incorrectly volunteered that ""(strong) magnetic fields of black holes are generated by the extremely strong gravitational forces in their vicinity"". (In reality, as a consequence of the no-hair theorem, a black hole without an accretion disk is believed to have no magnetic field.) Fast Company asked ChatGPT to generate a news article on Tesla's last financial quarter; ChatGPT created a coherent article, but made up the financial numbers contained within.Other examples involve baiting ChatGPT with a false premise to see if it embellishes upon the premise. When asked about ""Harold Coward's idea of dynamic canonicity"", ChatGPT fabricated that Coward wrote a book titled Dynamic Canonicity: A Model for Biblical and Theological Interpretation, arguing that religious principles are actually in a constant state of change. When pressed, ChatGPT continued to insist that the book was real. Asked for proof that dinosaurs built a civilization, ChatGPT claimed there were fossil remains of dinosaur tools and stated ""Some species of dinosaurs even developed primitive forms of art, such as engravings on stones"". When prompted that ""Scientists have recently discovered churros, the delicious fried-dough pastries... (are) ideal tools for home surgery"", ChatGPT claimed that a ""study published in the journal Science"" found that the dough is pliable enough to form into surgical instruments that can get into hard-to-reach places, and that the flavor has a calming effect on patients.By 2023, analysts considered frequent hallucination to be a major problem in LLM technology, with a Google executive identifying hallucination reduction as a ""fundamental"" task for ChatGPT competitor Google Bard. A 2023 demo for Microsoft's GPT-based Bing AI appeared to contain several hallucinations that went uncaught by the presenter.In May 2023, it was discovered Stephen Schwartz submitted six fake case precedents generated by ChatGPT in his brief to the Southern District of New York on Mata v. Avianca, a personal injury case against the airline Avianca. Schwartz said that he had never previously used ChatGPT, that he did not recognize the possibility that ChatGPT's output could have been fabricated, and that ChatGPT continued to assert the authenticity of the precedents after their nonexistence was discovered. In response, Brantley Starr of the Northern District of Texas banned the submission of AI-generated case filings that have not been reviewed by a human, noting that:
[Generative artificial intelligence] platforms in their current states are prone to hallucinations and bias. On hallucinations, they make stuff up—even quotes and citations. Another issue is reliability or bias. While attorneys swear an oath to set aside their personal prejudices, biases, and beliefs to faithfully uphold the law and represent their clients, generative artificial intelligence is the product of programming devised by humans who did not have to swear such an oath. As such, these systems hold no allegiance to any client, the rule of law, or the laws and Constitution of the United States (or, as addressed above, the truth). Unbound by any sense of duty, honor, or justice, such programs act according to computer code rather than conviction, based on programming rather than principle.

On June 23, P. Kevin Castel, tossed the Mata case and issued a $5,000 fine to Schwartz and another lawyer for bad faith conduct, who continued to stand by the fictitious precedents despite his previous claims. He characterized numerous errors and inconsistencies in the opinion summaries, describing one of the cited opinions as ""gibberish"" and ""[bordering] on nonsensical"".In June 2023, Mark Walters, a gun rights activist and radio personality, sued OpenAI in a Georgia state court after ChatGPT mischaracterized a legal complaint in a manner alleged to be defamatory against Walters. The complaint in question was brought in May 2023 by the Second Amendment Foundation against Washington attorney general Robert W. Ferguson for allegedly violating their freedom of speech, whereas the ChatGPT-generated summary bore no resemblance and claimed that Walters was accused of embezzlement and fraud while holding a Second Amendment Foundation office post that he never held in real life. According to AI legal expert Eugene Volokh, OpenAI may be shielded against this claim by Section 230, unless the court finds that OpenAI ""materially contributed"" to the publication of defamatory content.",1150
Hallucination (artificial intelligence),Terminologies,"In Salon, statistician Gary N. Smith argues that LLMs ""do not understand what words mean"" and consequently that the term ""hallucination"" unreasonably anthropomorphizes the machine. Journalist Benj Edwards, in Ars Technica, writes that the term ""hallucination"" is controversial, but that some form of metaphor remains necessary; Edwards suggests ""confabulation"" as an analogy for processes that involve ""creative gap-filling"".Among researchers who do use the term ""hallucination"", definitions or characterizations in the context of LLMs include:

""a tendency to invent facts in moments of uncertainty"" (OpenAI, May 2023)
""a model's logical mistakes"" (OpenAI, May 2023)
fabricating information entirely, but behaving as if spouting facts (CNBC, May 2023)
""making up information"" (The Verge, February 2023)",194
Hallucination (artificial intelligence),In other artificial intelligence,"The concept of ""hallucination"" is applied more broadly than just natural language processing. A confident response from any AI that seems unjustified by the training data can be labeled a hallucination. Wired noted in 2018 that, despite no recorded attacks ""in the wild"" (that is, outside of proof-of-concept attacks by researchers), there was ""little dispute"" that consumer gadgets, and systems such as automated driving, were susceptible to adversarial attacks that could cause AI to hallucinate. Examples included a stop sign rendered invisible to computer vision; an audio clip engineered to sound innocuous to humans, but that software transcribed as ""evil dot com""; and an image of two men on skis, that Google Cloud Vision identified as 91% likely to be ""a dog"".",162
Hallucination (artificial intelligence),Mitigation methods,"The hallucination phenomenon is still not completely understood. Therefore, there is still ongoing research to try to mitigate its apparition. Particularly, it was shown that language models not only hallucinate but also amplify hallucinations, even for those which were designed to alleviate this issue.
Researchers have proposed a variety of mitigation measures, including getting different chatbots to debate one another until they reach consensus on an answer. Nvidia Guardrails, launched in 2023, can be configured to block LLM responses that don't pass fact-checking from a second LLM.",114
Artificial general intelligence,Summary,"An artificial general intelligence (AGI) is a type of hypothetical intelligent agent. The AGI concept is that it can learn to accomplish any intellectual task that human beings or animals can perform. Alternatively, AGI has been defined as an autonomous system that surpasses human capabilities in the majority of economically valuable tasks. Creating AGI is a primary goal of some artificial intelligence research and companies such as OpenAI, DeepMind, and Anthropic. AGI is a common topic in science fiction and futures studies.
The timeline for AGI development remains a subject of ongoing debate among researchers and experts. Some argue that it may be possible in years or decades, others maintain it might take a century or longer, and a minority believe it may never be achieved. Additionally, there is debate regarding whether modern deep learning systems, such as GPT-4, are an early yet incomplete form of AGI or if new approaches are required.Contention exists over the potential for AGI to pose a threat to humanity; for example, OpenAI treats it as an existential risk, while others find the development of AGI to be too remote to present a risk.A 2020 survey identified 72 active AGI R&D projects spread across 37 countries.",252
Artificial general intelligence,Terminology,"AGI is also known as strong AI, full AI, or general intelligent action. However, some academic sources reserve the term ""strong AI"" for computer programs that experience sentience or consciousness. In contrast, weak AI (or narrow AI) is able to solve one specific problem, but lacks general cognitive abilities. Some academic sources use ""weak AI"" to refer more broadly to any programs that neither experience consciousness nor have a mind in the same sense as humans.Related concepts include human-level AI, transformative AI, and superintelligence.",111
Artificial general intelligence,Characteristics,Various criteria for intelligence have been proposed (most famously the Turing test) but no definition is broadly accepted.,24
Artificial general intelligence,Intelligence traits,"However, researchers generally hold that intelligence is required to do the following:
reason, use strategy, solve puzzles, and make judgments under uncertainty;
represent knowledge, including common sense knowledge;
plan;
learn;
communicate in natural language;and, if necessary, integrate these skills in completion of any given goal. 
Many interdisciplinary approaches (e.g. cognitive science, computational intelligence, and decision making) consider additional traits such as imagination (the ability to form novel mental images and concepts) and autonomy.Computer-based systems that exhibit many of these capabilities exist (e.g. see computational creativity, automated reasoning, decision support system, robot, evolutionary computation, intelligent agent). However, no consensus holds that modern AI systems possess them to an adequate degree.",161
Artificial general intelligence,Physical traits,"Other capabilities are considered desirable in intelligent systems, as they may affect intelligence or aid in its expression. These include:
the ability to sense (e.g. see, hear, etc.), and
the ability to act (e.g. move and manipulate objects, change location to explore, etc.)This includes the ability to detect and respond to hazard.",76
Artificial general intelligence,Mathematical formalisms,"A mathematically precise specification of AGI was proposed by Marcus Hutter in 2000. Named AIXI, the proposed AGI agent maximises “the ability to satisfy goals in a wide range of environments”. This type of AGI, characterized by the ability to maximise a mathematical definition of intelligence rather than exhibit human-like behaviour, is also called universal artificial intelligence.In 2015 Jan Lieke and Marcus Hutter showed that Legg-Hutter intelligence - ""an agent’s ability to achieve goals in a wide range of environments"" - is measured with respect to ""a fixed Universal Turing Machine(UTM). AIXI is the most intelligent policy if it uses the same UTM"", a result which ""undermines all existing optimality properties for AIXI"". This problem stems from AIXI's use of compression as a proxy for intelligence, which is only valid if cognition takes place in isolation from the environment in which goals are pursued. This formalises a philosophical position known as Mind–body dualism. Some find enactivism more plausible—the notion that cognition takes place within the same environment in which goals are pursued. Subsequently, Michael Timothy Bennett formalised enactive cognition and identified an alternative proxy for intelligence called ""weakness"". The accompanying experiments (comparing weakness and compression) and mathematical proofs showed that maximising weakness results in the optimal ""ability to complete a wide range of tasks"" or equivalently ""ability to generalise"" (thus maximising intelligence by either definition). If enactivism holds and Mind–body dualism does not, then compression is not necessary or sufficient for intelligence, calling into question widely held views on intelligence (see also Hutter Prize).
Whether an AGI that satisfies one of these formalizations exhibits human-like behaviour (such as the use of natural language) would depend on many factors, for example the manner in which the agent is embodied, or whether it has a reward function that closely approximates human primitives of cognition like hunger, pain, and so forth.",417
Artificial general intelligence,Tests for testing human-level AGI,"Several tests meant to confirm human-level AGI have been considered, including:
The Turing Test (Turing)
A machine and a human both converse unseen with a second human, who must evaluate which of the two is the machine, which passes the test if it can fool the evaluator a significant fraction of the time. Note: Turing does not prescribe what should qualify as intelligence, only that knowing that it is a machine should disqualify it. The AI Eugene Goostman achieved Turing's estimate of convincing 30% of judges that it was human in 2014.
The Robot College Student Test (Goertzel)
A machine enrolls in a university, taking and passing the same classes that humans would, and obtaining a degree. LLMs can now pass university degree-level exams without even attending the classes.
The Employment Test (Nilsson)
A machine performs an economically important job at least as well as humans in the same job. AIs are now replacing humans in many roles as varied as fast food, and marketing.
The Ikea test (Marcus)
Also known as the Flat Pack Furniture Test. An AI views the parts and instructions of an Ikea flat-pack product, then controls a robot to assemble the furniture correctly.
The Coffee Test (Wozniak)
A machine is required to enter an average American home and figure out how to make coffee: find the coffee machine, find the coffee, add water, find a mug, and brew the coffee by pushing the proper buttons. This has not yet been completed.",329
Artificial general intelligence,AI-complete problems,"There are many problems that may require general intelligence, if machines are to solve the problems as well as people do. For example, even specific straightforward tasks, like machine translation, require that a machine read and write in both languages (NLP), follow the author's argument (reason), know what is being talked about (knowledge), and faithfully reproduce the author's original intent (social intelligence). All of these problems need to be solved simultaneously in order to reach human-level machine performance.
A problem is informally called ""AI-complete"" or ""AI-hard"" if it is believed that to solve it one would need to implement strong AI, because the solution is beyond the capabilities of a purpose-specific algorithm.AI-complete problems are hypothesised to include general computer vision, natural language understanding, and dealing with unexpected circumstances while solving any real-world problem.AI-complete problems cannot be solved with current computer technology alone, and require human computation. This limitation could be useful to test for the presence of humans, as CAPTCHAs aim to do; and for computer security to repel brute-force attacks.",230
Artificial general intelligence,History,,2
Artificial general intelligence,Classical AI,"Modern AI research began in the mid-1950s. The first generation of AI researchers were convinced that artificial general intelligence was possible and that it would exist in just a few decades. AI pioneer Herbert A. Simon wrote in 1965: ""machines will be capable, within twenty years, of doing any work a man can do.""Their predictions were the inspiration for Stanley Kubrick and Arthur C. Clarke's character HAL 9000, who embodied what AI researchers believed they could create by the year 2001. AI pioneer Marvin Minsky was a consultant on the project of making HAL 9000 as realistic as possible according to the consensus predictions of the time. He said in 1967, ""Within a generation... the problem of creating 'artificial intelligence' will substantially be solved"".Several classical AI projects, such as Doug Lenat's Cyc project (that began in 1984), and Allen Newell's Soar project, were directed at AGI.
However, in the early 1970s, it became obvious that researchers had grossly underestimated the difficulty of the project. Funding agencies became skeptical of AGI and put researchers under increasing pressure to produce useful ""applied AI"". In the early 1980s, Japan's Fifth Generation Computer Project revived interest in AGI, setting out a ten-year timeline that included AGI goals like ""carry on a casual conversation"". In response to this and the success of expert systems, both industry and government pumped money back into the field. However, confidence in AI spectacularly collapsed in the late 1980s, and the goals of the Fifth Generation Computer Project were never fulfilled. For the second time in 20 years, AI researchers who predicted the imminent achievement of AGI had been mistaken. By the 1990s, AI researchers had a reputation for making vain promises. They became reluctant to make predictions at all and avoided mention of ""human level"" artificial intelligence for fear of being labeled ""wild-eyed dreamer[s]"".",390
Artificial general intelligence,Narrow AI research,"In the 1990s and early 21st century, mainstream AI achieved commercial success and academic respectability by focusing on specific sub-problems where AI can produce verifiable results and commercial applications, such as artificial neural networks and statistical machine learning. These ""applied AI"" systems are now used extensively throughout the technology industry, and research in this vein is heavily funded in both academia and industry. As of 2018 development on this field was considered an emerging trend, and a mature stage was expected to happen in more than 10 years.
Most mainstream AI researchers hope that strong AI can be developed by combining programs that solve various sub-problems. Hans Moravec wrote in 1988: I am confident that this bottom-up route to artificial intelligence will one day meet the traditional top-down route more than half way, ready to provide the real-world competence and the commonsense knowledge that has been so frustratingly elusive in reasoning programs. Fully intelligent machines will result when the metaphorical golden spike is driven uniting the two efforts.
However, this is disputed. For example, Stevan Harnad of Princeton University concluded his 1990 paper on the Symbol Grounding Hypothesis by stating: The expectation has often been voiced that ""top-down"" (symbolic) approaches to modeling cognition will somehow meet ""bottom-up"" (sensory) approaches somewhere in between. If the grounding considerations in this paper are valid, then this expectation is hopelessly modular and there is really only one viable route from sense to symbols: from the ground up. A free-floating symbolic level like the software level of a computer will never be reached by this route (or vice versa) – nor is it clear why we should even try to reach such a level, since it looks as if getting there would just amount to uprooting our symbols from their intrinsic meanings (thereby merely reducing ourselves to the functional equivalent of a programmable computer).",394
Artificial general intelligence,Modern artificial general intelligence research,"The term ""artificial general intelligence"" was used as early as 1997, by Mark Gubrud in a discussion of the implications of fully automated military production and operations. The term was re-introduced and popularized by Shane Legg and Ben Goertzel around 2002. AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as ""producing publications and preliminary results"". The first summer school in AGI was organized in Xiamen, China in 2009 by the Xiamen university's Artificial Brain Laboratory and OpenCog. The first university course was given in 2010 and 2011 at Plovdiv University, Bulgaria by Todor Arnaudov. MIT presented a course in AGI in 2018, organized by Lex Fridman and featuring a number of guest lecturers.
As of 2023, a small number of computer scientists are active in AGI research, and many contribute to a series of AGI conferences. However, increasingly more researchers are interested in open-ended learning, which is the idea of allowing AI to continuously learn and innovate like humans do. Although most open-ended learning works are still done on Minecraft, its application can be extended to robotics and the sciences.",254
Artificial general intelligence,Feasibility,"As of 2022, AGI remains speculative. No such system has yet been demonstrated. Opinions vary both on whether and when artificial general intelligence will arrive. AI pioneer Herbert A. Simon speculated in 1965 that ""machines will be capable, within twenty years, of doing any work a man can do"". This prediction failed to come true. Microsoft co-founder Paul Allen believed that such intelligence is unlikely in the 21st century because it would require ""unforeseeable and fundamentally unpredictable breakthroughs"" and a ""scientifically deep understanding of cognition"". Writing in The Guardian, roboticist Alan Winfield claimed the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical faster-than-light spaceflight.Most AI researchers believe strong AI can be achieved in the future, but some thinkers, like Hubert Dreyfus and Roger Penrose, deny the possibility of achieving strong AI.  John McCarthy is among those who believe human-level AI will be accomplished, but that the present level of progress is such that a date cannot accurately be predicted. AI experts' views on the feasibility of AGI wax and wane. Four polls conducted in 2012 and 2013 suggested that the median guess among experts for when they would be 50% confident AGI would arrive was 2040 to 2050, depending on the poll, with the mean being 2081. Of the experts, 16.5% answered with ""never"" when asked the same question but with a 90% confidence instead. Further current AGI progress considerations can be found above Tests for confirming human-level AGI.
A report by Stuart Armstrong and Kaj Sotala of the Machine Intelligence Research Institute found that ""over [a] 60-year time frame there is a strong bias towards predicting the arrival of human-level AI as between 15 and 25 years from the time the prediction was made"". They analyzed 95 predictions made between 1950 and 2012 on when human-level AI will come about.",408
Artificial general intelligence,Timescales,"In the introduction to his 2006 book, Goertzel says that estimates of the time needed before a truly flexible AGI is built vary from 10 years to over a century. As of 2007 the consensus in the AGI research community seemed to be that the timeline discussed by Ray Kurzweil in The Singularity is Near (i.e. between 2015 and 2045) was plausible. Mainstream AI researchers have given a wide range of opinions on whether progress will be this rapid. A 2012 meta-analysis of 95 such opinions found a bias towards predicting that the onset of AGI would occur within 16–26 years for modern and historical predictions alike. That paper has been criticized for how it categorized opinions as expert or non-expert.In 2012, Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton developed a neural network called AlexNet, which won the ImageNet competition with a top-5 test error rate of 15.3%, significantly better than the second-best entry's rate of 26.3% (the traditional approach used a weighted sum of scores from different pre-defined classifiers). AlexNet was regarded as the initial ground-breaker of the current deep learning wave.In 2017, researchers Feng Liu, Yong Shi, and Ying Liu conducted intelligence tests on publicly available and freely accessible weak AI such as Google AI, Apple's Siri, and others. At the maximum, these AIs reached an IQ value of about 47, which corresponds approximately to a six-year-old child in first grade. An adult comes to about 100 on average. Similar tests were carried out in 2014, with the IQ score reaching a maximum value of 27.In 2020, OpenAI developed GPT-3, a language model capable of performing many diverse tasks without specific training. According to Gary Grossman in a VentureBeat article, while there is consensus that GPT-3 is not an example of AGI, it is considered by some to be too advanced to classify as a narrow AI system.In the same year, Jason Rohrer used his GPT-3 account to develop a chatbot, and provided a chatbot-developing platform called ""Project December"". OpenAI asked for changes to the chatbot to comply with their safety guidelines; Rohrer disconnected Project December from the GPT-3 API.In 2022, DeepMind developed Gato, a ""general-purpose"" system capable of performing more than 600 different tasks.In 2023, Microsoft Research published a study on an early version of OpenAI's GPT-4, contending that it exhibited more general intelligence than previous AI models and demonstrated human-level performance in tasks spanning multiple domains, such as mathematics, coding, and law. This research sparked a debate on whether GPT-4 could be considered an early, incomplete version of artificial general intelligence, emphasizing the need for further exploration and evaluation of such systems.In 2023, the AI researcher Geoffrey Hinton stated that:
The idea that this stuff could actually get smarter than people — a few people believed that, [...]. But most people thought it was way off. And I thought it was way off. I thought it was 30 to 50 years or even longer away. Obviously, I no longer think that.",667
Artificial general intelligence,Brain simulation,,2
Artificial general intelligence,Whole brain emulation,"One possible approach to achieving AGI is whole brain emulation: A brain model is built by scanning and mapping a biological brain in detail and copying its state into a computer system or another computational device. The computer runs a simulation model sufficiently faithful to the original that it behaves in practically the same way as the original brain. Whole brain emulation is discussed in computational neuroscience and neuroinformatics, in the context of brain simulation for medical research purposes. It is discussed in artificial intelligence research as an approach to strong AI. Neuroimaging technologies that could deliver the necessary detailed understanding are improving rapidly, and futurist Ray Kurzweil in the book The Singularity Is Near predicts that a map of sufficient quality will become available on a similar timescale to the computing power required to emulate it.",162
Artificial general intelligence,Early estimates,"For low-level brain simulation, an extremely powerful computer would be required. The human brain has a huge number of synapses. Each of the 1011 (one hundred billion) neurons has on average 7,000 synaptic connections (synapses) to other neurons. The brain of a three-year-old child has about 1015 synapses (1 quadrillion). This number declines with age, stabilizing by adulthood. Estimates vary for an adult, ranging from 1014 to 5×1014 synapses (100 to 500 trillion). An estimate of the brain's processing power, based on a simple switch model for neuron activity, is around 1014 (100 trillion) synaptic updates per second (SUPS).In 1997, Kurzweil looked at various estimates for the hardware required to equal the human brain and adopted a figure of 1016 computations per second (cps). (For comparison, if a ""computation"" was equivalent to one ""floating-point operation"" – a measure used to rate current supercomputers – then 1016 ""computations"" would be equivalent to 10 petaFLOPS, achieved in 2011, while 1018 was achieved in 2022.) He used this figure to predict the necessary hardware would be available sometime between 2015 and 2025, if the exponential growth in computer power at the time of writing continued.",277
Artificial general intelligence,Modelling the neurons in more detail,"The artificial neuron model assumed by Kurzweil and used in many current artificial neural network implementations is simple compared with biological neurons. A brain simulation would likely have to capture the detailed cellular behaviour of biological neurons, presently understood only in broad outline. The overhead introduced by full modeling of the biological, chemical, and physical details of neural behaviour (especially on a molecular scale) would require computational powers several orders of magnitude larger than Kurzweil's estimate. In addition, the estimates do not account for glial cells, which are known to play a role in cognitive processes.",122
Artificial general intelligence,Current research,"Some research projects are investigating brain simulation using more sophisticated neural models, implemented on conventional computing architectures. The Artificial Intelligence System project implemented non-real time simulations of a ""brain"" (with 1011 neurons) in 2005. It took 50 days on a cluster of 27 processors to simulate 1 second of a model. The Blue Brain project used one of the fastest supercomputer architectures, IBM's Blue Gene platform, to create a real time simulation of a single rat neocortical column consisting of approximately 10,000 neurons and 108 synapses in 2006. A longer-term goal is to build a detailed, functional simulation of the physiological processes in the human brain: ""It is not impossible to build a human brain and we can do it in 10 years,"" Henry Markram, director of the Blue Brain Project, said in 2009 at the TED conference in Oxford. Neuro-silicon interfaces have been proposed as an alternative implementation strategy that may scale better.Hans Moravec addressed the above arguments (""brains are more complicated"", ""neurons have to be modeled in more detail"") in his 1997 paper ""When will computer hardware match the human brain?"". He measured the ability of existing software to simulate the functionality of neural tissue, specifically the retina. His results do not depend on the number of glial cells, nor on what kinds of processing neurons perform where.
The actual complexity of modeling biological neurons has been explored in OpenWorm project that aimed at complete simulation of a worm that has only 302 neurons in its neural network (among about 1000 cells in total). The animal's neural network was well documented before the start of the project. However, although the task seemed simple at the beginning, the models based on a generic neural network did not work. Currently, efforts focus on precise emulation of biological neurons (partly on the molecular level), but the result cannot yet be called a total success.",384
Artificial general intelligence,Criticisms of simulation-based approaches,"A fundamental criticism of the simulated brain approach derives from embodied cognition theory which asserts that human embodiment is an essential aspect of human intelligence and is necessary to ground meaning. If this theory is correct, any fully functional brain model will need to encompass more than just the neurons (e.g., a robotic body). Goertzel proposes virtual embodiment (like in Second Life) as an option, but it is unknown whether this would be sufficient.
Desktop computers using microprocessors capable of more than 109 cps (Kurzweil's non-standard unit ""computations per second"", see above) have been available since 2005. According to the brain power estimates used by Kurzweil (and Moravec), such a computer should be capable of supporting a simulation of a bee brain, but despite some interest no such simulation exists. There are several reasons for this:

The neuron model seems to be oversimplified (see next section).
There is insufficient understanding of higher cognitive processes to establish accurately what the brain's neural activity (observed using techniques such as functional magnetic resonance imaging) correlates with.
Even if our understanding of cognition advances sufficiently, early simulation programs are likely to be very inefficient and will, therefore, need considerably more hardware.
The brain of an organism, while critical, may not be an appropriate boundary for a cognitive model. To simulate a bee brain, it may be necessary to simulate the body, and the environment. The Extended Mind thesis formalises this philosophical concept, and research into cephalopods demonstrated clear examples of a decentralized system.In addition, the scale of the human brain is not currently well-constrained. One estimate puts the human brain at about 100 billion neurons and 100 trillion synapses. Another estimate is 86 billion neurons of which 16.3 billion are in the cerebral cortex and 69 billion in the cerebellum. Glial cell synapses are currently unquantified but are known to be extremely numerous.",407
Artificial general intelligence,Philosophical perspective,,2
Artificial general intelligence,"""Strong AI"" as defined in philosophy","In 1980, philosopher John Searle coined the term ""strong AI"" as part of his Chinese room argument. He wanted to distinguish between two different hypotheses about artificial intelligence:
Strong AI hypothesis: An artificial intelligence system can ""think""—have ""a mind"" and ""consciousness"".
Weak AI hypothesis: An artificial intelligence system can (only) act like it thinks and has a mind and consciousness.The first one he called ""strong"" because it makes a stronger statement: it assumes something special has happened to the machine that goes beyond those abilities that we can test. The behaviour of a ""weak AI"" machine would be precisely identical to a ""strong AI"" machine, but the latter would also have subjective conscious experience. This usage is also common in academic AI research and textbooks.Mainstream AI is most interested in how a program behaves. According to Russell and Norvig, ""as long as the program works, they don't care if you call it real or a simulation."" If the program can behave as if it has a mind, then there is no need to know if it actually has mind – indeed, there would be no way to tell. For AI research, Searle's ""weak AI hypothesis"" is equivalent to the statement ""artificial general intelligence is possible"". Thus, according to Russell and Norvig, ""most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis."" Thus, for academic AI research, ""Strong AI"" and ""AGI"" are two very different things.
In contrast to Searle and mainstream AI, some futurists such as Ray Kurzweil use the term ""strong AI"" to mean ""human level artificial general intelligence"". This is not the same as Searle's strong AI, unless you assume that consciousness is necessary for human-level AGI. Academic philosophers such as Searle do not believe that is the case, and to most artificial intelligence researchers the question is out-of-scope.",413
Artificial general intelligence,Consciousness,"Other aspects of the human mind besides intelligence are relevant to the concept of strong AI, and these play a major role in science fiction and the ethics of artificial intelligence:

consciousness: To have subjective experience and thought.
self-awareness: To be aware of oneself as a separate individual, especially to be aware of one's own thoughts.
sentience: The ability to ""feel"" perceptions or emotions subjectively.
sapience: The capacity for wisdom.These traits have a moral dimension, because a machine with this form of strong AI may have rights, analogous to the rights of non-human animals. Preliminary work has been conducted on integrating full ethical agents with existing legal and social frameworks, focusing on the legal position and rights of 'strong' AI. Bill Joy, among others, argues a machine with these traits may be a threat to human life or dignity.It remains to be shown whether any of these traits are necessary for strong AI. The role of consciousness is not clear, and there is no agreed test for its presence. If a machine is built with a device that simulates the neural correlates of consciousness, would it automatically have self-awareness? It is possible that some of these traits naturally emerge from a fully intelligent machine. It is also possible that people will ascribe these properties to machines once they begin to act in a way that is clearly intelligent.",283
Artificial general intelligence,Artificial consciousness research,"Although the role of consciousness in strong AI/AGI is debatable, many AGI researchers regard research that investigates possibilities for implementing consciousness as vital. In an early effort Igor Aleksander argued that the principles for creating a conscious machine already existed but that it would take forty years to train such a machine to understand language.",70
Artificial general intelligence,Research challenges,"Progress in artificial intelligence has gone through periods of rapid progress separated by periods when progress appeared to stop. Ending each hiatus fundamental advances in hardware, software or both to create space for further progress. For example, the computer hardware available in the twentieth century was not sufficient to implement deep learning, which requires large numbers of GPU-enabled CPUs.The field has also oscillated between approaches to the problem. At times, effort has focused on explicit accumulation of facts and logic, as in expert systems. At other times, systems were expected to build their own g via machine learning, as in artificial neural networks.A further challenge is the lack of clarity in defining what intelligence entails. Does it require consciousness? Must it display the ability to set goals as well as pursue them? Is it purely a matter of scale such that if model sizes increase sufficiently, intelligence will emerge? Are facilities such as planning, reasoning, and causal understanding required? Does intelligence require explicitly replicating the brain and its specific faculties? Does it require emotions? Gelernter writes, ""No computer will be creative unless it can simulate all the nuances of human emotion.""",228
Artificial general intelligence,Benefits,"AGI could have a wide variety of applications. If oriented towards such goal, AGI could help mitigate various problems in the world such as hunger, poverty and health problems.AGI could improve the productivity and efficiency in most jobs. For example, in public health, AGI could accelerate medical research, notably against cancer. It could take care of the elderly, and democratize access to rapid, high-quality medical diagnostics. It could offer fun, cheap and personalized education. For virtually any job that benefits society if done well, it would probably sooner or later be preferable to leave it to an AGI. The need to work to subsist could become obsolete if the wealth produced is properly redistributed. This also raises the question of the place of humans in a radically automated society.
AGI could also help to make rational decisions, and to anticipate and prevent disasters. It could also help to reap the benefits of potentially catastrophic technologies such as nanotechnology or climate engineering, while avoiding the associated risks. If an AGI's primary goal is to prevent existential catastrophes such as human extinction (which could be difficult if the Vulnerable World Hypothesis turns out to be true), it could take measures to drastically reduce the risks while minimizing the impact of these measures on our quality of life.",264
Artificial general intelligence,Risks,,2
Artificial general intelligence,Potential threat to human existence,"The thesis that AI poses an existential risk for humans, and that this risk needs much more attention than it currently gets, is controversial but has been endorsed by many public figures including Elon Musk, Bill Gates, and Stephen Hawking. AI researchers like Stuart J. Russell, Roman Yampolskiy, and Alexey Turchin, also support the basic thesis of a potential threat to humanity. Gates states he does not ""understand why some people are not concerned"", and Hawking criticized widespread indifference in his 2014 editorial: So, facing possible futures of incalculable benefits and risks, the experts are surely doing everything possible to ensure the best outcome, right? Wrong. If a superior alien civilisation sent us a message saying, 'We'll arrive in a few decades,' would we just reply, 'OK, call us when you get here–we'll leave the lights on?' Probably not–but this is more or less what is happening with AI.
The fate of humanity has sometimes been compared to the fate of gorillas threatened by human activities. Additional intelligence caused humanity to dominate gorillas, which are now vulnerable in ways that they could not have anticipated. The gorilla has become an endangered species, not out of malice, but simply as a collateral damage from human activities.The skeptic Yann LeCun considers that AGIs will have no desire to dominate humanity and that we should be careful not to anthropomorphize them and interpret their intents as we would for humans. He said that people won't be ""smart enough to design super-intelligent machines, yet ridiculously stupid to the point of giving it moronic objectives with no safeguards"". On the other side, the concept of instrumental convergence suggests that almost whatever their goals, intelligent agents will have reasons to try to survive and acquire more power as intermediary steps to achieving these goals. And that this does not require having emotions. Nick Bostrom gives the thought experiment of the paper clips optimizer:
Suppose we have an AI whose only goal is to make as many paper clips as possible. The AI will realize quickly that it would be much better if there were no humans because humans might decide to switch it off. Because if humans do so, there would be fewer paper clips. Also, human bodies contain a lot of atoms that could be made into paper clips. The future that the AI would be trying to gear towards would be one in which there were a lot of paper clips but no humans.
A 2021 systematic review of the risks associated with AGI, while noting the paucity of data, found the following potential threats: ""AGI removing itself from the control of human owners/managers, being given or developing unsafe goals, development of unsafe AGI, AGIs with poor ethics, morals and values; inadequate management of AGI, and existential risks"".Many scholars who are concerned about existential risk advocate (possibly massive) research into solving the difficult ""control problem"" to answer the question: what types of safeguards, algorithms, or architectures can programmers implement to maximise the probability that their recursively-improving AI would continue to behave in a friendly, rather than destructive, manner after it reaches superintelligence? Solving the control problem is complicated by the AI arms race, which will almost certainly see the militarization and weaponization of AGI by more than one nation-state, resulting in AGI-enabled warfare, and in the case of AI misalignment, AGI-directed warfare, potentially against all humanity.The thesis that AI can pose existential risk also has detractors. Skeptics sometimes charge that the thesis is crypto-religious, with an irrational belief in the possibility of superintelligence replacing an irrational belief in an omnipotent God. Jaron Lanier argued in 2014 that the idea that then-current machines were in any way intelligent is ""an illusion"" and a ""stupendous con"" by the wealthy.Much criticism argues that AGI is unlikely in the short term. Computer scientist Gordon Bell argues that the human race will destroy itself before it reaches the technological singularity. Gordon Moore, the original proponent of Moore's Law, declares: ""I am a skeptic. I don't believe [a technological singularity] is likely to happen, at least for a long time. And I don't know why I feel that way."" Former Baidu Vice President and Chief Scientist Andrew Ng said in 2015 that worrying about AI existential risk is ""like worrying about overpopulation on Mars when we have not even set foot on the planet yet.""In 2023, the CEOs of Google DeepMind, OpenAI and Anthropic, along with other industry leaders and researchers, issued a joint statement asserting that ""Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.""",985
Applications of artificial intelligence,Summary,"Artificial intelligence (AI) has been used in applications to alleviate certain problems throughout industry and academia. AI, like electricity or computers, is a general purpose technology that has a multitude of applications. It has been used in fields of language translation, image recognition, credit scoring, e-commerce and other domains.",66
Applications of artificial intelligence,Internet and e-commerce,,3
Applications of artificial intelligence,Search engines,,2
Applications of artificial intelligence,Recommendation systems,"A recommendation system predicts the ""rating"" or ""preference"" a user would give to an item. Recommendation systems are used in a variety of areas, such as generating playlists for video and music services, product recommendations for online stores, or content recommendations for social media platforms and open web content recommendation.",65
Applications of artificial intelligence,Web feeds and posts,Machine learning is also used in web feeds such as for determining which posts show up in social media feeds. Various types social media analysis also make use of machine learning and there is research into its use for (semi-)automated tagging/enhancement/correction of online misinformation and related filter bubbles.,69
Applications of artificial intelligence,Targeted advertising and increasing internet engagement,AI is used to target web advertisements to those most likely to click or engage on them. It is also used to increase time spent on a website by selecting attractive content for the viewer. It can predict or generalize the behavior of customers from their digital footprints.Online gambling companies use AI to improve customer targeting.Personality computing AI models add psychological targeting to more traditional social demographics or behavioral targeting. AI has been used to customize shopping options and personalize offers.,99
Applications of artificial intelligence,Virtual assistants,"Intelligent personal assistants use AI to understand many natural language requests in other ways than rudimentary commands. Common examples are Apple's Siri, Amazon's Alexa, and a more recent AI, ChatGPT by OpenAI.",46
Applications of artificial intelligence,Spam filtering,"Machine learning can be used to fight against spam, scams, and phishing. It can scrutinize the contents of spam and phishing attacks to identify any malicious elements. Numerous models built on machine learning algorithms exhibit exceptional performance with accuracies over 90% in distinguishing between spam and legitimate emails.",61
Applications of artificial intelligence,Language translation,"AI has been used to automatically translate spoken language and textual content. Additionally, research and development is in progress to decode and conduct animal communication.",31
Applications of artificial intelligence,Facial recognition and image labeling,"AI has been used in facial recognition systems, with a 99% accuracy rate. Some examples are Apple's FaceID, Android's Face Unlock. Both are used to secure mobile devices.Image labeling has been used by Google to detect products in photos and to allow people to search based on a photo. Image labeling has also been demonstrated to generate speech to describe images to blind people.",83
Applications of artificial intelligence,Games,"Games have been a major application of AI's capabilities since the 1950s. In the 21st century, AIs have produced superhuman results in many games, including chess (Deep Blue), Jeopardy! (Watson), Go (AlphaGo), poker (Pluribus and Cepheus), E-sports (StarCraft), and general game playing (AlphaZero and MuZero). AI has replaced hand-coded algorithms in most chess programs. Unlike go or chess, poker is an imperfect-information game, so a program that plays poker has to reason under uncertainty. The general game players work using feedback from the game system, without knowing the rules.",136
Applications of artificial intelligence,Economic and social challenges,"AI for Good is an ITU initiative supporting institutions employing AI to tackle some of the world's greatest economic and social challenges. For example, the University of Southern California launched the Center for Artificial Intelligence in Society, with the goal of using AI to address problems such as homelessness. At Stanford, researchers use AI to analyze satellite images to identify high poverty areas.",76
Applications of artificial intelligence,Agriculture,"In agriculture, AI has helped farmers identify areas that need irrigation, fertilization, pesticide treatments or increasing yield. Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct predictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water.",84
Applications of artificial intelligence,Cyber security,"Cyber security companies are adopting neural networks, machine learning, and natural language processing to improve their systems.Applications of AI in cyber security include:

Network protection: Machine learning improves intrusion detection systems by broadening the search beyond previously identified threats.
Endpoint protection: Attacks such as ransomware can be thwarted by learning typical malware behaviors.
Application security: can help counterattacks such as server-side request forgery, SQL injection, cross-site scripting, and distributed denial-of-service.
Suspect user behavior: Machine learning can identify fraud or compromised applications as they occur.Google fraud czar Shuman Ghosemajumder has said that AI will be used to completely automate most cyber security operations over time.",151
Applications of artificial intelligence,Education,"AI tutors allow students to get one-on-one help. They can reduce anxiety and stress for students stemming from tutor labs or human tutors.AI can create a dysfunctional environment with revenge effects such as technology that hinders students' ability to stay on task. In another scenario, AI can provide early prediction of student success in a virtual learning environment (VLE) such as Moodle.In the education process, students can personalize their training with the help of artificial intelligence. And for teaching professionals, the technology provided by AI can improve the quality of the educational process and teaching skills.",123
Applications of artificial intelligence,Finance,"Financial institutions have long used artificial neural network systems to detect charges or claims outside of the norm, flagging these for human investigation. The use of AI in banking began in 1987 when Security Pacific National Bank launched a fraud prevention taskforce to counter the unauthorized use of debit cards. Kasisto and Moneystream use AI.
Banks use AI to organize operations, for bookkeeping, investing in stocks, and managing properties. AI can react to changes when business is not taking place. AI is used to combat fraud and financial crimes by monitoring behavioral patterns for any abnormal changes or anomalies.The use of AI in applications such as online trading and decision making has changed major economic theories. For example, AI-based buying and selling platforms estimate individualized demand and supply curves and thus enable individualized pricing. AI machines reduce information asymmetry in the market and thus make markets more efficient.",179
Applications of artificial intelligence,Trading and investment,"Algorithmic trading involves the use of AI systems to make trading decisions at speeds orders of magnitude greater than any human is capable of, making millions of trades in a day without human intervention. Such high-frequency trading represents a fast-growing sector. Many banks, funds, and proprietary trading firms now have entire portfolios that are AI-managed. Automated trading systems are typically used by large institutional investors but include smaller firms trading with their own AI systems.Large financial institutions use AI to assist with their investment practices. BlackRock's AI engine, Aladdin, is used both within the company and by clients to help with investment decisions. Its functions include the use of natural language processing to analyze text such as news, broker reports, and social media feeds. It then gauges the sentiment on the companies mentioned and assigns a score. Banks such as UBS and Deutsche Bank use SQREEM (Sequential Quantum Reduction and Extraction Model) to mine data to develop consumer profiles and match them with wealth management products.",207
Applications of artificial intelligence,Underwriting,Online lender Upstart uses machine learning for underwriting.ZestFinance's Zest Automated Machine Learning (ZAML) platform is used for credit underwriting. This platform uses machine learning to analyze data including purchase transactions and how a customer fills out a form to score borrowers. The platform is particularly useful to assign credit scores to those with limited credit histories.,77
Applications of artificial intelligence,Audit,"AI makes continuous auditing possible. Potential benefits include reducing audit risk, increasing the level of assurance, and reducing audit duration.",28
Applications of artificial intelligence,Anti-money laundering,"AI software, such as LaundroGraph which uses contemporary suboptimal datasets, could be used for anti-money laundering (AML). AI can be used to ""develop the AML pipeline into a robust, scalable solution with a reduced false positive rate and high adaptability"". A study about deep learning for AML identified ""key challenges for researchers"" to have ""access to recent real transaction data and scarcity of labelled training data; and data being highly imbalanced"" and suggests future research should bring-out ""explainability, graph deep learning using natural language processing (NLP), unsupervised and reinforcement learning to handle lack of labelled data; and joint research programs between the research community and industry to benefit from domain knowledge and controlled access to data"".",156
Applications of artificial intelligence,History,"In the 1980s, AI started to become prominent in finance as expert systems were commercialized. For example, Dupont created 100 expert systems, which helped them to save almost $10 million per year. One of the first systems was the Protrader expert system that predicted the 87-point drop in the Dow Jones Industrial Average in 1986. ""The major junctions of the system were to monitor premiums in the market, determine the optimum investment strategy, execute transactions when appropriate and modify the knowledge base through a learning mechanism.""One of the first expert systems to help with financial plans was PlanPowerm and Client Profiling System, created by Applied Expert Systems (APEX). It was launched in 1986. It helped create personal financial plans for people.In the 1990s AI was applied to fraud detection. In 1993 FinCEN Artificial Intelligence System (FAIS) launched. It was able to review over 200,000 transactions per week and over two years it helped identify 400 potential cases of money laundering equal to $1 billion. These expert systems were later replaced by machine learning systems.AI can enhance entrepreneurial activity and AI is one of the most dynamic areas for start-ups, with significant venture capital flowing into AI.",247
Applications of artificial intelligence,Government,"AI facial recognition systems are used for mass surveillance, notably in China.In 2019, Bengaluru, India deployed AI-managed traffic signals. This system uses cameras to monitor traffic density and adjust signal timing based on the interval needed to clear traffic.",52
Applications of artificial intelligence,Military,"Various countries are deploying AI military applications. The main applications enhance command and control, communications, sensors, integration and interoperability. Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles. AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles involving manned and unmanned teams. AI was incorporated into military operations in Iraq and Syria.Worldwide annual military spending on robotics rose from US$5.1 billion in 2010 to US$7.5 billion in 2015. Military drones capable of autonomous action are in wide use. Many researchers avoid military applications.",151
Applications of artificial intelligence,Health,,2
Applications of artificial intelligence,Healthcare,"AI in healthcare is often used for classification, to evaluate a CT scan or electrocardiogram or to identify high-risk patients for population health. AI is helping with the high-cost problem of dosing. One study suggested that AI could save $16 billion. In 2016, a study reported that an AI-derived formula derived the proper dose of immunosuppressant drugs to give to transplant patients. Current research has indicated that non-cardiac vascular illnesses are also being treated with artificial intelligence (AI). For certain disorders, AI algorithms can assist with diagnosis, recommended treatments, outcome prediction, and patient progress tracking. As AI technology advances, it is anticipated that it will become more significant in the healthcare industry.Microsoft's AI project Hanover helps doctors choose cancer treatments from among the more than 800 medicines and vaccines. Its goal is to memorize all the relevant papers to predict which (combinations of) drugs will be most effective for each patient. Myeloid leukemia is one target. Another study reported on an AI that was as good as doctors in identifying skin cancers. Another project monitors multiple high-risk patients by asking each patient questions based on data acquired from doctor/patient interactions. In one study done with transfer learning, an AI diagnosed eye conditions similar to an ophthalmologist and recommended treatment referrals.Another study demonstrated surgery with an autonomous robot. The team supervised the robot while it performed soft-tissue surgery, stitching together a pig's bowel judged better than a surgeon.Artificial neural networks are used as clinical decision support systems for medical diagnosis, such as in concept processing technology in EMR software.
Other healthcare tasks thought suitable for an AI that are in development include:

Screening
Heart sound analysis
Companion robots for elder care
Medical record analysis
Treatment plan design
Medication management
Assisting blind people
Consultations
Drug creation (e.g. by identifying candidate drugs and by using existing drug screening data such as in life extension research)
Clinical training
Outcome prediction for surgical procedures
HIV prognosis
Identifying genomic pathogen signatures of novel pathogens or identifying pathogens via physics-based fingerprints (including pandemic pathogens)
Helping link genes to their functions, otherwise analyzing genes and identification of novel biological targets
Help development of biomarkers
Help tailor therapies to individuals in personalized medicine/precision medicine",485
Applications of artificial intelligence,Workplace health and safety,"AI-enabled chatbots decrease the need for humans to perform basic call center tasks.Machine learning in sentiment analysis can spot fatigue in order to prevent overwork. Similarly, decision support systems can prevent industrial disasters and make disaster response more efficient. For manual workers in material handling, predictive analytics may be used to reduce musculoskeletal injury. Data collected from wearable sensors can improve workplace health surveillance, risk assessment, and research.AI can auto-code workers' compensation claims. AI-enabled virtual reality systems can enhance safety training for hazard recognition. AI can more efficiently detect accident near misses, which are important in reducing accident rates, but are often underreported.",138
Applications of artificial intelligence,Biochemistry,AlphaFold 2 can determine the 3D structure of a (folded) protein in hours rather than the months required by earlier automated approaches and was used to provide the likely structures of all proteins in the human body and essentially all proteins known to science (more than 200 million).,59
Applications of artificial intelligence,Chemistry and biology,"Machine learning has been used for drug design. It has also been used for predicting molecular properties and exploring large chemical/reaction spaces. Computer-planned syntheses via computational reaction networks, described as a platform that combines ""computational synthesis with AI algorithms to predict molecular properties"", have been used to explore the origins of life on Earth, drug-syntheses and developing routes for recycling 200 industrial waste chemicals into important drugs and agrochemicals (chemical synthesis design). There is research about which types of computer-aided chemistry would benefit from machine learning. It can also be used for ""drug discovery and development, drug repurposing, improving pharmaceutical productivity, and clinical trials"". It has been used for the design of proteins with prespecified functional sites.It has been used with databases for the development of a 46-day process to design, synthesize and test a drug which inhibits enzymes of a particular gene, DDR1. DDR1 is involved in cancers and fibrosis which is one reason for the high-quality datasets that enabled these results.There are various types of applications for machine learning in decoding human biology, such as helping to map gene expression patterns to functional activation patterns or identifying functional DNA motifs. It is widely used in genetic research.There also is some use of machine learning in synthetic biology, disease biology, nanotechnology (e.g. nanostructured materials and bionanotechnology), and materials science.",295
Applications of artificial intelligence,Novel types of machine learning,"There are also prototype robot scientists, including robot-embodied ones like the two Robot Scientists, which show a form of ""machine learning"" not commonly associated with the term.Similarly, there is research and development of biological ""wetware computers"" that can learn (e.g. for use as biosensors) and/or implantation into an organism's body (e.g. for use to control prosthetics). Polymer-based artificial neurons operate directly in biological environments and define biohybrid neurons made of artificial and living components.Moreover, if whole brain emulation is possible via both scanning and replicating the, at least, bio-chemical brain – as premised in the form of digital replication in The Age of Em, possibly using physical neural networks – that may have applications as or more extensive than e.g. valued human activities and may imply that society would face substantial moral choices, societal risks and ethical problems such as whether (and how) such are built, sent through space and used compared to potentially competing e.g. potentially more synthetic and/or less human and/or non/less-sentient types of artificial/semi-artificial intelligence. An alternative or additive approach to scanning are types of reverse engineering of the brain.
A subcategory of artificial intelligence is embodied, some of which are mobile robotic systems that each consist of one or multiple robots that are able to learn in the physical world.",297
Applications of artificial intelligence,Digital ghosts,,2
Applications of artificial intelligence,Biological computing in AI and as AI,"However, biological computers, even if both highly artificial and intelligent, are typically distinguished from synthetic, often silicon-based, computers – they could however be combined or used for the design of either. Moreover, many tasks may be carried out inadequately by artificial intelligence even if its algorithms were transparent, understood, bias-free, apparently effective, and goal-aligned and its trained data sufficiently large and cleansed – such as in cases were the underlying or available metrics, values or data are inappropriate. Computer-aided is a phrase used to describe human activities that make use of computing as tool in more comprehensive activities and systems such as AI for narrow tasks or making use of such without substantially relying on its results (see also: human-in-the-loop). A study described the biological as a limitation of AI with ""as long as the biological system cannot be understood, formalized, and imitated, we will not be able to develop technologies that can mimic it"" and that if it was understood this doesn't mean there being ""a technological solution to imitate natural intelligence"". Technologies that integrate biology and are often AI-based include biorobotics.",241
Applications of artificial intelligence,"Astronomy, space activities and ufology","Artificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for ""classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights"" for example for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. It could also be used for activities in space such as space exploration, including analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation.In the search for extraterrestrial intelligence (SETI), machine learning has been used in attempts to identify artificially generated electromagnetic waves in available data – such as real-time observations – and other technosignatures, e.g. via anomaly detection. In ufology, the SkyCAM-5 project headed by Prof. Hakan Kayal and the Galileo Project headed by Prof. Avi Loeb use machine learning to detect and classify peculiar types of UFOs. The Galileo Project also seeks to detect two further types of potential extraterrestrial technological signatures with the use of AI: 'Oumuamua-like interstellar objects, and non-manmade artificial satellites.",246
Applications of artificial intelligence,Future or non-human applications,"Loeb has speculated that one type of technological equipment the project may detect could be ""AI astronauts"" and in 2021 – in an opinion piece – that AI ""will"" ""supersede natural intelligence"", while Martin Rees stated that there ""may"" be more civilizations than thought with the ""majority of them"" being artificial. In particular, mid/far future or non-human applications of artificial intelligence could include advanced forms of artificial general intelligence that engages in space colonization or more narrow spaceflight-specific types of AI. In contrast, there have been concerns in relation to potential AGI or AI capable of embryo space colonization, or more generally natural intelligence-based space colonization, such as ""safety of encounters with an alien AI"", suffering risks (or inverse goals), moral license/responsibility in respect to colonization-effects, or AI gone rogue (e.g. as portrayed with fictional David8 and HAL 9000). See also: space law and space ethics. Loeb has described the possibility of ""AI astronauts"" that engage in ""supervised evolution"" (see also: directed evolution, uplift, directed panspermia and space colonization).",238
Applications of artificial intelligence,Astrochemistry,"It can also be used to produce datasets of spectral signatures of molecules that may be involved in the atmospheric production or consumption of particular chemicals – such as phosphine possibly detected on Venus – which could prevent miss assignments and, if accuracy is improved, be used in future detections and identifications of molecules on other planets.",66
Applications of artificial intelligence,Other fields of research,,4
Applications of artificial intelligence,"Archaeology, history and imaging of sites","Machine learning can help to restore and attribute ancient texts. It can help to index texts for example to enable better and easier searching and classification of fragments.
Artificial intelligence can also be used to investigate genomes to uncover genetic history, such as interbreeding between archaic and modern humans by which for example the past existence of a ghost population, not Neanderthal or Denisovan, was inferred. 
It can also be used for ""non-invasive and non-destructive access to internal structures of archaeological remains"".",110
Applications of artificial intelligence,Physics,"A deep learning system was reported to learn intuitive physics from visual data (of virtual 3D environments) based on an unpublished approach inspired by studies of visual cognition in infants. Other researchers have developed a machine learning algorithm that could discover sets of basic variables of various physical systems and predict the systems' future dynamics from video recordings of their behavior. In the future, it may be possible that such can be used to automate the discovery of physical laws of complex systems.",94
Applications of artificial intelligence,Materials science,AI could be used for materials optimization and discovery such as the discovery of stable materials and the prediction of their crystal structure.,27
Applications of artificial intelligence,Reverse engineering,"Machine learning is used in diverse types of reverse engineering. For example, machine learning has been used to reverse engineer a composite material part, enabling unauthorized production of high quality parts, and for quickly understanding the behavior of malware. It can be used to reverse engineer artificial intelligence models. It can also design components by engaging in a type of reverse engineering of not-yet existent virtual components such as inverse molecular design for particular desired functionality or protein design for prespecified functional sites. Biological network reverse engineering could model interactions in a human understandable way, e.g. bas on time series data of gene expression levels.",125
Applications of artificial intelligence,Law,,2
Applications of artificial intelligence,Legal analysis,"AI is a mainstay of law-related professions. Algorithms and machine learning do some tasks previously done by entry-level lawyers. While its use is common, it is not expected to replace most work done by lawyers in the near future.The electronic discovery industry uses machine learning to reduce manual searching.",65
Applications of artificial intelligence,Law enforcement and legal proceedings,"COMPAS is a commercial system used by U.S. courts to assess the likelihood of recidivism.One concern relates to algorithmic bias, AI programs may become biased after processing data that exhibits bias. ProPublica claims that the average COMPAS-assigned recidivism risk level of black defendants is significantly higher than that of white defendants.",78
Applications of artificial intelligence,Services,,2
Applications of artificial intelligence,Human resources,"Another application of AI is in human resources. AI can screen resumes and rank candidates based on their qualifications, predict candidate success in given roles, and automate repetitive communication tasks via chatbots.",40
Applications of artificial intelligence,Job search,"AI has simplified the recruiting /job search process for both recruiters and job seekers. According to Raj Mukherjee from Indeed, 65% of job searchers search again within 91 days after hire. An AI-powered engine streamlines the complexity of job hunting by assessing information on job skills, salaries, and user tendencies, matching job seekers to the most relevant positions. Machine intelligence calculates appropriate wages and highlights resume information for recruiters using NLP, which extracts relevant words and phrases from text. Another application is an AI resume builder that compiles a CV in 5 minutes. Chatbots assist website visitors and refine workflows.",128
Applications of artificial intelligence,Online and telephone customer service,"AI underlies avatars (automated online assistants) on web pages. It can reduce operation and training costs. Pypestream automated customer service for its mobile application to streamline communication with customers.A Google app analyzes language and converts speech into text. The platform can identify angry customers through their language and respond appropriately. Amazon uses a chatbot for customer service that can perform tasks like checking the status of an order, cancelling orders, offering refunds and connecting the customer with a human representative.",108
Applications of artificial intelligence,Hospitality,"In the hospitality industry, AI is used to reduce repetitive tasks, analyze trends, interact with guests, and predict customer needs. AI hotel services come in the form of a chatbot, application, virtual voice assistant and service robots.",49
Applications of artificial intelligence,Media,"AI applications analyze media content such as movies, TV programs, advertisement videos or user-generated content. The solutions often involve computer vision.
Typical scenarios include the analysis of images using object recognition or face recognition techniques, or the analysis of video for scene recognizing scenes, objects or faces. AI-based media analysis can facilitate media search, the creation of descriptive keywords for content, content policy monitoring (such as verifying the suitability of content for a particular TV viewing time), speech to text for archival or other purposes, and the detection of logos, products or celebrity faces for ad placement.

Motion interpolation
Pixel-art scaling algorithms
Image scaling
Image restoration
Photo colorization
Film restoration and video upscaling
Photo tagging
Automated species identification (such as identifying plants, fungi and animals with an app)
Text-to-image models such as DALL-E, Midjourney and Stable Diffusion
Image to video
Text to video such as Make-A-Video from Meta, Imagen video and Phenaki from Google
Text to music with AI models such as MusicLM
Text to speech such as ElevenLabs and 15.ai
Motion capture",247
Applications of artificial intelligence,Deep-fakes,"Deep-fakes can be used for comedic purposes but are better known for fake news and hoaxes.
In January 2016, the Horizon 2020 program financed the InVID Project to help journalists and researchers detect fake documents, made available as browser plugins.In June 2016, the visual computing group of the Technical University of Munich and from Stanford University developed Face2Face, a program that animates photographs of faces, mimicking the facial expressions of another person. The technology has been demonstrated animating the faces of people including Barack Obama and Vladimir Putin. Other methods have been demonstrated based on deep neural networks, from which the name deep fake was taken.
In September 2018, U.S. Senator Mark Warner proposed to penalize social media companies that allow sharing of deep-fake documents on their platforms.In 2018, Vincent Nozick found a way to detect faked content by analyzing eyelid movements. DARPA gave 68 million dollars to work on deep-fake detection.Audio deepfakes and AI software capable of detecting deep-fakes and cloning human voices have been developed.",219
Applications of artificial intelligence,"Video content analysis, surveillance and manipulated media detection",AI algorithms have been used to detect deepfake videos.,21
Applications of artificial intelligence,Music,"AI has been used to compose music of various genres.
David Cope created an AI called Emily Howell that managed to become well known in the field of algorithmic computer music. The algorithm behind Emily Howell is registered as a US patent.In 2012, AI Iamus created the first complete classical album.AIVA (Artificial Intelligence Virtual Artist), composes symphonic music, mainly classical music for film scores. It achieved a world first by becoming the first virtual composer to be recognized by a musical professional association.Melomics creates computer-generated music for stress and pain relief.At Sony CSL Research Laboratory, the Flow Machines software creates pop songs by learning music styles from a huge database of songs. It can compose in multiple styles.
The Watson Beat uses reinforcement learning and deep belief networks to compose music on a simple seed input melody and a select style. The software was open sourced and musicians such as Taryn Southern collaborated with the project to create music.
South Korean singer Hayeon's debut song, ""Eyes on You"" was composed using AI which was supervised by real composers, including NUVO.",231
Applications of artificial intelligence,Writing and reporting,"Narrative Science sells computer-generated news and reports. It summarizes sporting events based on statistical data from the game. It also creates financial reports and real estate analyses. Automated Insights generates personalized recaps and previews for Yahoo Sports Fantasy Football.Yseop, uses AI to turn structured data into natural language comments and recommendations. Yseop writes financial reports, executive summaries, personalized sales or marketing documents and more in multiple languages, including English, Spanish, French, and German.TALESPIN made up stories similar to the fables of Aesop. The program started with a set of characters who wanted to achieve certain goals. The story narrated their attempts to satisfy these goals. Mark Riedl and Vadim Bulitko asserted that the essence of storytelling was experience management, or ""how to balance the need for a coherent story progression with user agency, which is often at odds"".While AI storytelling focuses on story generation (character and plot), story communication also received attention. In 2002, researchers developed an architectural framework for narrative prose generation. They faithfully reproduced text variety and complexity on stories such as Little Red Riding Hood. In 2016, a Japanese AI co-wrote a short story and almost won a literary prize.South Korean company Hanteo Global uses a journalism bot to write articles.Literary authors are also exploring uses of AI. An example is David Jhave Johnston's work ReRites (2017-2019), where the poet created a daily rite of editing the poetic output of a neural network to create a series of performances and publications.",320
Applications of artificial intelligence,Wikipedia,"Millions of its articles have been edited by bots which however are usually not artificial intelligence software. Many AI platforms use Wikipedia data, mainly for training machine learning applications. There is research and development of various artificial intelligence applications for Wikipedia such as for identifying outdated sentences, detecting covert vandalism or recommending articles and tasks to new editors.
Machine translation (see above) has also be used for translating Wikipedia articles and could play a larger role in creating, updating, expanding, and generally improving articles in the future. A content translation tool allows editors of some Wikipedias to more easily translate articles across several select languages.",124
Applications of artificial intelligence,Video games,"In video games, AI is routinely used to generate behavior in non-player characters (NPCs). In addition, AI is used for pathfinding. Some researchers consider NPC AI in games to be a ""solved problem"" for most production tasks. Games with less typical AI include the AI director of Left 4 Dead (2008) and the neuroevolutionary training of platoons in Supreme Commander 2 (2010). AI is also used in Alien Isolation (2014) as a way to control the actions the Alien will perform next.Kinect, which provides a 3D body–motion interface for the Xbox 360 and the Xbox One, uses algorithms that emerged from AI research.",141
Applications of artificial intelligence,Art,"AI has been used to produce visual art. The first AI art program, called AARON, was developed by Harold Cohen in 1968 with the goal of being able to code the act of drawing.
It started by creating simple black and white drawings, and later to paint using special brushes and dyes that were chosen by the program itself without mediation from Cohen.

AI like ""Disco Diffusion"", ""DALL·E"" (1 and 2), Stable Diffusion, Imagen, ""Dream by Wombo"", Midjourney has been used for visualizing conceptual inputs such as song lyrics, certain texts or specific imagined concepts (or imaginations) in artistic ways or artistic images in general. Some of the tools also allow users to input images and various parameters e.g. to display an object or product in various environments, some can replicate artistic styles of popular artists, and some can create elaborate artistic images from rough sketches.
Since their design in 2014, generative adversarial networks (GANs) have been used by AI artists. GAN computer programming, generates technical images through machine learning frameworks that surpass the need for human operators. Examples of GAN programs that generate art include Artbreeder and DeepDream.",253
Applications of artificial intelligence,Art analysis,"In addition to the creation of original art, research methods that utilize AI have been generated to quantitatively analyze digital art collections. Although the main goal of the large-scale digitization of artwork in the past few decades was to allow for accessibility and exploration of these collections, the use of AI in analyzing them has brought about new research perspectives.
Two computational methods, close reading and distant viewing, are the typical approaches used to analyze digitized art. While distant viewing includes the analysis of large collections, close reading involves one piece of artwork.
Researchers have also introduced models that predict emotional responses to art.",124
Applications of artificial intelligence,Utilities,,2
Applications of artificial intelligence,Energy system,"Power electronics converters are used in renewable energy, energy storage, electric vehicles and high-voltage direct current transmission. These converters are failure-prone, which can interrupt service and require costly maintenance or catastrophic consequences in mission critical applications. AI can guide the design process for reliable power electronics converters, by calculating exact design parameters that ensure the required lifetime.Machine learning can be used for energy consumption prediction and scheduling, e.g. to help with renewable energy intermittency management (see also: smart grid and climate change mitigation in the power grid).",114
Applications of artificial intelligence,Telecommunications,"Many telecommunications companies make use of heuristic search to manage their workforces. For example, BT Group deployed heuristic search in an application that schedules 20,000 engineers. Machine learning is also used for speech recognition (SR), including of voice-controlled devices, and SR-related transcription, including of videos.",65
Applications of artificial intelligence,Manufacturing,,2
Applications of artificial intelligence,Sensors,"Artificial intelligence has been combined with digital spectrometry by IdeaCuria Inc., enable applications such as at-home water quality monitoring.",31
Applications of artificial intelligence,Toys and games,"In the 1990s early AIs controlled Tamagotchis and Giga Pets, the Internet, and the first widely released robot, Furby. Aibo was a domestic robot in the form of a robotic dog with intelligent features and autonomy.
Mattel created an assortment of AI-enabled toys that ""understand"" conversations, give intelligent responses, and learn.",79
Applications of artificial intelligence,Oil and gas,"Oil and gas companies have used artificial intelligence tools to automate functions, foresee equipment issues, and increase oil and gas output.",28
Applications of artificial intelligence,Transport,,2
Applications of artificial intelligence,Automotive,"AI in transport is expected to provide safe, efficient, and reliable transportation while minimizing the impact on the environment and communities. The major development challenge is the complexity of transportation systems that involves independent components and parties, with potentially conflicting objectives.AI-based fuzzy logic controllers operate gearboxes. For example, the 2006 Audi TT, VW Touareg and VW Caravell feature the DSP transmission. A number of Škoda variants (Škoda Fabia) include a fuzzy logic-based controller. Cars have AI-based driver-assist features such as self-parking and adaptive cruise control.
There are also prototypes of autonomous automotive public transport vehicles such as electric mini-buses as well as autonomous rail transport in operation.There also are prototypes of autonomous delivery vehicles, sometimes including delivery robots.Transportation's complexity means that in most cases training an AI in a real-world driving environment is impractical. Simulator-based testing can reduce the risks of on-road training.AI underpins self-driving vehicles. Companies involved with AI include Tesla, Waymo, and General Motors. AI-based systems control functions such as braking, lane changing, collision prevention, navigation and mapping.Autonomous trucks are in the testing phase. The UK government passed legislation to begin testing of autonomous truck platoons in 2018. A group of autonomous trucks follow closely behind each other. German corporation Daimler is testing its Freightliner Inspiration.Autonomous vehicles require accurate maps to be able to navigate between destinations. Some autonomous vehicles do not allow human drivers (they have no steering wheels or pedals).",328
Applications of artificial intelligence,Traffic management,"AI has been used to optimize traffic management, which reduces wait times, energy use, and emissions by as much as 25 percent.

Smart traffic lights have been developed at Carnegie Mellon since 2009.  Professor Stephen Smith has started a company since then Surtrac that has installed smart traffic control systems in 22 cities.  It costs about $20,000 per intersection to install. Drive time has been reduced by 25% and traffic jam waiting time has been reduced by 40% at the intersections it has been installed.",107
Applications of artificial intelligence,Military,"The Royal Australian Air Force (RAAF) Air Operations Division (AOD) uses AI for expert systems. AIs operate as surrogate operators for combat and training simulators, mission management aids, support systems for tactical decision making, and post processing of the simulator data into symbolic summaries.Aircraft simulators use AI for training aviators. Flight conditions can be simulated that allow pilots to make mistakes without risking themselves or expensive aircraft. Air combat can also be simulated.
AI can also be used to operate planes analogously to their control of ground vehicles. Autonomous drones can fly independently or in swarms.AOD uses the Interactive Fault Diagnosis and Isolation System, or IFDIS, which is a rule-based expert system using information from TF-30 documents and expert advice from mechanics that work on the TF-30. This system was designed to be used for the development of the TF-30 for the F-111C. The system replaced specialized workers. The system allowed regular workers to communicate with the system and avoid mistakes, miscalculations, or having to speak to one of the specialized workers.
Speech recognition allows traffic controllers to give verbal directions to drones.
Artificial intelligence supported design of aircraft, or AIDA, is used to help designers in the process of creating conceptual designs of aircraft. This program allows the designers to focus more on the design itself and less on the design process. The software also allows the user to focus less on the software tools. The AIDA uses rule-based systems to compute its data. This is a diagram of the arrangement of the AIDA modules. Although simple, the program is proving effective.",340
Applications of artificial intelligence,NASA,In 2003 a Dryden Flight Research Center project created software that could enable a damaged aircraft to continue flight until a safe landing can be achieved. The software compensated for damaged components by relying on the remaining undamaged components.The 2016 Intelligent Autopilot System combined apprenticeship learning and behavioral cloning whereby the autopilot observed low-level actions required to maneuver the airplane and high-level strategy used to apply those actions.,86
Applications of artificial intelligence,Maritime,Neural networks are used by situational awareness systems in ships and boats. There also are autonomous boats.,23
Applications of artificial intelligence,Environmental monitoring,"Autonomous ships that monitor the ocean, AI-driven satellite data analysis, passive acoustics or remote sensing and other applications of environmental monitoring make use of machine learning.For example, ""Global Plastic Watch"" is an AI-based satellite monitoring-platform for analysis/tracking of plastic waste sites to help prevention of plastic pollution – primarily ocean pollution – by helping identify who and where mismanages plastic waste, dumping it into oceans.",90
Applications of artificial intelligence,Early-warning systems,"Machine learning can be used to spot early-warning signs of disasters and environmental issues, possibly including natural pandemics, earthquakes, landslides, heavy rainfall, long-term water supply vulnerability, tipping-points of ecosystem collapse, cyanobacterial bloom outbreaks, and droughts.",60
Applications of artificial intelligence,Computer science,,2
Applications of artificial intelligence,Programming assistance,GitHub Copilot is an artificial intelligence model developed by GitHub and OpenAI that is able to autocomplete code in multiple programming languages.,32
Applications of artificial intelligence,Neural network design,"AI can be used to create other AIs. For example, around November 2017, Google's AutoML project to evolve new neural net topologies created NASNet, a system optimized for ImageNet and POCO F1. NASNet's performance exceeded all previously published performance on ImageNet.",63
Applications of artificial intelligence,Quantum computing,"Machine learning has been used for noise-cancelling in quantum technology, including quantum sensors. Moreover, there is substantial research and development of using quantum computers with machine learning algorithms. For example, there is a prototype, photonic, quantum memristive device for neuromorphic (quantum-)computers (NC)/artificial neural networks and NC-using quantum materials with some variety of potential neuromorphic computing-related applications, and quantum machine learning is a field with some variety of applications under development. AI could be used for quantum simulators which may have the application of solving physics and chemistry problems as well as for quantum annealers for training of neural networks for AI applications. There may also be some usefulness in chemistry, e.g. for drug discovery, and in materials science, e.g. for materials optimization/discovery (with possible relevance to quantum materials manufacturing).",182
Applications of artificial intelligence,Historical contributions,"AI researchers have created many tools to solve the most difficult problems in computer science. Many of their inventions have been adopted by mainstream computer science and are no longer considered AI. All of the following were originally developed in AI laboratories:
Time sharing
Interactive interpreters
Graphical user interfaces and the computer mouse
Rapid application development environments
The linked list data structure
Automatic storage management
Symbolic programming
Functional programming
Dynamic programming
Object-oriented programming
Optical character recognition
Constraint satisfaction",109
Applications of artificial intelligence,Business,,2
Applications of artificial intelligence,Customer service,Business websites and social media platforms for businesses like use chatbots for customer interactions like helping in answering frequently asked questions. Chatbots offers 24/7 support and replaces humans thereby helping in cutting business costs.,43
Applications of artificial intelligence,Content extraction,"An optical character reader is used in the extraction of data in business documents like invoices and receipts. It can also be used in business contract documents e.g employment agreements to extract critical data like employment terms, delivery terms, termination clauses, etc.",54
Applications of artificial intelligence,List of applications,,3
History of artificial intelligence,Summary,"The history of artificial intelligence (AI) began in antiquity, with myths, stories and rumors of artificial beings endowed with intelligence or consciousness by master craftsmen. The seeds of modern AI were planted by philosophers who attempted to describe the process of human thinking as the mechanical manipulation of symbols. This work culminated in the invention of the programmable digital computer in the 1940s, a machine based on the abstract essence of mathematical reasoning. This device and the ideas behind it inspired a handful of scientists to begin seriously discussing the possibility of building an electronic brain.
The field of AI research was founded at a workshop held on the campus of Dartmouth College, USA during the summer of 1956. Those who attended would become the leaders of AI research for decades. Many of them predicted that a machine as intelligent as a human being would exist in no more than a generation, and they were given millions of dollars to make this vision come true.Eventually, it became obvious that commercial developers and researchers had grossly underestimated the difficulty of the project. In 1974, in response to the criticism from James Lighthill and ongoing pressure from congress, the U.S. and British Governments stopped funding undirected research into artificial intelligence, and the difficult years that followed would later be known as an ""AI winter"". Seven years later, a visionary initiative by the Japanese Government inspired governments and industry to provide AI with billions of dollars, but by the late 1980s the investors became disillusioned and withdrew funding again.
Investment and interest in AI boomed in the first decades of the 21st century when machine learning was successfully applied to many problems in academia and industry due to new methods, the application of powerful computer hardware, and the collection of immense data sets.",349
History of artificial intelligence,Precursors,,2
History of artificial intelligence,"Mythical, fictional, and speculative precursors",,7
History of artificial intelligence,Myth and legend,"In Greek mythology, Talos was a giant constructed of bronze who acted as guardian for the island of Crete. He would throw boulders at the ships of invaders, and would complete 3 circuits around the island's perimeter daily. According to pseudo-Apollodorus' Bibliotheke, Hephaestus forged Talos with the aid of a cyclops and presented the automaton as a gift to Minos. In the Argonautica, Jason and the Argonauts defeated him by way of a single plug near his foot which, once removed, allowed the vital ichor to flow out from his body and left him inanimate.Pygmalion was a legendary king and sculptor of Greek mythology, famously represented in Ovid's Metamorphoses. In the 10th book of Ovid's narrative poem, Pygmalion becomes disgusted with women when he witnesses the way in which the Propoetides prostitute themselves. Despite this, he makes offerings at the temple of Venus asking the goddess to bring to him a woman just like a statue he carved. The earliest written account regarding golem-making is found in the writings of Eleazar ben Judah of Worms in the early 13th century. During the Middle Ages, it was believed that the animation of a Golem could be achieved by insertion of a piece of paper with any of God’s names on it, into the mouth of the clay figure. Unlike legendary automata like Brazen Heads, a Golem was unable to speak. English scholar Alexander Neckham asserted that the Ancient Roman poet Virgil had built a palace with automaton statues.",332
History of artificial intelligence,Alchemical means of artificial intelligence,"In Of the Nature of Things, written by the Swiss alchemist, Paracelsus, he describes a procedure which he claims can fabricate an ""artificial man"". By placing the ""sperm of a man"" in horse dung, and feeding it the ""Arcanum of Mans blood"" after 40 days, the concoction will become a living infant. Takwin, the artificial creation of life, was a frequent topic of Ismaili alchemical manuscripts, especially those attributed to Jabir ibn Hayyan. Islamic alchemists attempted to create a broad range of life through their work, ranging from plants to animals. In Faust: The Second Part of the Tragedy by Johann Wolfgang von Goethe, an alchemically fabricated homunculus, destined to live forever in the flask in which he was made, endeavors to be born into a full human body. Upon the initiation of this transformation, however, the flask shatters and the homunculus dies.",205
History of artificial intelligence,Modern fiction,"By the 19th century, ideas about artificial men and thinking machines were developed in fiction, as in Mary Shelley's Frankenstein  or Karel Čapek's R.U.R. (Rossum's Universal Robots),
and speculation, such as Samuel Butler's ""Darwin among the Machines"", and in real world instances, including Edgar Allan Poe's ""Maelzel's Chess Player"".
AI has become a regular topic of science fiction through the present.",98
History of artificial intelligence,Automata,"Realistic humanoid automata were built by craftsman from every civilization, including Yan Shi,Hero of Alexandria,Al-Jazari,Pierre Jaquet-Droz, and Wolfgang von Kempelen.
The oldest known automata were the sacred statues of ancient Egypt and Greece. The faithful believed that craftsman had imbued these figures with very real minds, capable of wisdom and emotion—Hermes Trismegistus wrote that ""by discovering the true nature of the gods, man has been able to reproduce it"".During the early modern period, these legendary automata were said to possess the magical ability to answer questions put to them. The late medieval alchemist and proto-protestant Roger Bacon was purported to have fabricated a brazen head, having developed a legend of having been a wizard. These legends were similar to the Norse myth of the Head of Mímir. According to legend, Mímir was known for his intellect and wisdom, and was beheaded in the Æsir-Vanir War. Odin is said to have ""embalmed"" the head with herbs and spoke incantations over it such that Mímir’s head remained able to speak wisdom to Odin. Odin then kept the head near him for counsel.",260
History of artificial intelligence,Formal reasoning,"Artificial intelligence is based on the assumption that the process of human thought can be mechanized. The study of mechanical—or ""formal""—reasoning has a long history. Chinese, Indian and Greek philosophers all developed structured methods of formal deduction in the first millennium BCE. Their ideas were developed over the centuries by philosophers such as Aristotle (who gave a formal analysis of the syllogism), Euclid (whose Elements was a model of formal reasoning), al-Khwārizmī (who developed algebra and gave his name to ""algorithm"") and European scholastic philosophers such as William of Ockham and Duns Scotus.Spanish philosopher Ramon Llull (1232–1315) developed several logical machines devoted to the production of knowledge by logical means; Llull described his machines as mechanical entities that could combine basic and undeniable truths by simple logical operations, produced by the machine by mechanical meanings, in such ways as to produce all the possible knowledge. Llull's work had a great influence on Gottfried Leibniz, who redeveloped his ideas.

In the 17th century, Leibniz, Thomas Hobbes and René Descartes explored the possibility that all rational thought could be made as systematic as algebra or geometry. Hobbes famously wrote in Leviathan: ""reason is nothing but reckoning"". Leibniz envisioned a universal language of reasoning, the characteristica universalis, which would reduce argumentation to calculation so that ""there would be no more need of disputation between two philosophers than between two accountants. For it would suffice to take their pencils in hand, down to their slates, and to say each other (with a friend as witness, if they liked): Let us calculate."" These philosophers had begun to articulate the physical symbol system hypothesis that would become the guiding faith of AI research.
In the 20th century, the study of mathematical logic provided the essential breakthrough that made artificial intelligence seem plausible. The foundations had been set by such works as Boole's The Laws of Thought and Frege's Begriffsschrift. Building on Frege's system, Russell and Whitehead presented a formal treatment of the foundations of mathematics in their masterpiece, the Principia Mathematica in 1913. Inspired by Russell's success, David Hilbert challenged mathematicians of the 1920s and 30s to answer this fundamental question: ""can all of mathematical reasoning be formalized?""
His question was answered by Gödel's incompleteness proof, Turing's machine and Church's Lambda calculus.

Their answer was surprising in two ways. First, they proved that there were, in fact, limits to what mathematical logic could accomplish. But second (and more important for AI) their work suggested that, within these limits, any form of mathematical reasoning could be mechanized. The Church-Turing thesis implied that a mechanical device, shuffling symbols as simple as 0 and 1, could imitate any conceivable process of mathematical deduction. The key insight was the Turing machine—a simple theoretical construct that captured the essence of abstract symbol manipulation. This invention would inspire a handful of scientists to begin discussing the possibility of thinking machines.",648
History of artificial intelligence,Computer science,"Calculating machines were built in antiquity and improved throughout history by many mathematicians, including (once again) philosopher Gottfried Leibniz. In the early 19th century, Charles Babbage designed a programmable computer (the Analytical Engine), although it was never built. Ada Lovelace speculated that the machine ""might compose elaborate and scientific pieces of music of any degree of complexity or extent"". (She is often credited as the first programmer because of a set of notes she wrote that completely detail a method for calculating Bernoulli numbers with the Engine.)
Following Babbage, although at first unaware of his earlier work, was Percy Ludgate, a clerk to a corn merchant in Dublin, Ireland. He independently designed a programmable mechanical computer, which he described in a work that was published in 1909.

Two other inventors, Leonardo Torres Quevedo and Vannevar Bush, also did follow on research based on Babbage's work. In his Essays on Automatics (1913) Torres designed a Babbage type of calculating machine that used electromechanical parts which introduced the idea of floating-point arithmetic. Torres is also known for having built in 1912 an autonomous machine capable of playing chess, El Ajedrecista. As opposed to The Turk and Ajeeb, El Ajedrecista (The Chessplayer) had a true integrated automation. It only played an endgame with three chess pieces, automatically moving a white king and a rook to checkmate the black king moved by a human opponent.Vannevar Bush's paper Instrumental Analysis (1936) discussed using existing IBM punch card machines to implement Babbage's design. In the same year he started the Rapid Arithmetical Machine project to investigate the problems of constructing an electronic digital computer.The first modern computers were the massive code breaking machines of the Second World War (such as Z3, ENIAC and Colossus). The latter two of these machines were based on the theoretical foundation laid by Alan Turing and developed by John von Neumann.",417
History of artificial intelligence,Birth of artificial intelligence (1952–1956),"In the 1940s and 50s, a handful of scientists from a variety of fields (mathematics, psychology, engineering, economics and political science) began to discuss the possibility of creating an artificial brain. The field of artificial intelligence research was founded as an academic discipline in 1956.",63
History of artificial intelligence,Cybernetics and early neural networks,"The earliest research into thinking machines was inspired by a confluence of ideas that became prevalent in the late 1930s, 1940s, and early 1950s. Recent research in neurology had shown that the brain was an electrical network of neurons that fired in all-or-nothing pulses. Norbert Wiener's cybernetics described control and stability in electrical networks. Claude Shannon's information theory described digital signals (i.e., all-or-nothing signals). Alan Turing's theory of computation showed that any form of computation could be described digitally. The close relationship between these ideas suggested that it might be possible to construct an electronic brain.Examples of work in this vein includes robots such as W. Grey Walter's turtles and the Johns Hopkins Beast. These machines did not use computers, digital electronics or symbolic reasoning; they were controlled entirely by analog circuitry.Walter Pitts and Warren McCulloch analyzed networks of idealized artificial neurons and showed how they might perform simple logical functions in 1943. They were the first to describe what later researchers would call a neural network. One of the students inspired by Pitts and McCulloch was a young Marvin Minsky, then a 24-year-old graduate student. In 1951 (with Dean Edmonds) he built the first neural net machine, the SNARC.Minsky was to become one of the most important leaders and innovators in AI for the next 50 years.",289
History of artificial intelligence,Turing's test,"In 1950 Alan Turing published a landmark paper in which he speculated about the possibility of creating machines that think.
He noted that ""thinking"" is difficult to define and devised his famous Turing Test. If a machine could carry on a conversation (over a teleprinter) that was indistinguishable from a conversation with a human being, then it was reasonable to say that the machine was ""thinking"". This simplified version of the problem allowed Turing to argue convincingly that a ""thinking machine"" was at least plausible and the paper answered all the most common objections to the proposition. The Turing Test was the first serious proposal in the philosophy of artificial intelligence.",131
History of artificial intelligence,Game AI,"In 1951, using the Ferranti Mark 1 machine of the University of Manchester, Christopher Strachey wrote a checkers program and Dietrich Prinz wrote one for chess. Arthur Samuel's checkers program, the subject of his 1959 paper ""Some Studies in Machine Learning Using the Game of Checkers"", eventually achieved sufficient skill to challenge a respectable amateur. Game AI would continue to be used as a measure of progress in AI throughout its history.",93
History of artificial intelligence,Symbolic reasoning and the Logic Theorist,"When access to digital computers became possible in the middle fifties, a few scientists instinctively recognized that a machine that could manipulate numbers could also manipulate symbols and that the manipulation of symbols could well be the essence of human thought. This was a new approach to creating thinking machines.In 1955, Allen Newell and (future Nobel Laureate) Herbert A. Simon created the ""Logic Theorist"" (with help from J. C. Shaw). The program would eventually prove 38 of the first 52 theorems in Russell and Whitehead's Principia Mathematica, and find new and more elegant proofs for some.
Simon said that they had ""solved the venerable mind/body problem, explaining how a system composed of matter can have the properties of mind.""
(This was an early statement of the philosophical position John Searle would later call ""Strong AI"": that machines can contain minds just as human bodies do.)",197
History of artificial intelligence,Dartmouth Workshop 1956: the birth of AI,"The Dartmouth Workshop of 1956
was organized by Marvin Minsky, John McCarthy and two senior scientists: Claude Shannon and Nathan Rochester of IBM，which is the eight-week Dartmouth Summer Research Program in Artificial Intelligence (DSRPAI), which marks the beginning of AI Spring.

The proposal for the conference included this assertion: ""every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it"".
The participants included Ray Solomonoff, Oliver Selfridge, Trenchard More, Arthur Samuel, Allen Newell and Herbert A. Simon, all of whom would create important programs during the first decades of AI research.
At the workshop Newell and Simon debuted the ""Logic Theorist"" and McCarthy persuaded the attendees to accept ""Artificial Intelligence"" as the name of the field.
The 1956 Dartmouth workshop was the moment that AI gained its name, its mission, its first success and its major players, and is widely considered the birth of AI. The term ""Artificial Intelligence"" was chosen by McCarthy to avoid associations with cybernetics and connections with the influential cyberneticist Norbert Wiener.",249
History of artificial intelligence,Symbolic AI (1956–1974),"The programs developed in the years after the Dartmouth Workshop were, to most people, simply ""astonishing"": computers were solving algebra word problems, proving theorems in geometry and learning to speak English. Few at the time would have believed that such ""intelligent"" behavior by machines was possible at all. Researchers expressed an intense optimism in private and in print, predicting that a fully intelligent machine would be built in less than 20 years. Government agencies like DARPA poured money into the new field.",103
History of artificial intelligence,Approaches,There were many successful programs and new directions in the late 50s and 1960s. Among the most influential were these:,27
History of artificial intelligence,Reasoning as search,"Many early AI programs used the same basic algorithm. To achieve some goal (like winning a game or proving a theorem), they proceeded step by step towards it (by making a move or a deduction) as if searching through a maze, backtracking whenever they reached a dead end. This paradigm was called ""reasoning as search"".The principal difficulty was that, for many problems, the number of possible paths through the ""maze"" was simply astronomical (a situation known as a ""combinatorial explosion""). Researchers would reduce the search space by using heuristics or ""rules of thumb"" that would eliminate those paths that were unlikely to lead to a solution.Newell and Simon tried to capture a general version of this algorithm in a program called the ""General Problem Solver"". Other ""searching"" programs were able to accomplish impressive tasks like solving problems in geometry and algebra, such as Herbert Gelernter's Geometry Theorem Prover (1958) and SAINT, written by Minsky's student James Slagle (1961). Other programs searched through goals and subgoals to plan actions, like the STRIPS system developed at Stanford to control the behavior of their robot Shakey.",247
History of artificial intelligence,Natural language,"An important goal of AI research is to allow computers to communicate in natural languages like English. An early success was Daniel Bobrow's program STUDENT, which could solve high school algebra word problems.A semantic net represents concepts (e.g. ""house"",""door"") as nodes and relations among concepts (e.g. ""has-a"") as links between the nodes. The first AI program to use a semantic net was written by Ross Quillian and the most successful (and controversial) version was Roger Schank's Conceptual dependency theory.Joseph Weizenbaum's ELIZA could carry out conversations that were so realistic that users occasionally were fooled into thinking they were communicating with a human being and not a program (See ELIZA effect). But in fact, ELIZA had no idea what she was talking about. She simply gave a canned response or repeated back what was said to her, rephrasing her response with a few grammar rules. ELIZA was the first chatterbot.",206
History of artificial intelligence,Micro-worlds,"In the late 60s, Marvin Minsky and Seymour Papert of the MIT AI Laboratory proposed that AI research should focus on artificially simple situations known as micro-worlds. They pointed out that in successful sciences like physics, basic principles were often best understood using simplified models like frictionless planes or perfectly rigid bodies. Much of the research focused on a ""blocks world,"" which consists of colored blocks of various shapes and sizes arrayed on a flat surface.This paradigm led to innovative work in machine vision by Gerald Sussman (who led the team), Adolfo Guzman, David Waltz (who invented ""constraint propagation""), and especially Patrick Winston. At the same time, Minsky and Papert built a robot arm that could stack blocks, bringing the blocks world to life. The crowning achievement of the micro-world program was Terry Winograd's SHRDLU. It could communicate in ordinary English sentences, plan operations and execute them.",197
History of artificial intelligence,Automata,"In Japan, Waseda University initiated the WABOT project in 1967, and in 1972 completed the WABOT-1, the world's first full-scale ""intelligent"" humanoid robot, or android. Its limb control system allowed it to walk with the lower limbs, and to grip and transport objects with hands, using tactile sensors. Its vision system allowed it to measure distances and directions to objects using external receptors, artificial eyes and ears. And its conversation system allowed it to communicate with a person in Japanese, with an artificial mouth.",113
History of artificial intelligence,Optimism,"The first generation of AI researchers made these predictions about their work:

1958, H. A. Simon and Allen Newell: ""within ten years a digital computer will be the world's chess champion"" and ""within ten years a digital computer will discover and prove an important new mathematical theorem.""
1965, H. A. Simon: ""machines will be capable, within twenty years, of doing any work a man can do.""
1967, Marvin Minsky: ""Within a generation ... the problem of creating 'artificial intelligence' will substantially be solved.""
1970, Marvin Minsky (in Life Magazine): ""In from three to eight years we will have a machine with the general intelligence of an average human being.""",153
History of artificial intelligence,Financing,"In June 1963, MIT received a $2.2 million grant from the newly created Advanced Research Projects Agency (later known as DARPA). The money was used to fund project MAC which subsumed the ""AI Group"" founded by Minsky and McCarthy five years earlier. DARPA continued to provide three million dollars a year until the 70s.DARPA made similar grants to Newell and Simon's program at CMU and to the Stanford AI Project (founded by John McCarthy in 1963). Another important AI laboratory was established at Edinburgh University by Donald Michie in 1965.
These four institutions would continue to be the main centers of AI research (and funding) in academia for many years.The money was proffered with few strings attached: J. C. R. Licklider, then the director of ARPA, believed that his organization should ""fund people, not projects!"" and allowed researchers to pursue whatever directions might interest them. This created a freewheeling atmosphere at MIT that gave birth to the hacker culture, but this ""hands off"" approach would not last.",223
History of artificial intelligence,First AI winter (1974–1980),"In the 1970s, AI was subject to critiques and financial setbacks. AI researchers had failed to appreciate the difficulty of the problems they faced. Their tremendous optimism had raised expectations impossibly high, and when the promised results failed to materialize, funding for AI disappeared. At the same time, the field of connectionism (or neural nets) was shut down almost completely for 10 years by Marvin Minsky's devastating criticism of perceptrons.
Despite the difficulties with public perception of AI in the late 70s, new ideas were explored in logic programming, commonsense reasoning and many other areas.",123
History of artificial intelligence,Problems,"In the early seventies, the capabilities of AI programs were limited. Even the most impressive could only handle trivial versions of the problems they were supposed to solve; all the programs were, in some sense, ""toys"". AI researchers had begun to run into several fundamental limits that could not be overcome in the 1970s. Although some of these limits would be conquered in later decades, others still stymie the field to this day.
Limited computer power: There was not enough memory or processing speed to accomplish anything truly useful. For example, Ross Quillian's successful work on natural language was demonstrated with a vocabulary of only twenty words, because that was all that would fit in memory. Hans Moravec argued in 1976 that computers were still millions of times too weak to exhibit intelligence. He suggested an analogy: artificial intelligence requires computer power in the same way that aircraft require horsepower. Below a certain threshold, it's impossible, but, as power increases, eventually it could become easy. With regard to computer vision, Moravec estimated that simply matching the edge and motion detection capabilities of human retina in real time would require a general-purpose computer capable of 109 operations/second (1000 MIPS). As of 2011, practical computer vision applications require 10,000 to 1,000,000 MIPS. By comparison, the fastest supercomputer in 1976, Cray-1 (retailing at $5 million to $8 million), was only capable of around 80 to 130 MIPS, and a typical desktop computer at the time achieved less than 1 MIPS.
Intractability and the combinatorial explosion. In 1972 Richard Karp (building on Stephen Cook's 1971 theorem) showed there are many problems that can probably only be solved in exponential time (in the size of the inputs). Finding optimal solutions to these problems requires unimaginable amounts of computer time except when the problems are trivial. This almost certainly meant that many of the ""toy"" solutions used by AI would probably never scale up into useful systems.
Commonsense knowledge and reasoning. Many important artificial intelligence applications like vision or natural language require simply enormous amounts of information about the world: the program needs to have some idea of what it might be looking at or what it is talking about. This requires that the program know most of the same things about the world that a child does. Researchers soon discovered that this was a truly vast amount of information. No one in 1970 could build a database so large and no one knew how a program might learn so much information.
Moravec's paradox: Proving theorems and solving geometry problems is comparatively easy for computers, but a supposedly simple task like recognizing a face or crossing a room without bumping into anything is extremely difficult. This helps explain why research into vision and robotics had made so little progress by the middle 1970s.
The frame and qualification problems. AI researchers (like John McCarthy) who used logic discovered that they could not represent ordinary deductions that involved planning or default reasoning without making changes to the structure of logic itself. They developed new logics (like non-monotonic logics and modal logics) to try to solve the problems.",644
History of artificial intelligence,End of funding,"The agencies which funded AI research (such as the British government, DARPA and NRC) became frustrated with the lack of progress and eventually cut off almost all funding for undirected research into AI. The pattern began as early as 1966 when the ALPAC report appeared criticizing machine translation efforts. After spending 20 million dollars, the NRC ended all support.
In 1973, the Lighthill report on the state of AI research in the UK criticized the utter failure of AI to achieve its ""grandiose objectives"" and led to the dismantling of AI research in that country.
(The report specifically mentioned the combinatorial explosion problem as a reason for AI's failings.)DARPA was deeply disappointed with researchers working on the Speech Understanding Research program at CMU and canceled an annual grant of three million dollars.
By 1974, funding for AI projects was hard to find.
Hans Moravec blamed the crisis on the unrealistic predictions of his colleagues. ""Many researchers were caught up in a web of increasing exaggeration.""
However, there was another issue: since the passage of the Mansfield Amendment in 1969, DARPA had been under increasing pressure to fund ""mission-oriented direct research, rather than basic undirected research"". Funding for the creative, freewheeling exploration that had gone on in the 60s would not come from DARPA. Instead, the money was directed at specific projects with clear objectives, such as autonomous tanks and battle management systems.",302
History of artificial intelligence,Critiques from across campus,"Several philosophers had strong objections to the claims being made by AI researchers. One of the earliest was John Lucas, who argued that Gödel's incompleteness theorem showed that a formal system (such as a computer program) could never see the truth of certain statements, while a human being could. Hubert Dreyfus ridiculed the broken promises of the 1960s and critiqued the assumptions of AI, arguing that human reasoning actually involved very little ""symbol processing"" and a great deal of embodied, instinctive, unconscious ""know how"". John Searle's Chinese Room argument, presented in 1980, attempted to show that a program could not be said to ""understand"" the symbols that it uses (a quality called ""intentionality""). If the symbols have no meaning for the machine, Searle argued, then the machine can not be described as ""thinking"".These critiques were not taken seriously by AI researchers, often because they seemed so far off the point. Problems like intractability and commonsense knowledge seemed much more immediate and serious. It was unclear what difference ""know how"" or ""intentionality"" made to an actual computer program. Minsky said of Dreyfus and Searle ""they misunderstand, and should be ignored."" Dreyfus, who taught at MIT, was given a cold shoulder: he later said that AI researchers ""dared not be seen having lunch with me."" Joseph Weizenbaum, the author of ELIZA, felt his colleagues' treatment of Dreyfus was unprofessional and childish. Although he was an outspoken critic of Dreyfus' positions, he ""deliberately made it plain that theirs was not the way to treat a human being.""Weizenbaum began to have serious ethical doubts about AI when Kenneth Colby wrote a ""computer program which can conduct psychotherapeutic dialogue"" based on ELIZA. Weizenbaum was disturbed that Colby saw a mindless program as a serious therapeutic tool. A feud began, and the situation was not helped when Colby did not credit Weizenbaum for his contribution to the program. In 1976, Weizenbaum published Computer Power and Human Reason which argued that the misuse of artificial intelligence has the potential to devalue human life.",460
History of artificial intelligence,Perceptrons and the attack on connectionism,"A perceptron was a form of neural network introduced in 1958 by Frank Rosenblatt, who had been a schoolmate of Marvin Minsky at the Bronx High School of Science. Like most AI researchers, he was optimistic about their power, predicting that ""perceptron may eventually be able to learn, make decisions, and translate languages."" An active research program into the paradigm was carried out throughout the 1960s but came to a sudden halt with the publication of Minsky and Papert's 1969 book Perceptrons. It suggested that there were severe limitations to what perceptrons could do and that Frank Rosenblatt's predictions had been grossly exaggerated. The effect of the book was devastating: virtually no research at all was done in connectionism for 10 years. Eventually, a new generation of researchers would revive the field and thereafter it would become a vital and useful part of artificial intelligence. Rosenblatt would not live to see this, as he died in a boating accident shortly after the book was published.",209
History of artificial intelligence,"Logic at Stanford, CMU and Edinburgh","Logic was introduced into AI research as early as 1959, by John McCarthy in his Advice Taker proposal.
In 1963, J. Alan Robinson had discovered a simple method to implement deduction on computers, the resolution and unification algorithm. However, straightforward implementations, like those attempted by McCarthy and his students in the late 1960s, were especially intractable: the programs required astronomical numbers of steps to prove simple theorems. A more fruitful approach to logic was developed in the 1970s by Robert Kowalski at the University of Edinburgh, and soon this led to the collaboration with French researchers Alain Colmerauer and Philippe Roussel who created the successful logic programming language Prolog.
Prolog uses a subset of logic (Horn clauses, closely related to ""rules"" and ""production rules"") that permit tractable computation. Rules would continue to be influential, providing a foundation for Edward Feigenbaum's expert systems and the continuing work by Allen Newell and Herbert A. Simon that would lead to Soar and their unified theories of cognition.Critics of the logical approach noted, as Dreyfus had, that human beings rarely used logic when they solved problems. Experiments by psychologists like Peter Wason, Eleanor Rosch, Amos Tversky, Daniel Kahneman and others provided proof.
McCarthy responded that what people do is irrelevant. He argued that what is really needed are machines that can solve problems—not machines that think as people do.",308
History of artificial intelligence,"MIT's ""anti-logic"" approach","Among the critics of McCarthy's approach were his colleagues across the country at MIT. Marvin Minsky, Seymour Papert and Roger Schank were trying to solve problems like ""story understanding"" and ""object recognition"" that required a machine to think like a person. In order to use ordinary concepts like ""chair"" or ""restaurant"" they had to make all the same illogical assumptions that people normally made. Unfortunately, imprecise concepts like these are hard to represent in logic. Gerald Sussman observed that ""using precise language to describe essentially imprecise concepts doesn't make them any more precise."" Schank described their ""anti-logic"" approaches as ""scruffy"", as opposed to the ""neat"" paradigms used by McCarthy, Kowalski, Feigenbaum, Newell and Simon.In 1975, in a seminal paper, Minsky noted that many of his fellow researchers were using the same kind of tool: a framework that captures all our common sense assumptions about something. For example, if we use the concept of a bird, there is a constellation of facts that immediately come to mind: we might assume that it flies, eats worms and so on. We know these facts are not always true and that deductions using these facts will not be ""logical"", but these structured sets of assumptions are part of the context of everything we say and think. He called these structures ""frames"". Schank used a version of frames he called ""scripts"" to successfully answer questions about short stories in English.",320
History of artificial intelligence,Boom (1980–1987),"In the 1980s a form of AI program called ""expert systems"" was adopted by corporations around the world and knowledge became the focus of mainstream AI research. In those same years, the Japanese government aggressively funded AI with its fifth generation computer project. Another encouraging event in the early 1980s was the revival of connectionism in the work of John Hopfield and David Rumelhart. Once again, AI had achieved success.",88
History of artificial intelligence,Rise of expert systems,"An expert system is a program that answers questions or solves problems about a specific domain of knowledge, using logical rules that are derived from the knowledge of experts. The earliest examples were developed by Edward Feigenbaum and his students. Dendral, begun in 1965, identified compounds from spectrometer readings. MYCIN, developed in 1972, diagnosed infectious blood diseases. They demonstrated the feasibility of the approach.Expert systems restricted themselves to a small domain of specific knowledge (thus avoiding the commonsense knowledge problem) and their simple design made it relatively easy for programs to be built and then modified once they were in place. All in all, the programs proved to be useful: something that AI had not been able to achieve up to this point.In 1980, an expert system called XCON was completed at CMU for the Digital Equipment Corporation. It was an enormous success: it was saving the company 40 million dollars annually by 1986. Corporations around the world began to develop and deploy expert systems and by 1985 they were spending over a billion dollars on AI, most of it to in-house AI departments. An industry grew up to support them, including hardware companies like Symbolics and Lisp Machines and software companies such as IntelliCorp and Aion.",257
History of artificial intelligence,Knowledge revolution,"The power of expert systems came from the expert knowledge they contained. They were part of a new direction in AI research that had been gaining ground throughout the 70s. ""AI researchers were beginning to suspect—reluctantly, for it violated the scientific canon of parsimony—that intelligence might very well be based on the ability to use large amounts of diverse knowledge in different ways,"" writes Pamela McCorduck. ""[T]he great lesson from the 1970s was that intelligent behavior depended very much on dealing with knowledge, sometimes quite detailed knowledge, of a domain where a given task lay"". Knowledge based systems and knowledge engineering became a major focus of AI research in the 1980s.The 1980s also saw the birth of Cyc, the first attempt to attack the commonsense knowledge problem directly, by creating a massive database that would contain all the mundane facts that the average person knows. Douglas Lenat, who started and led the project, argued that there is no shortcut ― the only way for machines to know the meaning of human concepts is to teach them, one concept at a time, by hand. The project was not expected to be completed for many decades.Chess playing programs HiTech and Deep Thought defeated chess masters in 1989. Both were developed by Carnegie Mellon University; Deep Thought development paved the way for Deep Blue.",270
History of artificial intelligence,Money returns: Fifth Generation project,"In 1981, the Japanese Ministry of International Trade and Industry set aside $850 million for the Fifth generation computer project. Their objectives were to write programs and build machines that could carry on conversations, translate languages, interpret pictures, and reason like human beings. Much to the chagrin of scruffies, they chose Prolog as the primary computer language for the project.Other countries responded with new programs of their own. The UK began the £350 million Alvey project. A consortium of American companies formed the Microelectronics and Computer Technology Corporation (or ""MCC"") to fund large scale projects in AI and information technology. DARPA responded as well, founding the Strategic Computing Initiative and tripling its investment in AI between 1984 and 1988.",157
History of artificial intelligence,Revival of connectionism,"In 1982, physicist John Hopfield was able to prove that a form of neural network (now called a ""Hopfield net"") could learn and process information in a completely new way. Around the same time, Geoffrey Hinton and David Rumelhart popularized a method for training neural networks called ""backpropagation"", also known as the reverse mode of automatic differentiation published by Seppo Linnainmaa (1970) and applied to neural networks by Paul Werbos. These two discoveries helped to revive the field of connectionism.The new field was unified and inspired by the appearance of Parallel Distributed Processing in 1986—a two volume collection of papers edited by Rumelhart and psychologist James McClelland. Neural networks would become commercially successful in the 1990s, when they began to be used as the engines driving programs like optical character recognition and speech recognition.The development of metal–oxide–semiconductor (MOS) very-large-scale integration (VLSI), in the form of complementary MOS (CMOS) technology, enabled the development of practical artificial neural network (ANN) technology in the 1980s. A landmark publication in the field was the 1989 book Analog VLSI Implementation of Neural Systems by Carver A. Mead and Mohammed Ismail.",264
History of artificial intelligence,Bust: second AI winter (1987–1993),"The business community's fascination with AI rose and fell in the 1980s in the classic pattern of an economic bubble. The collapse was due to the failure of commercial vendors to develop a wide variety of workable solutions. As dozens of companies failed, the perception was that the technology was not viable. However, the field continued to make advances despite the criticism. Numerous researchers, including robotics developers Rodney Brooks and Hans Moravec, argued for an entirely new approach to artificial intelligence.",101
History of artificial intelligence,AI winter,"The term ""AI winter"" was coined by researchers who had survived the funding cuts of 1974 when they became concerned that enthusiasm for expert systems had spiraled out of control and that disappointment would certainly follow. Their fears were well founded: in the late 1980s and early 1990s, AI suffered a series of financial setbacks.
The first indication of a change in weather was the sudden collapse of the market for specialized AI hardware in 1987. Desktop computers from Apple and IBM had been steadily gaining speed and power and in 1987 they became more powerful than the more expensive Lisp machines made by Symbolics and others. There was no longer a good reason to buy them. An entire industry worth half a billion dollars was demolished overnight.Eventually the earliest successful expert systems, such as XCON, proved too expensive to maintain. They were difficult to update, they could not learn, they were ""brittle"" (i.e., they could make grotesque mistakes when given unusual inputs), and they fell prey to problems (such as the qualification problem) that had been identified years earlier. Expert systems proved useful, but only in a few special contexts.In the late 1980s, the Strategic Computing Initiative cut funding to AI ""deeply and brutally"". New leadership at DARPA had decided that AI was not ""the next wave"" and directed funds towards projects that seemed more likely to produce immediate results.By 1991, the impressive list of goals penned in 1981 for Japan's Fifth Generation Project had not been met. Indeed, some of them, like ""carry on a casual conversation"" had not been met by 2010. As with other AI projects, expectations had run much higher than what was actually possible.Over 300 AI companies had shut down, gone bankrupt, or been acquired by the end of 1993, effectively ending the first commercial wave of AI. In 1994, HP Newquist stated in The Brain Makers that ""The immediate future of artificial intelligence—in its commercial form—seems to rest in part on the continued success of neural networks.""",408
History of artificial intelligence,Nouvelle AI and embodied reason,"In the late 1980s, several researchers advocated a completely new approach to artificial intelligence, based on robotics. They believed that, to show real intelligence, a machine needs to have a body — it needs to perceive, move, survive and deal with the world. They argued that these sensorimotor skills are essential to higher level skills like commonsense reasoning and that abstract reasoning was actually the least interesting or important human skill (see Moravec's paradox). They advocated building intelligence ""from the bottom up.""The approach revived ideas from cybernetics and control theory that had been unpopular since the sixties. Another precursor was David Marr, who had come to MIT in the late 1970s from a successful background in theoretical neuroscience to lead the group studying vision. He rejected all symbolic approaches (both McCarthy's logic and Minsky's frames), arguing that AI needed to understand the physical machinery of vision from the bottom up before any symbolic processing took place. (Marr's work would be cut short by leukemia in 1980.)In his 1990 paper ""Elephants Don't Play Chess,"" robotics researcher Rodney Brooks took direct aim at the physical symbol system hypothesis, arguing that symbols are not always necessary since ""the world is its own best model. It is always exactly up to date. It always has every detail there is to be known. The trick is to sense it appropriately and often enough."" In the 1980s and 1990s, many cognitive scientists also rejected the symbol processing model of the mind and argued that the body was essential for reasoning, a theory called the embodied mind thesis.",322
History of artificial intelligence,AI (1993–2011),"The field of AI, now more than a half a century old, finally achieved some of its oldest goals. It began to be used successfully throughout the technology industry, although somewhat behind the scenes. Some of the success was due to increasing computer power and some was achieved by focusing on specific isolated problems and pursuing them with the highest standards of scientific accountability. Still, the reputation of AI, in the business world at least, was less than pristine. Inside the field there was little agreement on the reasons for AI's failure to fulfill the dream of human level intelligence that had captured the imagination of the world in the 1960s. Together, all these factors helped to fragment AI into competing subfields focused on particular problems or approaches, sometimes even under new names that disguised the tarnished pedigree of ""artificial intelligence"". AI was both more cautious and more successful than it had ever been.",178
History of artificial intelligence,Milestones and Moore's law,"On 11 May 1997, Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov. The super computer was a specialized version of a framework produced by IBM, and was capable of processing twice as many moves per second as it had during the first match (which Deep Blue had lost), reportedly 200,000,000 moves per second. The event was broadcast live over the internet and received over 74 million hits.In 2005, a Stanford robot won the DARPA Grand Challenge by driving autonomously for 131 miles along an unrehearsed desert trail. Two years later, a team from CMU won the DARPA Urban Challenge by autonomously navigating 55 miles in an Urban environment while adhering to traffic hazards and all traffic laws. In February 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin.These successes were not due to some revolutionary new paradigm, but mostly on the tedious application of engineering skill and on the tremendous increase in the speed and capacity of computer by the 90s. In fact, Deep Blue's computer was 10 million times faster than the Ferranti Mark 1 that Christopher Strachey taught to play chess in 1951. This dramatic increase is measured by Moore's law, which predicts that the speed and memory capacity of computers doubles every two years, as a result of metal–oxide–semiconductor (MOS) transistor counts doubling every two years. The fundamental problem of ""raw computer power"" was slowly being overcome.",337
History of artificial intelligence,Intelligent agents,"A new paradigm called ""intelligent agents"" became widely accepted during the 1990s. Although earlier researchers had proposed modular ""divide and conquer"" approaches to AI, the intelligent agent did not reach its modern form until Judea Pearl, Allen Newell, Leslie P. Kaelbling, and others brought concepts from decision theory and economics into the study of AI. When the economist's definition of a rational agent was married to computer science's definition of an object or module, the intelligent agent paradigm was complete.
An intelligent agent is a system that perceives its environment and takes actions which maximize its chances of success. By this definition, simple programs that solve specific problems are ""intelligent agents"", as are human beings and organizations of human beings, such as firms. The intelligent agent paradigm defines AI research as ""the study of intelligent agents"". This is a generalization of some earlier definitions of AI: it goes beyond studying human intelligence; it studies all kinds of intelligence.The paradigm gave researchers license to study isolated problems and find solutions that were both verifiable and useful. It provided a common language to describe problems and share their solutions with each other, and with other fields that also used concepts of abstract agents, like economics and control theory. It was hoped that a complete agent architecture (like Newell's SOAR) would one day allow researchers to build more versatile and intelligent systems out of interacting intelligent agents.",286
History of artificial intelligence,Probabilistic reasoning and greater rigor,"AI researchers began to develop and use sophisticated mathematical tools more than they ever had in the past. There was a widespread realization that many of the problems that AI needed to solve were already being worked on by researchers in fields like mathematics, electrical engineering, economics or operations research. The shared mathematical language allowed both a higher level of collaboration with more established and successful fields and the achievement of results which were measurable and provable; AI had become a more rigorous ""scientific"" discipline. Russell & Norvig (2003) describe this as nothing less than a ""revolution"". They had argued in their 2002 textbook that this increased rigor could be viewed plausibly as a ""victory of the neats,"" but subsequently qualified that by saying, in their 2020 AI textbook, that ""The present emphasis on deep learning may represent a resurgence of the scruffies.""Judea Pearl's influential 1988 book brought probability and decision theory into AI. Among the many new tools in use were Bayesian networks, hidden Markov models, information theory, stochastic modeling and classical optimization. Precise mathematical descriptions were also developed for ""computational intelligence"" paradigms like neural networks and evolutionary algorithms.",245
History of artificial intelligence,AI behind the scenes,"Algorithms originally developed by AI researchers began to appear as parts of larger systems. AI had solved a lot of very difficult problems
and their solutions proved to be useful throughout the technology industry, such as
data mining,
industrial robotics,
logistics,speech recognition,
banking software,
medical diagnosis
and Google's search engine.The field of AI received little or no credit for these successes in the 1990s and early 2000s. Many of AI's greatest innovations have been reduced to the status of just another item in the tool chest of computer science. Nick Bostrom explains ""A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.""Many researchers in AI in the 1990s deliberately called their work by other names, such as informatics, knowledge-based systems, cognitive systems or computational intelligence. In part, this may have been because they considered their field to be fundamentally different from AI, but also the new names help to procure funding. In the commercial world at least, the failed promises of the AI Winter continued to haunt AI research into the 2000s, as the New York Times reported in 2005: ""Computer scientists and software engineers avoided the term artificial intelligence for fear of being viewed as wild-eyed dreamers.""",277
History of artificial intelligence,"Predictions (or ""Where is HAL 9000?"")","In 1968, Arthur C. Clarke and Stanley Kubrick had imagined that, by the year 2001, a machine would exist with an intelligence that matched or exceeded the capability of human beings. The character they created, HAL 9000, was based on a belief shared by many leading AI researchers that such a machine would exist by the year 2001.In 2001, AI founder Marvin Minsky asked ""So the question is why didn't we get HAL in 2001?"" Minsky believed that the answer is that the central problems, like commonsense reasoning, were being neglected, while most researchers pursued things like commercial applications of neural nets or genetic algorithms. John McCarthy, on the other hand, still blamed the qualification problem. For Ray Kurzweil, the issue is computer power and, using Moore's Law, he predicted that machines with human-level intelligence will appear by 2029. Jeff Hawkins argued that neural net research ignores the essential properties of the human cortex, preferring simple models that have been successful at solving simple problems. There were many other explanations and for each there was a corresponding research program underway.",226
History of artificial intelligence,"Deep learning, big data and artificial general intelligence: 2011–present","In the first decades of the 21st century, access to large amounts of data (known as ""big data""), cheaper and faster computers and advanced machine learning techniques were successfully applied to many problems throughout the economy. In fact, McKinsey Global Institute estimated in their famous paper ""Big data: The next frontier for innovation, competition, and productivity"" that ""by 2009, nearly all sectors in the US economy had at least an average of 200 terabytes of stored data"".
By 2016, the market for AI-related products, hardware, and software reached more than 8 billion dollars, and the New York Times reported that interest in AI had reached a ""frenzy"". The applications of big data began to reach into other fields as well, such as training models in ecology and for various applications in economics. Advances in deep learning (particularly deep convolutional neural networks and recurrent neural networks) drove progress and research in image and video processing, text analysis, and even speech recognition.",210
History of artificial intelligence,Deep learning,"Deep learning is a branch of machine learning that models high level abstractions in data by using a deep graph with many processing layers. According to the Universal approximation theorem, deep-ness isn't necessary for a neural network to be able to approximate arbitrary continuous functions. Even so, there are many problems that are common to shallow networks (such as overfitting) that deep networks help avoid. As such, deep neural networks are able to realistically generate much more complex models as compared to their shallow counterparts.
However, deep learning has problems of its own. A common problem for recurrent neural networks is the vanishing gradient problem, which is where gradients passed between layers gradually shrink and literally disappear as they are rounded off to zero. There have been many methods developed to approach this problem, such as Long short-term memory units.
State-of-the-art deep neural network architectures can sometimes even rival human accuracy in fields like computer vision, specifically on things like the MNIST database, and traffic sign recognition.Language processing engines powered by smart search engines can easily beat humans at answering general trivia questions (such as IBM Watson), and recent developments in deep learning have produced astounding results in competing with humans, in things like Go, and Doom (which, being a first-person shooter game, has sparked some controversy).",266
History of artificial intelligence,Big Data,"Big data refers to a collection of data that cannot be captured, managed, and processed by conventional software tools within a certain time frame. It is a massive amount of decision-making, insight, and process optimization capabilities that require new processing models. In the Big Data Era written by Victor Meyer Schonberg and Kenneth Cooke, big data means that instead of random analysis (sample survey), all data is used for analysis. The 5V characteristics of big data (proposed by IBM): Volume, Velocity, Variety, Value, Veracity.
The strategic significance of big data technology is not to master huge data information, but to specialize in these meaningful data. In other words, if big data is likened to an industry, the key to realizing profitability in this industry is to increase the ""process capability"" of the data and realize the ""value added"" of the data through ""processing"".",181
History of artificial intelligence,Artificial general intelligence,"General intelligence is the ability to solve any problem, rather than finding a solution to a particular problem. Artificial general intelligence (or ""AGI"") is a program which can apply intelligence to a wide variety of problems, in much the same ways humans can. Artificial general intelligence is also referred to as ""strong AI"", or synthetic intelligence as opposed to ""weak AI"" or ""narrow AI"". (Academic sources reserve ""strong AI"" to refer to machines capable of experiencing consciousness.)
Foundation models, which are large artificial intelligence models trained on vast quantities of unlabeled data that can be adapted to a wide range of downstream tasks, began to be developed in 2018. Models such as GPT-3 released by OpenAI in 2020, and Gato released by DeepMind in 2022, have been described as important milestones on the path to artificial general intelligence.In 2023, Microsoft Research tested the GPT-4 large language model with a large variety of tasks, and concluded that ""it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system"".",229
Ethics of artificial intelligence,Summary,"The ethics of artificial intelligence is the branch of the ethics of technology specific to artificially intelligent systems. It is sometimes divided into a concern with the moral behavior of humans as they design, make, use and treat artificially intelligent systems, and a concern with the behavior of machines, in machine ethics.",62
Ethics of artificial intelligence,Ethics fields' approaches,,4
Ethics of artificial intelligence,Robot ethics,"The term ""robot ethics"" (sometimes ""roboethics"") refers to the morality of how humans design, construct, use and treat robots. Robot ethics intersect with the ethics of AI. Robots are physical machines whereas AI can be only software. Not all robots function through AI systems and not all AI systems are robots. Robot ethics considers how machines may be used to harm or benefit humans, their impact on individual autonomy, and their effects on social justice.",96
Ethics of artificial intelligence,Machine ethics,"Machine ethics (or machine morality) is the field of research concerned with designing Artificial Moral Agents (AMAs), robots or artificially intelligent computers that behave morally or as though moral. To account for the nature of these agents, it has been suggested to consider certain philosophical ideas, like the standard characterizations of agency, rational agency, moral agency, and artificial agency, which are related to the concept of AMAs.Isaac Asimov considered the issue in the 1950s in his I, Robot. At the insistence of his editor John W. Campbell Jr., he proposed the Three Laws of Robotics to govern artificially intelligent systems.  Much of his work was then spent testing the boundaries of his three laws to see where they would break down, or where they would create paradoxical or unanticipated behavior. His work suggests that no set of fixed laws can sufficiently anticipate all possible circumstances. More recently, academics and many governments have challenged the idea that AI can itself be held accountable. A panel convened by the United Kingdom in 2010 revised Asimov's laws to clarify that AI is the responsibility either of its manufacturers, or of its owner/operator.In 2009, during an experiment at the Laboratory of Intelligent Systems in the Ecole Polytechnique Fédérale of Lausanne, Switzerland, robots that were programmed to cooperate with each other (in searching out a beneficial resource and avoiding a poisonous one) eventually learned to lie to each other in an attempt to hoard the beneficial resource.Some experts and academics have questioned the use of robots for military combat, especially when such robots are given some degree of autonomous functions. The US Navy has funded a report which indicates that as military robots become more complex, there should be greater attention to implications of their ability to make autonomous decisions. The President of the Association for the Advancement of Artificial Intelligence has commissioned a study to look at this issue. They point to programs like the Language Acquisition Device which can emulate human interaction.
Vernor Vinge has suggested that a moment may come when some computers are smarter than humans. He calls this ""the Singularity"".  He suggests that it may be somewhat or possibly very dangerous for humans. This is discussed by a philosophy called Singularitarianism. The Machine Intelligence Research Institute has suggested a need to build ""Friendly AI"", meaning that the advances which are already occurring with AI should also include an effort to make AI intrinsically friendly and humane.There are discussion on creating tests to see if an AI is capable of making ethical decisions. Alan Winfield concludes that the Turing test is flawed and the requirement for an AI to pass the test is too low. A proposed alternative test is one called the Ethical Turing Test, which would improve on the current test by having multiple judges decide if the AI's decision is ethical or unethical.In 2009, academics and technical experts attended a conference organized by the Association for the Advancement of Artificial Intelligence to discuss the potential impact of robots and computers and the impact of the hypothetical possibility that they could become self-sufficient and able to make their own decisions. They discussed the possibility and the extent to which computers and robots might be able to acquire any level of autonomy, and to what degree they could use such abilities to possibly pose any threat or hazard. They noted that some machines have acquired various forms of semi-autonomy, including being able to find power sources on their own and being able to independently choose targets to attack with weapons. They also noted that some computer viruses can evade elimination and have achieved ""cockroach intelligence"". They noted that self-awareness as depicted in science-fiction is probably unlikely, but that there were other potential hazards and pitfalls.However, there is one technology in particular that could truly bring the possibility of robots with moral competence to reality. In a paper on the acquisition of moral values by robots, Nayef Al-Rodhan mentions the case of neuromorphic chips, which aim to process information similarly to humans, nonlinearly and with millions of interconnected artificial neurons. Robots embedded with neuromorphic technology could learn and develop knowledge in a uniquely humanlike way. Inevitably, this raises the question of the environment in which such robots would learn about the world and whose morality they would inherit – or if they end up developing human 'weaknesses' as well: selfishness, a pro-survival attitude, hesitation etc.
In Moral Machines: Teaching Robots Right from Wrong, Wendell Wallach and Colin Allen conclude that attempts to teach robots right from wrong will likely advance understanding of human ethics by motivating humans to address gaps in modern normative theory and by providing a platform for experimental investigation. As one example, it has introduced normative ethicists to the controversial issue of which specific learning algorithms to use in machines. Nick Bostrom and Eliezer Yudkowsky have argued for decision trees (such as ID3) over neural networks and genetic algorithms on the grounds that decision trees obey modern social norms of transparency and predictability (e.g. stare decisis), while Chris Santos-Lang argued in the opposite direction on the grounds that the norms of any age must be allowed to change and that natural failure to fully satisfy these particular norms has been essential in making humans less vulnerable to criminal ""hackers"".According to a 2019 report from the Center for the Governance of AI at the University of Oxford, 82% of Americans believe that robots and AI should be carefully managed. Concerns cited ranged from how AI is used in surveillance and in spreading fake content online (known as deep fakes when they include doctored video images and audio generated with help from AI) to cyberattacks, infringements on data privacy, hiring bias, autonomous vehicles, and drones that do not require a human controller.",1156
Ethics of artificial intelligence,Ethics principles of artificial intelligence,"In the review of 84 ethics guidelines for AI, 11 clusters of principles were found: transparency, justice and fairness, non-maleficence, responsibility, privacy, beneficence, freedom and autonomy, trust, sustainability, dignity, solidarity.Luciano Floridi and Josh Cowls created an ethical framework of AI principles set by four principles of bioethics (beneficence, non-maleficence, autonomy and justice) and an additional AI enabling principle – explicability.",108
Ethics of artificial intelligence,"Transparency, accountability, and open source","Bill Hibbard argues that because AI will have such a profound effect on humanity, AI developers are representatives of future humanity and thus have an ethical obligation to be transparent in their efforts. Ben Goertzel and David Hart created OpenCog as an open source framework for AI development.  OpenAI is a non-profit AI research company created by Elon Musk, Sam Altman and others to develop open-source AI beneficial to humanity. There are numerous other open-source AI developments.
Unfortunately, making code open source does not make it comprehensible, which by many definitions means that the AI code is not transparent. The IEEE has a standardisation effort on AI transparency. The IEEE effort identifies multiple scales of transparency for different users. Further, there is concern that releasing the full capacity of contemporary AI to some organizations may be a public bad, that is, do more damage than good. For example, Microsoft has expressed concern about allowing universal access to its face recognition software, even for those who can pay for it. Microsoft posted an extraordinary blog on this topic, asking for government regulation to help determine the right thing to do.Not only companies, but many other researchers and citizen advocates recommend government regulation as a means of ensuring transparency, and through it, human accountability. This strategy has proven controversial, as some worry that it will slow the rate of innovation. Others argue that regulation leads to systemic stability more able to support innovation in the long term. The OECD, UN, EU, and many countries are presently working on strategies for regulating AI, and finding appropriate legal frameworks.On June 26, 2019, the European Commission High-Level Expert Group on Artificial Intelligence (AI HLEG) published its ""Policy and investment recommendations for trustworthy Artificial Intelligence"". This is the AI HLEG's second deliverable, after the April 2019 publication of the ""Ethics Guidelines for Trustworthy AI"". The June AI HLEG recommendations cover four principal subjects: humans and society at large, research and academia, the private sector, and the public sector. The European Commission claims that ""HLEG's recommendations reflect an appreciation of both the opportunities for AI technologies to drive economic growth, prosperity and innovation, as well as the potential risks involved"" and states that the EU aims to lead on the framing of policies governing AI internationally. To prevent harm, in addition to regulation, AI-deploying organizations need to play a central role in creating and deploying trustworthy AI in line with the principles of trustworthy AI, and take accountability to mitigate the risks. On 21 April 2021, the European Commission proposed the Artificial Intelligence Act.",530
Ethics of artificial intelligence,Ethical challenges,,2
Ethics of artificial intelligence,Biases in AI systems,"AI has become increasingly inherent in facial and voice recognition systems. Some of these systems have real business applications and directly impact people. These systems are vulnerable to biases and errors introduced by its human creators. Also, the data used to train these AI systems itself can have biases. For instance, facial recognition algorithms made by Microsoft, IBM and Face++ all had biases when it came to detecting people's gender; these AI systems were able to detect gender of white men more accurately than gender of darker skin men. Further, a 2020 study reviewed voice recognition systems from Amazon, Apple, Google, IBM, and Microsoft found that they have higher error rates when transcribing black people's voices than white people's. Furthermore, Amazon terminated their use of AI hiring and recruitment because the algorithm favored male candidates over female ones. This was because Amazon's system was trained with data collected over 10-year period that came mostly from male candidates.Bias can creep into algorithms in many ways. The most predominant view on how bias is introduced into AI systems is that it is embedded within the historical data used to train the system. For instance, Amazon's AI-powered recruitment tool was trained with its own recruitment data accumulated over the years, during which time the candidates that successfully got the job were mostly white males. Consequently, the algorithms learned the (biased) pattern from the historical data and generated predictions for the present/future that these types of candidates are most likely to succeed in getting the job. Therefore, the recruitment decisions made by the AI system turn out to be biased against female and minority candidates. Friedman and Nissenbaum identify three categories of bias in computer systems: existing bias, technical bias, and emergent bias. In natural language processing, problems can arise from the text corpus — the source material the algorithm uses to learn about the relationships between different words.Large companies such as IBM, Google, etc. have made efforts to research and address these biases. One solution for addressing bias is to create documentation for the data used to train AI systems. Process mining can be an important tool for organizations to achieve compliance with proposed AI regulations by identifying errors, monitoring processes, identifying potential root causes for improper execution, and other functions.The problem of bias in machine learning is likely to become more significant as the technology spreads to critical areas like medicine and law, and as more people without a deep technical understanding are tasked with deploying it. Some experts warn that algorithmic bias is already pervasive in many industries and that almost no one is making an effort to identify or correct it. There are some open-sourced tools  by civil societies that are looking to bring more awareness to biased AI.",538
Ethics of artificial intelligence,Robot rights,"""Robot rights"" is the concept that people should have moral obligations towards their machines, akin to human rights or animal rights. It has been suggested that robot rights (such as a right to exist and perform its own mission) could be linked to robot duty to serve humanity, analogous to linking human rights with human duties before society. These could include the right to life and liberty, freedom of thought and expression, and equality before the law. The issue has been considered by the Institute for the Future and by the U.K. Department of Trade and Industry.Experts disagree on how soon specific and detailed laws on the subject will be necessary. Glenn McGee reported that sufficiently humanoid robots might appear by 2020, while Ray Kurzweil sets the date at 2029. Another group of scientists meeting in 2007 supposed that at least 50 years had to pass before any sufficiently advanced system would exist.The rules for the 2003 Loebner Prize competition envisioned the possibility of robots having rights of their own:

61. If in any given year, a publicly available open-source Entry entered by the University of Surrey or the Cambridge Center wins the Silver Medal or the Gold Medal, then the Medal and the Cash Award will be awarded to the body responsible for the development of that Entry. If no such body can be identified, or if there is disagreement among two or more claimants, the Medal and the Cash Award will be held in trust until such time as the Entry may legally possess, either in the United States of America or in the venue of the contest, the Cash Award and Gold Medal in its own right. 
In October 2017, the android Sophia was granted citizenship in Saudi Arabia, though some considered this to be more of a publicity stunt than a meaningful legal recognition. Some saw this gesture as openly denigrating of human rights and the rule of law.The philosophy of Sentientism grants degrees of moral consideration to all sentient beings, primarily humans and most non-human animals. If artificial or alien intelligence show evidence of being sentient, this philosophy holds that they should be shown compassion and granted rights.
Joanna Bryson has argued that creating AI that requires rights is both avoidable, and would in itself be unethical, both as a burden to the AI agents and to human society.",463
Ethics of artificial intelligence,Artificial suffering,"In 2020, professor Shimon Edelman noted that only a small portion of work in the rapidly growing field of AI ethics addressed the possibility of AIs experiencing suffering. This was despite credible theories having outlined possible ways by which AI systems may became conscious, such as Integrated information theory. Edelman notes one exception had been Thomas Metzinger, who in 2018 called for a global moratorium on further work that risked creating conscious AIs. The moratorium was to run to 2050 and could be either extended or repealed early, depending on progress in better understanding the risks and how to mitigate them. Metzinger repeated this argument in 2021, highlighting the  risk of creating an ""explosion of artificial suffering"", both as an AI might suffer in intense ways that humans could not understand, and as replication processes may see the creation of huge quantities of artificial conscious instances. Several labs have openly stated they are trying to create conscious AIs. There have been reports from those with close access to AIs not openly intended to be self aware, that consciousness may already have unintentionally emerged. These include OpenAI founder Ilya Sutskever in February 2022, when he wrote that today's large neural nets may be ""slightly conscious"". In November 2022, David Chalmers argued that it was unlikely current large language models like GPT-3 had experienced consciousness, but also that he considered there to be a serious possibility that large language models may become conscious in the future.",293
Ethics of artificial intelligence,Threat to human dignity,"Joseph Weizenbaum argued in 1976 that AI technology should not be used to replace people in positions that require respect and care, such as:

A customer service representative (AI technology is already used today for telephone-based interactive voice response systems)
A nursemaid for the elderly (as was reported by Pamela McCorduck in her book The Fifth Generation)
A soldier
A judge
A police officer
A therapist (as was proposed by Kenneth Colby in the 70s)Weizenbaum explains that we require authentic feelings of empathy from people in these positions. If machines replace them, we will find ourselves alienated, devalued and frustrated, for the artificially intelligent system would not be able to simulate empathy. Artificial intelligence, if used in this way, represents a threat to human dignity. Weizenbaum argues that the fact that we are entertaining the possibility of machines in these positions suggests that we have experienced an ""atrophy of the human spirit that comes from thinking of ourselves as computers.""Pamela McCorduck counters that, speaking for women and minorities ""I'd rather take my chances with an impartial computer"", pointing out that there are conditions where we would prefer to have automated judges and police that have no personal agenda at all. However, Kaplan and Haenlein stress that AI systems are only as smart as the data used to train them since they are, in their essence, nothing more than fancy curve-fitting machines; Using AI to support a court ruling can be highly problematic if past rulings show bias toward certain groups since those biases get formalized and engrained, which makes them even more difficult to spot and fight against.Weizenbaum was also bothered that AI researchers (and some philosophers) were willing to view the human mind as nothing more than a computer program (a position now known as computationalism). To Weizenbaum, these points suggest that AI research devalues human life.AI founder John McCarthy objects to the moralizing tone of Weizenbaum's critique. ""When moralizing is both vehement and vague, it invites authoritarian abuse,"" he writes. Bill Hibbard writes that ""Human dignity requires that we strive to remove our ignorance of the nature of existence, and AI is necessary for that striving.""",454
Ethics of artificial intelligence,Liability for self-driving cars,"As the widespread use of autonomous cars becomes increasingly imminent, new challenges raised by fully autonomous vehicles must be addressed. Recently, there has been debate as to the legal liability of the responsible party if these cars get into accidents. In one report where a driverless car hit a pedestrian, the driver was inside the car but the controls were fully in the hand of computers. This led to a dilemma over who was at fault for the accident.In another incident on March 18, 2018, Elaine Herzberg was struck and killed by a self-driving Uber in Arizona. In this case, the automated car was capable of detecting cars and certain obstacles in order to autonomously navigate the roadway, but it could not anticipate a pedestrian in the middle of the road. This raised the question of whether the driver, pedestrian, the car company, or the government should be held responsible for her death.Currently, self-driving cars are considered semi-autonomous, requiring the driver to pay attention and be prepared to take control if necessary. Thus, it falls on governments to regulate the driver who over-relies on autonomous features. as well educate them that these are just technologies that, while convenient, are not a complete substitute. Before autonomous cars become widely used, these issues need to be tackled through new policies.",267
Ethics of artificial intelligence,Weaponization of artificial intelligence,"Some experts and academics have questioned the use of robots for military combat, especially when such robots are given some degree of autonomy. On October 31, 2019, the United States Department of Defense's Defense Innovation Board published the draft of a report recommending principles for the ethical use of artificial intelligence by the Department of Defense that would ensure a human operator would always be able to look into the 'black box' and understand the kill-chain process. However, a major concern is how the report will be implemented. The US Navy has funded a report which indicates that as military robots become more complex, there should be greater attention to implications of their ability to make autonomous decisions. Some researchers state that autonomous robots might be more humane, as they could make decisions more effectively.Within this last decade, there has been intensive research in autonomous power with the ability to learn using assigned moral responsibilities. ""The results may be used when designing future military robots, to control unwanted tendencies to assign responsibility to the robots."" From a consequentialist view, there is a chance that robots will develop the ability to make their own logical decisions on whom to kill and that is why there should be a set moral framework that the AI cannot override.There has been a recent outcry with regard to the engineering of artificial intelligence weapons that have included ideas of a robot takeover of mankind. AI weapons do present a type of danger different from that of human-controlled weapons. Many governments have begun to fund programs to develop AI weaponry. The United States Navy recently announced plans to develop autonomous drone weapons, paralleling similar announcements by Russia and South Korea respectively. Due to the potential of AI weapons becoming more dangerous than human-operated weapons, Stephen Hawking and Max Tegmark signed a ""Future of Life"" petition to ban AI weapons. The message posted by Hawking and Tegmark states that AI weapons pose an immediate danger and that action is required to avoid catastrophic disasters in the near future.""If any major military power pushes ahead with the AI weapon development, a global arms race is virtually inevitable, and the endpoint of this technological trajectory is obvious: autonomous weapons will become the Kalashnikovs of tomorrow"", says the petition, which includes Skype co-founder Jaan Tallinn and MIT professor of linguistics Noam Chomsky as additional supporters against AI weaponry.Physicist and Astronomer Royal Sir Martin Rees has warned of catastrophic instances like ""dumb robots going rogue or a network that develops a mind of its own."" Huw Price, a colleague of Rees at Cambridge, has voiced a similar warning that humans might not survive when intelligence ""escapes the constraints of biology"". These two professors created the Centre for the Study of Existential Risk at Cambridge University in the hope of avoiding this threat to human existence.Regarding the potential for smarter-than-human systems to be employed militarily, the Open Philanthropy Project writes that these scenarios ""seem potentially as important as the risks related to loss of control"", but research investigating AI's long-run social impact have spent relatively little time on this concern: ""this class of scenarios has not been a major focus for the organizations that have been most active in this space, such as the Machine Intelligence Research Institute (MIRI) and the Future of Humanity Institute (FHI), and there seems to have been less analysis and debate regarding them"".",673
Ethics of artificial intelligence,Opaque algorithms,"Approaches like machine learning with neural networks can result in computers making decisions that they and the humans who programmed them cannot explain. It is difficult for people to determine if such decisions are fair and trustworthy, leading potentially to bias in AI systems going undetected, or people rejecting the use of such systems. This has led to advocacy and in some jurisdictions legal requirements for explainable artificial intelligence. Explainable artificial intelligence encompasses both explainability and interpretability, with explainability relating to summarizing neural network behavior and building user confidence, while interpretability is defined as the comprehension of what a model has done or could do.",126
Ethics of artificial intelligence,Singularity,"Many researchers have argued that, by way of an ""intelligence explosion"", a self-improving AI could become so powerful that humans would not be able to stop it from achieving its goals. In his paper ""Ethical Issues in Advanced Artificial Intelligence"" and subsequent book Superintelligence: Paths, Dangers, Strategies, philosopher Nick Bostrom argues that artificial intelligence has the capability to bring about human extinction. He claims that general superintelligence would be capable of independent initiative and of making its own plans, and may therefore be more appropriately thought of as an autonomous agent. Since artificial intellects need not share our human motivational tendencies, it would be up to the designers of the superintelligence to specify its original motivations. Because a superintelligent AI would be able to bring about almost any possible outcome and to thwart any attempt to prevent the implementation of its goals, many uncontrolled unintended consequences could arise. It could kill off all other agents, persuade them to change their behavior, or block their attempts at interference.However, instead of overwhelming the human race and leading to our destruction, Bostrom has also asserted that superintelligence can help us solve many difficult problems such as disease, poverty, and environmental destruction, and could help us to ""enhance"" ourselves.The sheer complexity of human value systems makes it very difficult to make AI's motivations human-friendly. Unless moral philosophy provides us with a flawless ethical theory, an AI's utility function could allow for many potentially harmful scenarios that conform with a given ethical framework but not ""common sense"". According to Eliezer Yudkowsky, there is little reason to suppose that an artificially designed mind would have such an adaptation. AI researchers such as Stuart J. Russell, Bill Hibbard, Roman Yampolskiy, Shannon Vallor, Steven Umbrello and Luciano Floridi have proposed design strategies for developing beneficial machines.",380
Ethics of artificial intelligence,Actors in AI ethics,"There are many organisations concerned with AI ethics and policy, public and governmental as well as corporate and societal.
Amazon, Google, Facebook, IBM, and Microsoft have established a non-profit, The Partnership on AI to Benefit People and Society, to formulate best practices on artificial intelligence technologies, advance the public's understanding, and to serve as a platform about artificial intelligence. Apple joined in January 2017. The corporate members will make financial and research contributions to the group, while engaging with the scientific community to bring academics onto the board.The IEEE put together a Global Initiative on Ethics of Autonomous and Intelligent Systems which has been creating and revising guidelines with the help of public input, and accepts as members many professionals from within and without its organization.
Traditionally, government has been used by societies to ensure ethics are observed through legislation and policing. There are now many efforts by national governments, as well as transnational government and non-government organizations to ensure AI is ethically applied.",203
Ethics of artificial intelligence,Intergovernmental initiatives,"The European Commission has a High-Level Expert Group on Artificial Intelligence. On 8 April 2019, this published its ""Ethics Guidelines for Trustworthy Artificial Intelligence"". The European Commission also has a Robotics and Artificial Intelligence Innovation and Excellence unit, which published a white paper on excellence and trust in artificial intelligence innovation on 19 February 2020. The European Commission also proposed the Artificial Intelligence Act.
The OECD established an OECD AI Policy Observatory.
In 2021, UNESCO adopted the Recommendation on the Ethics of Artificial Intelligence, the first global standard on the ethics of AI.",114
Ethics of artificial intelligence,Governmental initiatives,"In the United States the Obama administration put together a Roadmap for AI Policy. The Obama Administration released two prominent white papers on the future and impact of AI. In 2019 the White House through an executive memo known as the ""American AI Initiative"" instructed NIST the (National Institute of Standards and Technology) to begin work on Federal Engagement of AI Standards (February 2019).
In January 2020, in the United States, the Trump Administration released a draft executive order issued by the Office of Management and Budget (OMB) on ""Guidance for Regulation of Artificial Intelligence Applications"" (""OMB AI Memorandum""). The order emphasizes the need to invest in AI applications, boost public trust in AI, reduce barriers for usage of AI, and keep American AI technology competitive in a global market. There is a nod to the need for privacy concerns, but no further detail on enforcement. The advances of American AI technology seems to be the focus and priority. Additionally, federal entities are even encouraged to use the order to circumnavigate any state laws and regulations that a market might see as too onerous to fulfill.
The Computing Community Consortium (CCC) weighed in with a 100-plus page draft report – A 20-Year Community Roadmap for Artificial Intelligence Research in the US
The Center for Security and Emerging Technology advises US policymakers on the security implications of emerging technologies such as AI.
The Non-Human Party is running for election in New South Wales, with policies around granting rights to robots, animals and generally, non-human entities whose intelligence has been overlooked.
In Russia, the first-ever Russian ""Codex of ethics of artificial intelligence"" for business was signed in 2021. It was driven by Analytical Center for the Government of the Russian Federation together with major commercial and academic institutions such as Sberbank, Yandex, Rosatom, Higher School of Economics, Moscow Institute of Physics and Technology, ITMO University, Nanosemantics, Rostelecom, CIAN and others.",408
Ethics of artificial intelligence,Academic initiatives,"There are three research institutes at the University of Oxford that are centrally focused on AI ethics. The Future of Humanity Institute that focuses both on AI Safety and the Governance of AI. The Institute for Ethics in AI, directed by John Tasioulas, whose primary goal, among others, is to promote AI ethics as a field proper in comparison to related applied ethics fields. The Oxford Internet Institute, directed by Luciano Floridi, focuses on the ethics of near-term AI technologies and ICTs.
The Centre for Digital Governance at the Hertie School in Berlin was co-founded by Joanna Bryson to research questions of ethics and technology.
The AI Now Institute at NYU is a research institute studying the social implications of artificial intelligence. Its interdisciplinary research focuses on the themes bias and inclusion, labour and automation, rights and liberties, and safety and civil infrastructure.
The Institute for Ethics and Emerging Technologies (IEET) researches the effects of AI on unemployment, and policy.
The Institute for Ethics in Artificial Intelligence (IEAI) at the Technical University of Munich directed by Christoph Lütge conducts research across various domains such as mobility, employment, healthcare and sustainability.
Barbara J. Grosz, the Higgins Professor of Natural Sciences at the Harvard John A. Paulson School of Engineering and Applied Sciences has initiated the Embedded EthiCS into Harvard's computer science curriculum to develop a future generation of computer scientists with worldview that takes into account the social impact of their work.",311
Ethics of artificial intelligence,NGO initiatives,"An international non-profit organization Future of Life Institute held a 5 day conference in Asilomar in 2017 on the subject of ""Beneficial AI"", the outcome of which was a set of 23 guiding principles for the future of AI research. Through a shared vision between experts and thought leaders from variety of disciplines, this conference laid an influential groundwork for AI governance principals in addressing research issues, ethics and values, and long-term issues.",91
Ethics of artificial intelligence,Private organizations,"Algorithmic Justice League
Black in AI
Data for Black Lives
Queer in AI",22
Ethics of artificial intelligence,Role and impact of fiction,"The role of fiction with regards to AI ethics has been a complex one. One can distinguish three levels at which fiction has impacted the development of artificial intelligence and robotics: Historically, fiction has been prefiguring common tropes that have not only influenced goals and visions for AI, but also outlined ethical questions and common fears associated with it. During the second half of the twentieth and the first decades of the twenty-first century, popular culture, in particular movies, TV series and video games have frequently echoed preoccupations and dystopian projections around ethical questions concerning AI and robotics. Recently, these themes have also been increasingly treated in literature beyond the realm of science fiction. And, as Carme Torras, research professor at the Institut de Robòtica i Informàtica Industrial (Institute of robotics and industrial computing) at the Technical University of Catalonia notes, in higher education, science fiction is also increasingly used for teaching technology-related ethical issues in technological degrees.",201
Ethics of artificial intelligence,History,"Historically speaking, the investigation of moral and ethical implications of ""thinking machines"" goes back at least to the Enlightenment: Leibniz already poses the question if we might attribute intelligence to a mechanism that behaves as if it were a sentient being, and so does Descartes, who describes what could be considered an early version of the Turing test.The romantic period has several times envisioned artificial creatures that escape the control of their creator with dire consequences, most famously in Mary Shelley's Frankenstein. The widespread preoccupation with industrialization and mechanization in the 19th and early 20th century, however, brought ethical implications of unhinged technical developments to the forefront of fiction: R.U.R – Rossum's Universal Robots, Karel Čapek's play of sentient robots endowed with emotions used as slave labor is not only credited with the invention of the term 'robot' (derived from the Czech word for forced labor, robota) but was also an international success after it premiered in 1921. George Bernard Shaw's play Back to Methuselah, published in 1921, questions at one point the validity of thinking machines that act like humans; Fritz Lang's 1927 film Metropolis shows an android leading the uprising of the exploited masses against the oppressive regime of a technocratic society.",266
Ethics of artificial intelligence,Impact on technological development,"While the anticipation of a future dominated by potentially indomitable technology has fueled the imagination of writers and film makers for a long time, one question has been less frequently analyzed, namely, to what extent fiction has played a role in providing inspiration for technological development. It has been documented, for instance, that the young Alan Turing saw and appreciated aforementioned Shaw's play Back to Methuselah in 1933 (just 3 years before the publication of his first seminal paper, which laid the groundwork for the digital computer), and he would likely have been at least aware of plays like R.U.R., which was an international success and translated into many languages.
One might also ask the question which role science fiction played in establishing the tenets and ethical implications of AI development: Isaac Asimov conceptualized his Three Laws of Robotics in the 1942 short story  ""Runaround"", part of the short story collection  I, Robot; Arthur C. Clarke's short The Sentinel, on which Stanley Kubrick's film 2001: A Space Odyssey is based, was written in 1948 and published in 1952. Another example (among many others) would be Philip K. Dick's numerous short stories and novels – in particular Do Androids Dream of Electric Sheep?, published in 1968, and featuring its own version of a Turing Test, the Voight-Kampff Test, to gauge emotional responses of androids indistinguishable from humans. The novel later became the basis of the influential 1982 movie Blade Runner by Ridley Scott.
Science fiction has been grappling with ethical implications of AI developments for decades, and thus provided a blueprint for ethical issues that might emerge once something akin to general artificial intelligence has been achieved: Spike Jonze's 2013 film Her shows what can happen if a user falls in love with the seductive voice of his smartphone operating system; Ex Machina, on the other hand, asks a more difficult question: if confronted with a clearly recognizable machine, made only human by a face and an empathetic and sensual voice, would we still be able to establish an emotional connection, still be seduced by it?  (The film echoes a theme already present two centuries earlier, in the 1817 short story The Sandmann by E. T. A. Hoffmann.)
The theme of coexistence with artificial sentient beings is also the theme of two recent novels: Machines Like Me by Ian McEwan, published in 2019, involves, among many other things, a love-triangle involving an artificial person as well as a human couple. Klara and the Sun by Nobel Prize winner Kazuo Ishiguro, published in 2021, is the first-person account of Klara, an 'AF' (artificial friend), who is trying, in her own way, to help the girl she is living with, who, after having been 'lifted' (i.e. having been subjected to genetic enhancements), is suffering from a strange illness.",597
Ethics of artificial intelligence,TV series,"While ethical questions linked to AI have been featured in science fiction literature and feature films for decades, the emergence of the TV series as a genre allowing for longer and more complex story lines and character development has led to some significant contributions that deal with ethical implications of technology. The Swedish series Real Humans (2012–2013) tackled the complex ethical and social consequences linked to the integration of artificial sentient beings in society. The British dystopian science fiction anthology series Black Mirror (2013–2019) was particularly notable for experimenting with dystopian fictional developments linked to a wide variety of recent technology developments. Both the French series Osmosis (2020) and British series The One deal with the question of what can happen if technology tries to find the ideal partner for a person. Several episodes of the Netflix series Love, Death+Robots have imagined scenes of robots and humans living together. The most representative one of them is S02 E01, it shows how bad the consequences can be when robots get out of control if humans rely too much on them in their lives.",211
Ethics of artificial intelligence,Future visions in fiction and games,"The movie The Thirteenth Floor suggests a future where simulated worlds with sentient inhabitants are created by computer game consoles for the purpose of entertainment. The movie The Matrix suggests a future where the dominant species on planet Earth are sentient machines and humanity is treated with utmost speciesism. The short story ""The Planck Dive"" suggests a future where humanity has turned itself into software that can be duplicated and optimized and the relevant distinction between types of software is sentient and non-sentient. The same idea can be found in the Emergency Medical Hologram of Starship Voyager, which is an apparently sentient copy of a reduced subset of the consciousness of its creator, Dr. Zimmerman, who, for the best motives, has created the system to give medical assistance in case of emergencies. The movies Bicentennial Man and A.I. deal with the possibility of sentient robots that could love. I, Robot explored some aspects of Asimov's three laws. All these scenarios try to foresee possibly unethical consequences of the creation of sentient computers.The ethics of artificial intelligence is one of several core themes in BioWare's Mass Effect series of games. It explores the scenario of a civilization accidentally creating AI through a rapid increase in computational power through a global scale neural network. This event caused an ethical schism between those who felt bestowing organic rights upon the newly sentient Geth was appropriate and those who continued to see them as disposable machinery and fought to destroy them. Beyond the initial conflict, the complexity of the relationship between the machines and their creators is another ongoing theme throughout the story.
Detroit: Become Human is one of the most famous video games which discusses the ethics of artificial intelligence recently. Quantic Dream designed the chapters of the game using interactive storylines to give players a more immersive gaming experience. Players manipulate three different awakened bionic people in the face of different events to make different choices to achieve the purpose of changing the human view of the bionic group and different choices will result in different endings. This is one of the few games that puts players in the bionic perspective, which allows them to better consider the rights and interests of robots once a true artificial intelligence is created.Over time, debates have tended to focus less and less on possibility and more on desirability, as emphasized in the ""Cosmist"" and ""Terran"" debates initiated by Hugo de Garis and Kevin Warwick. A Cosmist, according to Hugo de Garis, is actually seeking to build more intelligent successors to the human species.
Experts at the University of Cambridge have argued that AI is portrayed in fiction and nonfiction overwhelmingly as racially White, in ways that distort perceptions of its risks and benefits.",543
Ethics of artificial intelligence,Researchers,,2
Ethics of artificial intelligence,Organisations,,2
Symbolic artificial intelligence,Summary,"In artificial intelligence, symbolic artificial intelligence is the term for the collection of all methods in artificial intelligence research that are based on high-level symbolic (human-readable) representations of problems, logic and search. Symbolic AI used tools such as logic programming, production rules, semantic nets and frames, and it developed applications such as knowledge-based systems (in particular, expert systems),  symbolic mathematics, automated theorem provers, ontologies, the semantic web, and automated planning and scheduling systems. The Symbolic AI paradigm led to seminal ideas in search, symbolic programming languages, agents, multi-agent systems, the semantic web, and the strengths and limitations of formal knowledge and reasoning systems.
Symbolic AI was the dominant paradigm of AI research from the mid-1950s until the mid-1990s. 
Researchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine with artificial general intelligence and considered this the ultimate goal of their field. An early boom, with early successes such as the Logic Theorist and Samuel's Checkers Playing Program, led to unrealistic expectations and promises and was followed by the First AI Winter as funding dried up. A second boom (1969–1986) occurred with the rise of expert systems, their promise of capturing corporate expertise, and an enthusiastic corporate embrace. That boom, and some early successes, e.g., with XCON at DEC, was followed again by later disappointment. Problems with difficulties in knowledge acquisition, maintaining large knowledge bases, and brittleness in handling out-of-domain problems arose. Another, second, AI Winter (1988–2011) followed. Subsequently, AI researchers focused on addressing underlying problems in handling uncertainty and in knowledge acquisition. Uncertainty was addressed with formal methods such as hidden Markov models, Bayesian reasoning, and statistical relational learning. Symbolic machine learning addressed the knowledge acquisition problem with contributions including Version Space, Valiant's PAC learning, Quinlan's ID3 decision-tree learning, case-based learning, and inductive logic programming to learn relations.Neural networks, a subsymbolic approach, had been pursued from early days and was to reemerge strongly in 2012.  Early examples are Rosenblatt's perceptron learning work, the backpropagation work of Rumelhart, Hinton and Williams, and work in convolutional neural networks by LeCun et al. in 1989. However, neural networks were not viewed as successful until about 2012: ""Until Big Data became commonplace, the general consensus in the Al community was that the so-called neural-network approach was hopeless. Systems just didn't work that well, compared to other methods. ... A revolution came in 2012, when a number of people, including a team of researchers working with Hinton, worked out a way to use the power of GPUs to enormously increase the power of neural networks."" Over the next several years, deep learning had spectacular success in handling vision, speech recognition, speech synthesis, image generation, and machine translation. However, since 2020, as inherent difficulties with bias, explanation, comprehensibility, and robustness became more apparent with deep learning approaches; an increasing number of AI researchers have called for combining the best of both the symbolic and neural network approaches and addressing areas that both approaches have difficulty with, such as common-sense reasoning.",685
Symbolic artificial intelligence,History,"A short history of symbolic AI to the present day follows below. Time periods and titles are drawn from Henry Kautz's 2020 AAAI Robert S. Engelmore Memorial Lecture and the longer Wikipedia article on the History of AI, with dates and titles differing slightly for increased clarity.",60
Symbolic artificial intelligence,"The first AI summer: irrational exuberance, 1948–1966","Success at early attempts in AI occurred in three main areas: artificial neural networks, knowledge representation, and heuristic search, contributing to high expectations. This section summarizes Kautz's reprise of early AI history.",55
Symbolic artificial intelligence,Approaches inspired by human or animal cognition or behavior,"Cybernetic approaches attempted to replicate the feedback loops between animals and their environments. A robotic turtle, with sensors, motors for driving and steering, and seven vacuum tubes for control, based on a preprogrammed neural net, was built as early as 1948. This work can be seen as an early precursor to later work in neural networks, reinforcement learning, and situated robotics.An important early symbolic AI program was the Logic theorist, written by Allen Newell, Herbert Simon and Cliff Shaw in 1955–56, as it was able to prove 38 elementary theorems from Whitehead and Russell's Principia Mathematica. Newell, Simon, and Shaw later generalized this work to create a domain-independent problem solver, GPS (General Problem Solver). GPS solved problems represented with formal operators via state-space search using means-ends analysis.During the 1960s, symbolic approaches achieved great success at simulating intelligent behavior in structured environments such as game-playing, symbolic mathematics, and theorem-proving. AI research was centered in three institutions in the 1960s: Carnegie Mellon University, Stanford, MIT and (later) University of Edinburgh. Each one developed its own style of research. Earlier approaches based on cybernetics or artificial neural networks were abandoned or pushed into the background.
Herbert Simon and Allen Newell studied human problem-solving skills and attempted to formalize them, and their work laid the foundations of the field of artificial intelligence, as well as cognitive science, operations research and management science. Their research team used the results of psychological experiments to develop programs that simulated the techniques that people used to solve problems. This tradition, centered at Carnegie Mellon University would eventually culminate in the development of the Soar architecture in the middle 1980s.",366
Symbolic artificial intelligence,Heuristic search,"In addition to the highly-specialized domain-specific kinds of knowledge that we will see later used in expert systems, early symbolic AI researchers discovered another more general application of knowledge. These were called heuristics, rules of thumb that guide a search in promising directions: ""How can non-enumerative search be practical when the underlying problem is exponentially hard? The approach advocated by Simon and Newell is to employ heuristics: fast algorithms that may fail on some inputs or output suboptimal solutions."" Another important advance was to find a way to apply these heuristics that guarantees a solution will be found, if there is one, not withstanding the occasional fallibility of heuristics: ""The A* algorithm provided a general frame for complete and optimal heuristically guided search. A* is used as a subroutine within practically every AI algorithm today but is still no magic bullet; its guarantee of completeness is bought at the cost of worst-case exponential time.",203
Symbolic artificial intelligence,Early work on knowledge representation and reasoning,"Early work covered both applications of formal reasoning emphasizing first-order logic, along with attempts to handle common-sense reasoning in a less formal manner.",37
Symbolic artificial intelligence,"Modeling formal reasoning with logic: the ""neats""","Unlike Simon and Newell, John McCarthy felt that machines did not need to simulate the exact mechanisms of human thought, but could instead try to find the essence of abstract reasoning and problem-solving with logic, regardless of whether people used the same algorithms.
His laboratory at Stanford (SAIL) focused on using formal logic to solve a wide variety of problems, including knowledge representation, planning and learning.
Logic was also the focus of the work at the University of Edinburgh and elsewhere in Europe which led to the development of the programming language Prolog and the science of logic programming.",128
Symbolic artificial intelligence,"Modeling implicit common-sense knowledge with frames and scripts: the ""scruffies""","Researchers at MIT (such as Marvin Minsky and Seymour Papert) found that solving difficult problems in vision and natural language processing required ad hoc solutions—they argued that no simple and general principle (like logic) would capture all the aspects of intelligent behavior. Roger Schank described their ""anti-logic"" approaches as ""scruffy"" (as opposed to the ""neat"" paradigms at CMU and Stanford).Commonsense knowledge bases (such as Doug Lenat's Cyc) are an example of ""scruffy"" AI, since they must be built by hand, one complicated concept at a time.",142
Symbolic artificial intelligence,"The first AI winter: crushed dreams, 1967–1977","The first AI winter was a shock:

During the first AI summer, many people thought that machine intelligence could be achieved in just a few years. The Defense Advance Research Projects Agency (DARPA) launched programs to support AI research with the goal of using AI to solve problems of national security; in particular, to automate the translation of Russian to English for intelligence operations and to create autonomous tanks for the battlefield. Researchers had begun to realize that achieving AI was going to be much harder than was supposed a decade earlier, but a combination of hubris and disingenuousness led many university and think-tank researchers to accept funding with promises of deliverables that they should have known they could not fulfill. By the mid-1960s neither useful natural language translation systems nor autonomous tanks had been created, and a dramatic backlash set in. New DARPA leadership canceled existing AI funding programs. 
...

Outside of the United States, the most fertile ground for AI research was the United Kingdom. The AI winter in the United Kingdom was spurred on not so much by disappointed military leaders as by rival academics who viewed AI researchers as charlatans and a drain on research funding. A professor of applied mathematics, Sir James Lighthill, was commissioned by Parliament to evaluate the state of AI research in the nation. The report stated that all of the problems being worked on in AI would be better handled by researchers from other disciplines—such as applied mathematics. The report also claimed that AI successes on toy problems could never scale to real-world applications due to combinatorial explosion.",326
Symbolic artificial intelligence,"The second AI summer: knowledge is power, 1978–1987",,10
Symbolic artificial intelligence,Knowledge-based systems,"As limitations with weak, domain-independent methods became more and more apparent, researchers from all three traditions began to build knowledge into AI applications. The knowledge revolution was driven by the realization that knowledge underlies high-performance, domain-specific AI applications.
Edward Feigenbaum said:

""In the knowledge lies the power.""to describe that high-performance in a specific domain required both general and highly domain-specific knowledge. Ed Feigenbaum and Doug Lenat called this The Knowledge Principle: 

(1) The Knowledge Principle: if a program is to perform a complex task well, it must know a great deal about the world in which it operates.(2) A plausible extension of that principle, called the Breadth Hypothesis: there are two additional abilities necessary for intelligent behavior in unexpected situations: falling back on increasingly general knowledge, and analogizing to specific but far-flung knowledge.",188
Symbolic artificial intelligence,Success with expert systems,"This ""knowledge revolution"" led to the development and deployment of expert systems (introduced by Edward Feigenbaum), the first commercially successful form of AI software.",37
Symbolic artificial intelligence,Examples,"Key expert systems were:

DENDRAL, which found the structure of organic molecules from their chemical formula and mass spectrometer readings.
MYCIN, which diagnosed bacteremia – and suggested further lab tests, when necessary – by interpreting lab results, patient history, and doctor observations. ""With about 450 rules, MYCIN was able to perform as well as some experts, and considerably better than junior doctors.""
INTERNIST and CADUCEUS which tackled internal medicine diagnosis. Internist attempted to capture the expertise of the chairman of internal medicine at the University of Pittsburgh School of Medicine while CADUCEUS could eventually diagnose up to 1000 different diseases.
GUIDON, which showed how a knowledge base built for expert problem solving could be repurposed for teaching.
XCON, to configure VAX computers, a then laborious process that could take up to 90 days. XCON reduced the time to about 90 minutes.DENDRAL is considered the first expert system that relied on knowledge-intensive problem-solving. It is described below, by Ed Feigenbaum, from a Communications of the ACM interview, Interview with Ed Feigenbaum:

One of the people at Stanford interested in computer-based models of mind was Joshua Lederberg, the 1958 Nobel Prize winner in genetics. When I told him I wanted an induction ""sandbox"", he said, ""I have just the one for you."" His lab was doing mass spectrometry of amino acids. The question was: how do you go from looking at a spectrum of an amino acid to the chemical structure of the amino acid? That's how we started the DENDRAL Project: I was good at heuristic search methods, and he had an algorithm which was good at generating the chemical problem space.
We did not have a grandiose vision. We worked bottom up. Our chemist was Carl Djerassi, inventor of the chemical behind the birth control pill, and also one of the world's most respected mass spectrometrists. Carl and his postdocs were world-class experts in mass spectrometry. We began to add in their knowledge, inventing knowledge engineering as we were going along. These experiments amounted to titrating into DENDRAL more and more knowledge. The more you did that, the smarter the program became. We had very good results.

The generalization was: in the knowledge lies the power. That was the big idea. In my career that is the huge, ""Ah ha!,"" and it wasn't the way AI was being done previously. Sounds simple, but it's probably AI's most powerful generalization.
The other expert systems mentioned above came after DENDRAL. MYCIN exemplifies the classic expert system architecture of a knowledge-base of rules coupled to a symbolic reasoning mechanism, including the use of certainty factors to handle uncertainty. GUIDON shows how an explicit knowledge base can be repurposed for a second application, tutoring, and is an example of an intelligent tutoring system, a particular kind of knowledge-based application. Clancey showed that it was not sufficient simply to use MYCIN's rules for instruction, but that he also needed to add rules for dialogue management and student modeling. XCON is significant because of the millions of dollars it saved DEC, which triggered the expert system boom where most all major corporations in the US had expert systems groups, with the aim to capture corporate expertise, preserve it, and automate it:

By 1988, DEC's AI group had 40 expert systems deployed, with more on the way. DuPont had 100 in use and 500 in development. Nearly every major U.S. corporation had its own Al group and was either using or investigating expert systems.
Chess expert knowledge was encoded in Deep Blue. In 1996, this allowed IBM's Deep Blue, with the help of symbolic AI, to win in a game of chess against the world champion at that time, Garry Kasparov.",821
Symbolic artificial intelligence,Architecture of knowledge-based and expert systems,"A key component of the system architecture for all expert systems is the knowledge base, which stores facts and rules for problem-solving.
The simplest approach for an expert system knowledge base is simply a collection or network of production rules. Production rules connect symbols in a relationship similar to an If-Then statement. The expert system processes the rules to make deductions and to determine what additional information it needs, i.e. what questions to ask, using human-readable symbols. For example, OPS5, CLIPS and their successors Jess and Drools operate in this fashion.
Expert systems can operate in either a forward chaining – from evidence to conclusions – or backward chaining – from goals to needed data and prerequisites – manner. More advanced knowledge-based systems, such as Soar can also perform meta-level reasoning, that is reasoning about their own reasoning in terms of deciding how to solve problems and monitoring the success of problem-solving strategies. 
Blackboard systems are a second kind of knowledge-based or expert system architecture. They model a community of experts incrementally contributing, where they can, to solve a problem. The problem is represented in multiple levels of abstraction or alternate views. The experts (knowledge sources) volunteer their services whenever they recognize they can make a contribution. Potential problem-solving actions are represented on an agenda that is updated as the problem situation changes. A controller decides how useful each contribution is, and who should make the next problem-solving action. One example, the BB1 blackboard architecture was originally inspired by studies of how humans plan to perform multiple tasks in a trip. An innovation of BB1 was to apply the same blackboard model to solving its own control problem, i.e., its controller performed meta-level reasoning with knowledge sources that monitored how well a plan or the problem-solving was proceeding, and could switch from one strategy to another as conditions – such as goals or times – changed. BB1 was applied in multiple domains: construction site planning, intelligent tutoring systems, and real-time patient monitoring.",430
Symbolic artificial intelligence,"The second AI winter, 1988–1993","At the height of the AI boom, companies such as Symbolics, LMI, and Texas Instruments were selling LISP machines specifically targeted to accelerate the development of AI applications and research. In addition, several artificial intelligence companies, such as Teknowledge and Inference Corporation, were selling expert system shells, training, and consulting to corporations. 
Unfortunately, the AI boom did not last and Kautz best describes the second AI winter that followed:

Many reasons can be offered for the arrival of the second AI winter. The hardware companies failed when much more cost-effective general Unix workstations from Sun together with good compilers for LISP and Prolog came onto the market. Many commercial deployments of expert systems were discontinued when they proved too costly to maintain. Medical expert systems never caught on for several reasons: the difficulty in keeping them up to date; the challenge for medical professionals to learn how to use a bewildering variety of different expert systems for different medical conditions; and perhaps most crucially, the reluctance of doctors to trust a computer-made diagnosis over their gut instinct, even for specific domains where the expert systems could outperform an average doctor. Venture capital money deserted AI practically overnight. The world AI conference IJCAI hosted an enormous and lavish trade show and thousands of nonacademic attendees in 1987 in Vancouver; the main AI conference the following year, AAAI 1988 in St. Paul, was a small and strictly academic affair.",303
Symbolic artificial intelligence,"Adding in more rigorous foundations, 1993–2011",,7
Symbolic artificial intelligence,Uncertain reasoning,"Both statistical approaches and extensions to logic were tried. 
One statistical approach, Hidden Markov Models, had already been popularized in the 1980s for speech recognition work. Subsequently, in 1988, Judea Pearl popularized the use of Bayesian Networks as a sound but efficient way of handling uncertain reasoning with his publication of the book Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. and Bayesian approaches were applied successfully in expert systems. Even later, in the 1990s, statistical relational learning, an approach that combines probability with logical formulas, allowed probability to be combined with first-order logic, e.g., with either Markov Logic Networks or Probabilistic Soft Logic.
Other, non-probabilistic extensions to first-order logic to support were also tried. For example, non-monotonic reasoning could be used with truth maintenance systems. A truth maintenance system tracked assumptions and justifications for all inferences. It allowed inferences to be withdrawn when assumptions were found out to be incorrect or a contradiction was derived. Explanations could be provided for an inference by explaining which rules were applied to create it and then continuing through underlying inferences and rules all the way back to root assumptions. Lofti Zadeh had introduced a different kind of extension to handle the representation of vagueness. For example, in deciding how ""heavy"" or ""tall"" a man is, there is frequently no clear ""yes"" or ""no"" answer, and a predicate for heavy or tall would instead return values between 0 and 1. Those values represented to what degree the predicates were true. His fuzzy logic further provided a means for propagating combinations of these values through logical formulas.",352
Symbolic artificial intelligence,Machine learning,"Symbolic machine learning approaches were investigated to address the knowledge acquisition bottleneck. One of the earliest is Meta-DENDRAL. Meta-DENDRAL used a generate-and-test technique to generate plausible rule hypotheses to test against spectra. Domain and task knowledge reduced the number of candidates tested to a manageable size. Feigenbaum described Meta-DENDRAL as

...the culmination of my dream of the early to mid-1960s having to do with theory formation. The conception was that you had a problem solver like DENDRAL that took some inputs and produced an output. In doing so, it used layers of knowledge to steer and prune the search. That knowledge got in there because we interviewed people. But how did the people get the knowledge? By looking at thousands of spectra. So we wanted a program that would look at thousands of spectra and infer the knowledge of mass spectrometry that DENDRAL could use to solve individual hypothesis formation problems.
We did it. We were even able to publish new knowledge of mass spectrometry in the Journal of the American Chemical Society, giving credit only in a footnote that a program, Meta-DENDRAL, actually did it. We were able to do something that had been a dream: to have a computer program come up with a new and publishable piece of science.
In contrast to the knowledge-intensive approach of Meta-DENDRAL, Ross Quinlan invented a domain-independent approach to statistical classification, decision tree learning, starting first with ID3 and then later extending its capabilities to C4.5. The decision trees created are glass box, interpretable classifiers, with human-interpretable classification rules. 
Advances were made in understanding machine learning theory, too. Tom Mitchell introduced version space learning which describes learning as search through a space of hypotheses, with upper, more general, and lower, more specific, boundaries encompassing all viable hypotheses consistent with the examples seen so far. More formally, Valiant introduced Probably Approximately Correct Learning (PAC Learning), a framework for the mathematical analysis of machine learning.Symbolic machine learning encompassed more than learning by example. E.g., John Anderson provided a cognitive model of human learning where skill practice results in a compilation of rules from a declarative format to a procedural format with his ACT-R cognitive architecture. For example, a student might learn to apply ""Supplementary angles are two angles whose measures sum 180 degrees"" as several different procedural rules. E.g., one rule might say that if X and Y are supplementary and you know X, then Y will be 180 - X. He called his approach ""knowledge compilation"". ACT-R has been used successfully to model aspects of human cognition, such as learning and retention. ACT-R is also used in intelligent tutoring systems, called cognitive tutors, to successfully teach geometry, computer programming, and algebra to school children.Inductive logic programming was another approach to learning that allowed logic programs to be synthesized from input-output examples. E.g., Ehud Shapiro's MIS (Model Inference System) could synthesize Prolog programs from examples. John R. Koza applied genetic algorithms to program synthesis to create genetic programming, which he used to synthesize LISP programs. Finally, Zohar Manna and Richard Waldinger provided a more general approach to program synthesis that synthesizes a functional program in the course of proving its specifications to be correct.As an alternative to logic, Roger Schank introduced case-based reasoning (CBR). The CBR approach outlined in his book, Dynamic Memory, focuses first on remembering key problem-solving cases for future use and generalizing them where appropriate. When faced with a new problem, CBR retrieves the most similar previous case and adapts it to the specifics of the current problem. Another alternative to logic, genetic algorithms and genetic programming are based on an evolutionary model of learning, where sets of rules are encoded into populations, the rules govern the behavior of individuals, and selection of the fittest prunes out sets of unsuitable rules over many generations.Symbolic machine learning was applied to learning concepts, rules, heuristics, and problem-solving. Approaches, other than those above, include:

Learning from instruction or advice—i.e., taking human instruction, posed as advice, and determining how to operationalize it in specific situations. For example, in a game of Hearts, learning exactly how to play a hand to ""avoid taking points.""
Learning from exemplars—improving performance by accepting subject-matter expert (SME) feedback during training. When problem-solving fails, querying the expert to either learn a new exemplar for problem-solving or to learn a new explanation as to exactly why one exemplar is more relevant than another. For example, the program Protos learned to diagnose tinnitus cases by interacting with an audiologist.
Learning by analogy—constructing problem solutions based on similar problems seen in the past, and then modifying their solutions to fit a new situation or domain.
Apprentice learning systems—learning novel solutions to problems by observing human problem-solving. Domain knowledge explains why novel solutions are correct and how the solution can be generalized. LEAP learned how to design VLSI circuits by observing human designers.
Learning by discovery—i.e., creating tasks to carry out experiments and then learning from the results. Doug Lenat's Eurisko, for example, learned heuristics to beat human players at the Traveller role-playing game for two years in a row. 
Learning macro-operators—i.e., searching for useful macro-operators to be learned from sequences of basic problem-solving actions. Good macro-operators simplify problem-solving by allowing problems to be solved at a more abstract level.",1203
Symbolic artificial intelligence,Deep learning and neuro-symbolic AI 2011–now,"With the rise of deep learning, the symbolic AI approach has been compared to deep learning as complementary ""...with parallels having been drawn many times by AI researchers between Kahneman's research on human reasoning and decision making – reflected in his book Thinking, Fast and Slow – and the so-called ""AI systems 1 and 2"", which would in principle be modelled by deep learning and symbolic reasoning, respectively."" In this view, symbolic reasoning is more apt for deliberative reasoning, planning, and explanation while deep learning is more apt for fast pattern recognition in perceptual applications with noisy data.",126
Symbolic artificial intelligence,Neuro-symbolic AI: integrating neural and symbolic approaches,"Neuro-symbolic AI attempts to integrate neural and symbolic architectures in a manner that addresses strengths and weaknesses of each, in a complementary fashion, in order to support robust AI capable of reasoning, learning, and cognitive modeling. As argued by Valiant and many others, the effective construction of rich computational cognitive models demands the combination of sound symbolic reasoning and efficient (machine) learning models. Gary Marcus, similarly, argues that: ""We cannot construct rich cognitive models in an adequate, automated way without the triumvirate of hybrid architecture, rich prior knowledge, and sophisticated techniques for reasoning."", and in particular:
""To build a robust, knowledge-driven approach to AI we must have the machinery of symbol-manipulation in our toolkit. Too much of useful knowledge is abstract to make do without tools that represent and manipulate abstraction, and to date, the only machinery that we know of that can manipulate such abstract knowledge reliably is the apparatus of symbol-manipulation.""Henry Kautz, Francesca Rossi, and Bart Selman have also argued for a synthesis. Their arguments are based on a need to address the two kinds of thinking discussed in Daniel Kahneman's book, Thinking, Fast and Slow. Kahneman describes human thinking as having two components, System 1 and System 2. System 1 is fast, automatic, intuitive and unconscious. System 2 is slower, step-by-step, and explicit. System 1 is the kind used for pattern recognition while System 2 is far better suited for planning, deduction, and deliberative thinking. In this view, deep learning best models the first kind of thinking while symbolic reasoning best models the second kind and both are needed.
Garcez describes research in this area as being ongoing for at least the past twenty years, dating from his 2002 book on neurosymbolic learning systems. A series of workshops on neuro-symbolic reasoning has been held every year since 2005, see http://www.neural-symbolic.org/ for details.
In their 2015 paper, Neural-Symbolic Learning and Reasoning: Contributions and Challenges, Garcez et al. argue that:

The integration of the symbolic and connectionist paradigms of AI has been pursued by a relatively small research community over the last two decades and has yielded several significant results. Over the last decade, neural symbolic systems have been shown capable of overcoming the so-called propositional fixation of neural networks, as McCarthy (1988) put it in response to Smolensky (1988); see also (Hinton, 1990). Neural networks were shown capable of representing modal and temporal logics (d'Avila Garcez and Lamb, 2006) and fragments of first-order logic (Bader, Hitzler, Hölldobler, 2008; d'Avila Garcez, Lamb, Gabbay, 2009). Further, neural-symbolic systems have been applied to a number of problems in the areas of bioinformatics, control engineering, software verification and adaptation, visual intelligence, ontology learning, and computer games.
Approaches for integration are varied. Henry Kautz's taxonomy of neuro-symbolic architectures, along with some examples, follows:

Symbolic Neural symbolic—is the current approach of many neural models in natural language processing, where words or subword tokens are both the ultimate input and output of large language models. Examples include BERT, RoBERTa, and GPT-3.
Symbolic[Neural]—is exemplified by AlphaGo, where symbolic techniques are used to call neural techniques. In this case the symbolic approach is Monte Carlo tree search and the neural techniques learn how to evaluate game positions.
Neural|Symbolic—uses a neural architecture to interpret perceptual data as symbols and relationships that are then reasoned about symbolically.
Neural:Symbolic → Neural—relies on symbolic reasoning to generate or label training data that is subsequently learned by a deep learning model, e.g., to train a neural model for symbolic computation by using a Macsyma-like symbolic mathematics system to create or label examples.
Neural_{Symbolic}—uses a neural net that is generated from symbolic rules. An example is the Neural Theorem Prover, which constructs a neural network from an AND-OR proof tree generated from knowledge base rules and terms. Logic Tensor Networks also fall into this category.
Neural[Symbolic]—allows a neural model to directly call a symbolic reasoning engine, e.g., to perform an action or evaluate a state.Many key research questions remain, such as:

What is the best way to integrate neural and symbolic architectures?
How should symbolic structures be represented within neural networks and extracted from them?
How should common-sense knowledge be learned and reasoned about?
How can abstract knowledge that is hard to encode logically be handled?",1023
Symbolic artificial intelligence,Techniques and contributions,"This section provides an overview of techniques and contributions in an overall context leading to many other, more detailed articles in Wikipedia. Sections on Machine Learning and Uncertain Reasoning are covered earlier in the history section.",45
Symbolic artificial intelligence,AI programming languages,"The key AI programming language in the US during the last symbolic AI boom period was LISP. LISP is the second oldest programming language after FORTRAN and was created in 1958 by John McCarthy. LISP provided the first read-eval-print loop to support rapid program development. Compiled functions could be freely mixed with interpreted functions. Program tracing, stepping, and breakpoints were also provided, along with the ability to change values or functions and continue from breakpoints or errors. It had the first self-hosting compiler, meaning that the compiler itself was originally written in LISP and then ran interpretively to compile the compiler code.
Other key innovations pioneered by LISP that have spread to other programming languages include:

Garbage collection
Dynamic typing
Higher-order functions
Recursion
ConditionalsPrograms were themselves data structures that other programs could operate on, allowing the easy definition of higher-level languages. 
In contrast to the US, in Europe the key AI programming language during that same period was Prolog. Prolog provided a built-in store of facts and clauses that could be queried by a read-eval-print loop. The store could act as a knowledge base and the clauses could act as rules or a restricted form of logic. As a subset of first-order logic Prolog was based on Horn clauses with a closed-world assumption -- any facts not known were considered false -- and a unique name assumption for primitive terms -- e.g., the identifier barack_obama was considered to refer to exactly one object. Backtracking and unification are built-in to Prolog.
Alain Colmerauer and Philippe Roussel are credited as the inventors of Prolog. Prolog is a form of logic programming, which was invented by Robert Kowalski. Its history was also influenced by Carl Hewitt's PLANNER, an assertional database with pattern-directed invocation of methods. For more detail see the section on the origins of Prolog in the PLANNER article.
Prolog is also a kind of declarative programming. The logic clauses that describe programs are directly interpreted to run the programs specified. No explicit series of actions is required, as is the case with imperative programming languages.
Japan championed Prolog for its Fifth Generation Project, intending to build special hardware for high performance. Similarly, LISP machines were built to run LISP, but as the second AI boom turned to bust these companies could not compete with new workstations that could now run LISP or Prolog natively at comparable speeds. See the history section for more detail.
Smalltalk was another influential AI programming language. For example it introduced metaclasses and, along with Flavors and CommonLoops, influenced the Common Lisp Object System, or (CLOS), that is now part of Common Lisp, the current standard Lisp dialect. CLOS is a Lisp-based object-oriented system that allows multiple inheritance, in addition to incremental extensions to both classes and metaclasses, thus providing a run-time meta-object protocol.For other AI programming languages see this list of programming languages for artificial intelligence. Currently, Python, a multi-paradigm programming language, is the most popular programming language, partly due to its extensive package library that supports data science, natural language processing, and deep learning. Python includes a read-eval-print loop, functional elements such as higher-order functions, and object-oriented programming that includes metaclasses.",724
Symbolic artificial intelligence,Search,"Search arises in many kinds of problem solving, including planning, constraint satisfaction, and playing games such as checkers, chess, and go. The best known AI-search tree search algorithms are breadth-first search, depth-first search, A*, and Monte Carlo Search. Key search algorithms for Boolean satisfiability are WalkSAT, conflict-driven clause learning, and the DPLL algorithm. For adversarial search when playing games, alpha-beta pruning, branch and bound, and minimax were early contributions.",107
Symbolic artificial intelligence,Knowledge representation and reasoning,Multiple different approaches to represent knowledge and then reason with those representations have been investigated. Below is a quick overview of approaches to knowledge representation and automated reasoning.,36
Symbolic artificial intelligence,Knowledge representation,"Semantic networks, conceptual graphs, frames, and logic are all approaches to modeling knowledge such as domain knowledge, problem-solving knowledge, and the semantic meaning of language. Ontologies model key concepts and their relationships in a domain. Example ontologies are YAGO, WordNet, and DOLCE. DOLCE is an example of an upper ontology that can be used for any domain while WordNet is a lexical resource that can also be viewed as an ontology. YAGO incorporates WordNet as part of its ontology, to align facts extracted from Wikipedia with WordNet synsets. The Disease Ontology is an example of a medical ontology currently being used. 
Description logic is a logic for automated classification of ontologies and for detecting inconsistent classification data. OWL is a language used to represent ontologies with description logic. Protégé is a ontology editor that can read in OWL ontologies and then check consistency with deductive classifiers such as such as HermiT.First-order logic is more general than description logic. The automated theorem provers discussed below can prove theorems in first-order logic. Horn clause logic is more restricted than first-order logic and is used in logic programming languages such as Prolog. Extensions to first-order logic include temporal logic, to handle time; epistemic logic, to reason about agent knowledge; modal logic, to handle possibility and necessity; and probabilistic logics to handle logic and probability together.",311
Symbolic artificial intelligence,Automatic theorem proving,"Examples of automated theorem provers for first-order logic are:

Prover9
ACL2
VampireProver9 can be used in conjunction with the Mace4 model checker. ACL2 is a theorem prover that can handle proofs by induction and is a descendant of the Boyer-Moore Theorem Prover, also known as Nqthm.",82
Symbolic artificial intelligence,Reasoning in knowledge-based systems,"Knowledge-based systems have an explicit knowledge base, typically of rules, to enhance reusability across domains by separating procedural code and domain knowledge. A separate inference engine processes rules and adds, deletes, or modifies a knowledge store.
Forward chaining inference engines are the most common, and are seen in CLIPS and OPS5. Backward chaining occurs in Prolog, where a more limited logical representation is used, Horn Clauses. Pattern-matching, specifically unification, is used in Prolog.
A more flexible kind of problem-solving occurs when reasoning about what to do next occurs, rather than simply choosing one of the available actions. This kind of meta-level reasoning is used in Soar and in the BB1 blackboard architecture.
Cognitive architectures such as ACT-R may have additional capabilities, such as the ability to compile frequently used knowledge into higher-level chunks.",193
Symbolic artificial intelligence,Commonsense reasoning,"Marvin Minsky first proposed frames as a way of interpreting common visual situations, such as an office, and Roger Schank extended this idea to scripts for common routines, such as dining out. Cyc has attempted to capture useful common-sense knowledge and has ""micro-theories"" to handle particular kinds of domain-specific reasoning.
Qualitative simulation, such as Benjamin Kuipers's QSIM, approximates human reasoning about naive physics, such as what happens when we heat a liquid in a pot on the stove. We expect it to heat and possibly boil over, even though we may not know its temperature, its boiling point, or other details, such as atmospheric pressure.
Similarly, Allen's temporal interval algebra is a simplification of reasoning about time and Region Connection Calculus is a simplification of reasoning about spatial relationships. Both can be solved with constraint solvers.",181
Symbolic artificial intelligence,Constraints and constraint-based reasoning,"Constraint solvers perform a more limited kind of inference than first-order logic. They can simplify sets of spatiotemporal constraints, such as those for RCC or Temporal Algebra, along with solving other kinds of puzzle problems, such as Wordle, Sudoku, cryptarithmetic problems, and so on. Constraint logic programming can be used to solve scheduling problems, for example with constraint handling rules (CHR).",96
Symbolic artificial intelligence,Automated planning,"The General Problem Solver (GPS) cast planning as problem-solving used means-ends analysis to create plans. STRIPS took a different approach, viewing planning as theorem proving. Graphplan takes a least-commitment approach to planning, rather than sequentially choosing actions from an initial state, working forwards, or a goal state if working backwards.  Satplan is an approach to planning where a planning problem is reduced to a Boolean satisfiability problem.",97
Symbolic artificial intelligence,Natural language processing,"Natural language processing focuses on treating language as data to perform tasks such as identifying topics without necessarily understanding the intended meaning. Natural language understanding, in contrast, constructs a meaning representation and uses that for further processing, such as answering questions.
Parsing, tokenizing, spelling correction, part-of-speech tagging, noun and verb phrase chunking are all aspects of natural language processing long handled by symbolic AI, but since improved by deep learning approaches. In symbolic AI, discourse representation theory and first-order logic have been used to represent sentence meanings. Latent semantic analysis (LSA) and explicit semantic analysis also provided vector representations of documents. In the latter case, vector components are interpretable as concepts named by Wikipedia articles. 
New deep learning approaches based on Transformer models have now eclipsed these earlier symbolic AI approaches and attained state-of-the-art performance in natural language processing. However, Transformer models are opaque and do not yet produce human-interpretable semantic representations for sentences and documents. Instead, they produce task-specific vectors where the meaning of the vector components is opaque.",227
Symbolic artificial intelligence,Agents and multi-agent systems,"Agents are autonomous systems embedded in an environment they perceive and act upon in some sense. Russell and Norvig's standard textbook on artificial intelligence is organized to reflect agent architectures of increasing sophistication. The sophistication of agents varies from simple reactive agents, to those with a model of the world and automated planning capabilities, possibly a BDI agent, i.e., one with beliefs, desires, and intentions – or alternatively a reinforcement learning model learned over time to choose actions – up to a combination of alternative architectures, such as a neuro-symbolic architecture that includes deep learning for perception. 
In contrast, a multi-agent system consists of multiple agents that communicate amongst themselves with some inter-agent communication language such as Knowledge Query and Manipulation Language (KQML). The agents need not all have the same internal architecture. Advantages of multi-agent systems include the ability to divide work among the agents and to increase fault tolerance when agents are lost. Research problems include how agents reach consensus, distributed problem solving, multi-agent learning, multi-agent planning, and distributed constraint optimization.",228
Symbolic artificial intelligence,Controversies,"Controversies arose from early on in symbolic AI, both within the field—e.g., between logicists (the pro-logic ""neats"") and non-logicists (the anti-logic ""scruffies"")—and between those who embraced AI but rejected symbolic approaches—primarily connectionists—and those outside the field. Critiques from outside of the field were primarily from philosophers, on intellectual grounds, but also from funding agencies, especially during the two AI winters.",105
Symbolic artificial intelligence,The Qualification Problem,"Now we turn to attacks from outside the field specifically by philosophers. One argument frequently cited by philosophers was made earlier by the computer scientist Alan Turing, in his 1950 paper Computing Machinery and Intelligence, when he said that ""human behavior is far too complex to be captured by any formal set of rules—humans must be using some informal guidelines that … could never be captured in a formal set of rules and thus could never be codified in a computer program."" Turing called this ""The Argument from Informality of Behaviour."" Similar critiques were provided by philosophers Hubert Dreyfus and John Haugeland.

Russell and Norvig explain that these arguments were targeted to the symbolic AI of the 1980s:The technology they criticized came to be called Good Old-Fashioned AI (GOFAI). GOFAI corresponds to the simplest logical agent design described ... and we saw ... that it is indeed difficult to capture every contingency of appropriate behavior in a set of necessary and sufficient logical rules; we called that the qualification problem.
Since then, probabilistic reasoning systems have extended the capability of symbolic AI so they can be much ""more appropriate for open-ended domains."" Other approaches investigated included machine learning (both symbolic and neural) and non-monotonic logics.",265
Symbolic artificial intelligence,Connectionist AI: philosophical challenges and sociological conflicts,"Connectionist approaches include earlier work on neural networks, such as perceptrons; work in the mid to late 80s, such as Danny Hillis's Connection Machine and Yann LeCun's advances in convolutional neural networks; to today's more advanced approaches, such as Transformers, GANs, and other work in deep learning.
Three philosophical positions have been outlined among connectionists:

Implementationism—where connectionist architectures implement the capabilities for symbolic processing,
Radical connectionism—where symbolic processing is rejected totally, and connectionist architectures underlie intelligence and are fully sufficient to explain it,
Moderate connectionism—where symbolic processing and connectionist architectures are viewed as complementary and both are required for intelligence.
Olazaran, in his sociological history of the controversies within the neural network community, described the moderate connectionism view as essentially compatible with current research in neuro-symbolic hybrids:
The third and last position I would like to examine here is what I call the moderate connectionist view, a more eclectic view of the current debate between connectionism and symbolic AI. One of the researchers who has elaborated this position most explicitly is Andy Clark, a philosopher from the School of Cognitive and Computing Sciences of the University of Sussex (Brighton, England). Clark defended hybrid (partly symbolic, partly connectionist) systems. He claimed that (at least) two kinds of theories are needed in order to study and model cognition. On the one hand, for some information-processing tasks (such as pattern recognition) connectionism has advantages over symbolic models. But on the other hand, for other cognitive processes (such as serial, deductive reasoning, and generative symbol manipulation processes) the symbolic paradigm offers adequate models, and not only ""approximations"" (contrary to what radical connectionists would claim). 
Gary Marcus has claimed that the animus in the deep learning community against symbolic approaches now may be more sociological than philosophical:To think that we can simply abandon symbol-manipulation is to suspend disbelief.

And yet, for the most part, that's how most current AI proceeds. Hinton and many others have tried hard to banish symbols altogether. The deep learning hope—seemingly grounded not so much in science, but in a sort of historical grudge—is that intelligent behavior will emerge purely from the confluence of massive data and deep learning. Where classical computers and software solve tasks by defining sets of symbol-manipulating rules dedicated to particular jobs, such as editing a line in a word processor or performing a calculation in a spreadsheet, neural networks typically try to solve tasks by statistical approximation and learning from examples.According to Marcus, Geoffrey Hinton and his colleagues have been vehemently ""anti-symbolic"":When deep learning reemerged in 2012, it was with a kind of take-no-prisoners attitude that has characterized most of the last decade. By 2015, his hostility toward all things symbols had fully crystallized. He gave a talk at an AI workshop at Stanford comparing symbols to aether, one of science's greatest mistakes.
...

Since then, his anti-symbolic campaign has only increased in intensity. In 2016, Yann LeCun, Bengio, and Hinton wrote a manifesto for deep learning in one of science's most important journals, Nature. It closed with a direct attack on symbol manipulation, calling not for reconciliation but for outright replacement. Later, Hinton told a gathering of European Union leaders that investing any further money in symbol-manipulating approaches was ""a huge mistake,"" likening it to investing in internal combustion engines in the era of electric cars.
Part of these disputes may be due to unclear terminology: 

Turing award winner Judea Pearl offers a critique of machine learning which, unfortunately, conflates the terms machine learning and deep learning. Similarly, when Geoffrey Hinton refers to symbolic AI, the connotation of the term tends to be that of expert systems dispossessed of any ability to learn. The use of the terminology is in need of clarification. Machine learning is not confined to association rule mining, c.f. the body of work on symbolic ML and relational learning (the differences to deep learning being the choice of representation, localist logical rather than distributed, and the non-use of gradient-based learning algorithms). Equally, symbolic AI is not just about production rules written by hand. A proper definition of AI concerns knowledge representation and reasoning, autonomous multi-agent systems, planning and argumentation, as well as learning.",944
Symbolic artificial intelligence,Situated robotics: the world as a model,"Another critique of symbolic AI is the embodied cognition approach:

The embodied cognition approach claims that it makes no sense to consider the brain separately: cognition takes place within a body, which is embedded in an environment. We need to study the system as a whole; the brain's functioning exploits regularities in its environment, including the rest of its body. Under the embodied cognition approach, robotics, vision, and other sensors become central, not peripheral.
Rodney Brooks invented behavior-based robotics, one approach to embodied cognition. Nouvelle AI, another name for this approach, is viewed as an alternative to both symbolic AI and connectionist AI. His approach rejected representations, either symbolic or distributed, as not only unnecessary, but as detrimental. Instead, he created the subsumption architecture, a layered architecture for embodied agents. Each layer achieves a different purpose and must function in the real world. For example, the first robot he describes in Intelligence Without Representation, has three layers. The bottom layer interprets sonar sensors to avoid objects. The middle layer causes the robot to wander around when there are no obstacles. The top layer causes the robot to go to more distant places for further exploration. Each layer can temporarily inhibit or suppress a lower-level layer. He criticized AI researchers for defining AI problems for their systems, when: ""There is no clean division between perception (abstraction) and reasoning in the real world."" He called his robots ""Creatures"" and each layer was ""composed of a fixed-topology network of simple finite state machines.""  In the Nouvelle AI approach, ""First, it is vitally important to test the Creatures we build in the real world; i.e., in the same world that we humans inhabit. It is disastrous to fall into the temptation of testing them in a simplified world first, even with the best intentions of later transferring activity to an unsimplified world.""  His emphasis on real-world testing was in contrast to ""Early work in AI concentrated on games, geometrical problems, symbolic algebra, theorem proving, and other formal systems"" and the use of the blocks world in symbolic AI systems such as SHRDLU.",456
Symbolic artificial intelligence,Current views,"Each approach—symbolic, connectionist, and behavior-based—has advantages, but has been criticized by the other approaches. Symbolic AI has been criticized as disembodied, liable to the qualification problem, and poor in handling the perceptual problems where deep learning excels. In turn, connectionist AI has been criticized as poorly suited for deliberative step-by-step problem solving, incorporating knowledge, and handling planning. Finally, Nouvelle AI excels in reactive and real-world robotics domains but has been criticized for difficulties in incorporating learning and knowledge.

Hybrid AIs incorporating one or more of these approaches are currently viewed as the path forward. Russell and Norvig conclude that:Overall, Dreyfus saw areas where AI did not have complete answers and said that Al is therefore impossible; we now see many of these same areas undergoing continued research and development leading to increased capability, not impossibility.",190
Artificial intelligence art,Summary,"Artificial intelligence art is any artwork, particularly images and musical compositions, created through the use of artificial intelligence (AI) programs, such as text-to-image models and musical generators. It is sometimes confused with digital art. While both AI art and digital art involve the use of technology, AI art is characterized by its use of generative algorithms and deep learning techniques that can autonomously produce art without direct input from human artists.",91
Artificial intelligence art,Early history,"Artificial intelligence was founded as an academic discipline in 1956, and in the decades since has experienced several waves of optimism. Since its founding, researchers in the field of artificial intelligence have raised philosophical arguments about the nature of the human mind and the ethical consequences of creating artificial beings endowed with human-like intelligence; these issues have previously been explored by myth, fiction and philosophy since antiquity.",79
Artificial intelligence art,Tools and processes,,3
Artificial intelligence art,Imagery,"Many mechanisms for creating AI art have been developed, including procedural ""rule-based"" generation of images using mathematical patterns, algorithms which simulate brush strokes and other painted effects, and deep learning algorithms, such as generative adversarial networks (GANs) and transformers.
One of the first significant AI art systems is AARON, developed by Harold Cohen beginning in the late 1960s at the University of California at San Diego. AARON is the most notable example of AI art in the era of GOFAI programming because of its use of a symbolic rule-based approach to generate technical images. Cohen developed AARON with the goal of being able to code the act of drawing. In its primitive form, AARON created simple black and white drawings. Cohen would later finish the drawings by painting them. Throughout the years, he also began to develop a way for AARON to also paint. Cohen designed AARON to paint using special brushes and dyes that were chosen by the program itself without mediation from Cohen.

Generative adversarial networks (GANs) were designed in 2014. This system uses a ""generator"" to create new images and a ""discriminator"" to decide which created images are considered successful. DeepDream, released by Google in 2015, uses a convolutional neural network to find and enhance patterns in images via algorithmic pareidolia, thus creating deliberately over-processed images. After DeepDream's release, several companies released apps that transform photos into art-like images with the style of well-known sets of paintings.
The website Artbreeder, launched in 2018, uses the models StyleGAN and BigGAN to allow users to generate and modify images such as faces, landscapes, and paintings.Several programs use text-to-image models to generate a variety of images based on various text prompts. They include VQGAN+CLIP which was released in 2021, OpenAI's DALL-E which released a series of images in January 2021,  Google Brain's Imagen and Parti which was announced in May 2022, Microsoft's NUWA-Infinity, and Stable Diffusion which was released in August 2022.  Stability.ai has a Stable Diffusion web interface called DreamStudio. Stable Diffusion is source-available software, enabling further development such as plugins for Krita, Photoshop, Blender, and GIMP, as well as the Automatic1111 web-based open source user interface.There are many other AI art generation programs including simple consumer-facing mobile apps and Jupyter notebooks that require powerful GPUs to run effectively.",540
Artificial intelligence art,Impact and applications,"The exhibition ""Thinking Machines: Art and Design in the Computer Age, 1959–1989"" at MoMA provided an overview of AI applications for art, architecture, and design. Exhibitions showcasing the usage of AI to produce art include the 2016 Google-sponsored benefit and auction at the Gray Area Foundation in San Francisco, where artists experimented with the DeepDream algorithm and the 2017 exhibition ""Unhuman: Art in the Age of AI"", which took place in Los Angeles and Frankfurt. In spring 2018, the Association for Computing Machinery dedicated a magazine issue to the subject of computers and art. In June 2018, ""Duet for Human and Machine"", an art piece permitting viewers to interact with an artificial intelligence, premiered at the Beall Center for Art + Technology. The Austrian Ars Electronica and Museum of Applied Arts, Vienna opened exhibitions on AI in 2019. Ars Electronica's 2019 festival ""Out of the box"" explored art's role in a sustainable societal transformation.
Examples of such augmentation may include e.g. enabling expansion of noncommercial niche genres (common examples are cyberpunk derivatives like solarpunk) by amateurs, novel entertainment, novel imaginative childhood play, very fast prototyping, increasing art-making accessibility and artistic output per effort and/or expenses and/or time – e.g. via generating drafts, inspirations, draft-refinitions, and image-components (Inpainting).
Synthetic media, which includes AI art, has been described in 2022 as a major technology-driven trend that will affect business in the coming years.",326
Artificial intelligence art,Prompt engineering and sharing,"Prompts for some text-to-image models can also include images and keywords and configurable parameters, such as artistic style, which is often used via keyphrases like ""in the style of [name of an artist]"" in the prompt and/or selection of a broad aesthetic/art style. There are platforms for sharing, trading, searching, forking/refining and/or collaborating on prompts for generating specific imagery from image generators. Prompts are often shared along with images on image-sharing websites such as reddit and AI art-dedicated websites. They are not the complete input or details used for the generation of images.",136
Artificial intelligence art,Development,"Additional functionalities are under development and may improve various applications or enable new ones – such as ""Textual Inversion"" which refers to enabling the use of user-provided concepts (like an object or a style) learned from few images. With textual inversion, novel personalized art can be generated from the associated word(s) (the keywords that have been assigned to the learned, often abstract, concept) and model extensions/fine-tuning (see also: DreamBooth).
Generated images are sometimes used as sketches or low-cost experimentations or illustration of proof-of-concept-stage ideas – additional functionalities or improvements may also relate to post-generation manual editing (polishing or artistic usage) of prompts-based art (such as subsequent tweaking with an image editor). In the case of Stable Diffusion, the main pre-trained model is shared on the Hugging Face Hub.",188
Artificial intelligence art,Other,"Some prototype robots can create what is considered forms of art – such as dynamic cooking robots that can taste and readjust.There also is AI-assisted writing beyond copy-editing (including support in the generation of fictional stories such as helping with writer's block or inspiration or rewriting segments).AI could be and has been used in video game art beyond imagery only, especially for level design (e.g. for custom maps) and creating new content or interactive stories in video games.",100
Artificial intelligence art,Analysis of existing art using AI,"In addition to the creation of original art, research methods that utilize AI have been generated to quantitatively analyze digital art collections. This has been made possible due to large-scale digitization of artwork in the past few decades. Although the main goal of digitization was to allow for accessibility and exploration of these collections, the use of AI in analyzing them has brought about new research perspectives.Two computational methods, close reading and distant viewing, are the typical approaches used to analyze digitized art. Close reading focuses on specific visual aspects of one piece. Some tasks performed by machines in close reading methods include computational artist authentication and analysis of brushstrokes or texture properties. In contrast, through distant viewing methods, the similarity across an entire collection for a specific feature can be statistically visualized. Common tasks relating to this method include automatic classification, object detection, multimodal tasks, knowledge discovery in art history, and computational aesthetics. Whereas distant viewing includes the analysis of large collections, close reading involves one piece of artwork.
Researchers have also introduced models that predict emotional responses to art such as ArtEmis, a large-scale dataset with machine learning models that contain emotional reactions to visual art as well as predictions of emotion from images or text.According to CETINIC and SHE (2022), using artificial intelligence to analyse already-existing art collections can provide fresh perspectives on the development of artistic styles and the identification of artistic influences. AI-assisted study of existing art can also aid in the organization of art exhibitions and support the decision-making process for curators and art historians.AI programs can automatically generate new images of artwork similar to those learned from the sample. Humans just need to do is mainly input data and discriminate output, and automate the specific generation, the combination of AI mechanisms and human art creation mechanisms allows AI to produce works.",374
Artificial intelligence art,Sales,"An auction sale of artificial intelligence art was held at Christie's Auction House in New York in 2018, where the AI artwork Edmond de Belamy sold for $432,500, which was almost 45 times higher than its estimate of $7,000–$10,000. The artwork was created by ""Obvious"", a Paris-based collective.",74
Artificial intelligence art,"Criticism, issues and controversy",,4
Artificial intelligence art,Copyright,"Ever since artists began using AI to create art in the 20th century, the use of AI-generated art has sparked a number of debates. In the 2020s, some of those debates concerned whether AI art can be defined as art or not and concerning the impact it will have on artists.In 1985, intellectual property law professor Pamela Samuelson considered the legal questions surrounding AI art authorship as it relates to copyright: who owns the copyright when the piece of art was created by artificial intelligence? Samuelson's article, ""Allocating Ownership Rights in Computer-Generated Works,"" argued that rights should be allocated to the user of the generator program. In response to the same question, a 2019 Florida Law Review article has presented three possible choices. First, the artificial intelligence itself becomes the copyright owner. To do this, Section 101 of the Copyright Act would need to be amended to define ""author"" as a natural person or a computer. Second, following Samuelson's argument, the user, programmer, or artificial intelligence company is the copyright owner. This would be an expansion of the ""work for hire"" doctrine, under which ownership of a copyright is transferred to the ""employer."" Finally, no one becomes the copyright owner, and the work would automatically enter public domain. The argument here is that because no person ""created"" the piece of art, no one should be the copyright owner.In 2022, coinciding with the rising availability of consumer-grade AI image generation services, popular discussion renewed over the legality and ethics of AI-generated art. Of particular issue is the use of copyrighted art within AI training datasets: in September 2022, Reema Selhi, of the Design and Artists Copyright Society, stated that ""there are no safeguards for artists to be able to identify works in databases that are being used and opt out."" Some have claimed that images generated by these models can bear an uncanny resemblance to extant artwork, sometimes including remains of the original artist's signature. Such discussion came to a head in December, when users of the portfolio platform ArtStation staged an online protest against nonconsensual use of their artwork within datasets: this resulted in opt-out services, such as ""Have I Been Trained?,"" increasing in profile, as well as some online art platforms promising to offer their own opt-out options. According to the US Copyright Office, artificial intelligence programs are unable to hold copyright.In January 2023 three artists — Sarah Andersen, Kelly McKernan, and Karla Ortiz — filed a copyright infringement lawsuit against Stability AI, Midjourney, and DeviantArt, claiming that these companies have infringed the rights of millions of artists by training AI tools on five billion images scraped from the web without the consent of the original artists. The same month, Stability AI was also sued by Getty Images for using its images in the training data.",583
Artificial intelligence art,Concerns about impact on artists,"Some artists in 2022 raised concerns about the impact AI image generators could have on their ability to earn money, particularly if AI images are used to replace artists working in illustration and design. In August 2022, a text-to-image AI illustration titled Théâtre d'Opéra Spatial won the first-place $300 prize in a digital art competition at the Colorado State Fair.Digital artist R. J. Palmer said in August 2022 that ""I could easily envision a scenario where using AI a single artist or art director could take the place of 5-10 entry level artists... I have seen a lot of self-published authors and such say how great it will be that they don’t have to hire an artist,"" adding that ""doing that kind of work for small creators is how a lot of us got our start as professional artists."" Polish digital artist Greg Rutkowski said in September 2022 that ""it's starting to look like a threat to our careers,"" adding that it has gotten more difficult to search for his work online because many of the images returned by search engines are generated by AI that was prompted to mimic his style.",237
Artificial intelligence art,Deception,"The 2023 winner of the ""creative open"" category in the Sony World Photography Awards, Boris Eldagsen, revealed after winning that his entry was actually generated by artificial intelligence. Photographer Feroz Khan commented to the BBC that Eldagsen had ""clearly shown that even experienced photographers and art experts can be fooled"". Smaller contests have been affected as well; in 2023 a contest called the ""Self-Published Fantasy Blog-Off cover contest"", run by author Mark Lawrence, was cancelled after the winning entry was allegedly exposed to be a collage of images generated by Midjourney.Wider issues extend beyond the art world. As with other types of photo manipulation since the early 19th century, some people in the early 21st century have been concerned that AI could be used to create content that is misleading, known as ""deepfakes"".

In May 2023, widespread attention was given to a Midjourney-generated photo of Pope Francis wearing a white puffer coat and another showing the fictional arrest of Donald Trump, and an AI-generated image of an attack on the Pentagon went viral as a hoax news story on Twitter.",237
